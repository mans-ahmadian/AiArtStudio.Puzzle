{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92bce612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Require Python 3.12.3\n",
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f53fc",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccdbb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5edc07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assts_dir= Path(os.getcwd())/\"assets\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee1f25",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2713a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoEditor:\n",
    "    def __init__(self, fps=30, fourcc='mp4v', frame_size=(640, 480)):\n",
    "        self.fps = fps\n",
    "        self.frame_size = frame_size\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*fourcc)\n",
    "        self.temp_video_path = tempfile.mktemp(suffix='.mp4')\n",
    "        self.video_writer = cv2.VideoWriter(self.temp_video_path, self.fourcc, self.fps, self.frame_size)\n",
    "        self.current_time = 0  # in seconds, tracks the actual current duration of the video content\n",
    "        self.audio_clips = []\n",
    "\n",
    "        # Default settings for time text overlay\n",
    "        self.default_show_time_text = False\n",
    "        self.default_time_text_position = (50, 50)  # Default top-left corner\n",
    "        self.default_time_text_color = (0, 255, 0)  # Default Green (B, G, R)\n",
    "        self.default_time_text_font_scale = 1.0\n",
    "        self.default_time_text_thickness = 2\n",
    "        self.default_time_text_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        self.default_time_display_format = \"HH:MM:SS.MS\"\n",
    "\n",
    "        # New class-level parameter for global display time offset\n",
    "        # If None, the displayed time will default to the actual video current_time.\n",
    "        # If set, it overrides the default behavior for all subsequent additions\n",
    "        # where display_time_start_offset is not explicitly provided.\n",
    "        self.global_display_time_offset_start = None\n",
    "        # New: Counter for the displayed time when global_display_time_offset_start is active\n",
    "        self.display_time_counter = 0.0\n",
    "\n",
    "    def set_global_display_time_offset_start(self, offset_time):\n",
    "        \"\"\"\n",
    "        Sets a global offset for the time displayed on the video.\n",
    "        Any subsequent images added with show_time_text enabled and without\n",
    "        an explicit display_time_start_offset will use this value as their base.\n",
    "\n",
    "        Args:\n",
    "            offset_time (float): The starting time (in seconds) to display on the video.\n",
    "                                 Set to None to revert to using the actual video's current_time.\n",
    "        \"\"\"\n",
    "        if not isinstance(offset_time, (int, float)) and offset_time is not None:\n",
    "            raise TypeError(\"offset_time must be a number (int or float) or None.\")\n",
    "        if offset_time is not None and offset_time < 0:\n",
    "            raise ValueError(\"offset_time cannot be negative.\")\n",
    "        self.global_display_time_offset_start = offset_time\n",
    "        # Initialize the display_time_counter when the global offset is set\n",
    "        self.display_time_counter = offset_time if offset_time is not None else 0.0\n",
    "\n",
    "\n",
    "    def set_default_time_display_format(self, format_string):\n",
    "        \"\"\"\n",
    "        Sets the default format for displaying time on frames.\n",
    "\n",
    "        Args:\n",
    "            format_string (str): The desired format string using placeholders:\n",
    "                                 HH (hours), MM (minutes), SS (seconds), MS (milliseconds).\n",
    "                                 Placeholders are case-insensitive (e.g., \"hh\", \"mm\", \"ss\", \"ms\" also work).\n",
    "                                 Example: \"HH:MM:SS.MS\", \"ss.ms\", \"MM:SS\".\n",
    "        \"\"\"\n",
    "        self.default_time_display_format = format_string\n",
    "\n",
    "    def _resize_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Resizes an image frame to the video's frame_size.\n",
    "        \"\"\"\n",
    "        return cv2.resize(frame, self.frame_size)\n",
    "\n",
    "    def _convert_to_cv2(self, image):\n",
    "        \"\"\"\n",
    "        Converts various image types (path, Pillow, NumPy array) to an OpenCV image (NumPy array)\n",
    "        and resizes it.\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            img = cv2.imread(image)\n",
    "            if img is None:\n",
    "                raise FileNotFoundError(f\"Image file not found or could not be read: {image}\")\n",
    "        elif isinstance(image, Image.Image):\n",
    "            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            img = image\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image type. Must be a file path (string), Pillow Image, or NumPy array.\")\n",
    "        return self._resize_frame(img)\n",
    "\n",
    "    def _draw_time_on_frame(self, frame, time_in_sec, position, color, font_scale, thickness, font, time_format):\n",
    "        \"\"\"\n",
    "        Draws the current time onto a video frame with a specified format.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): The OpenCV image frame.\n",
    "            time_in_sec (float): The time in seconds to display.\n",
    "            position (tuple): (x, y) coordinates for the text.\n",
    "            color (tuple): (B, G, R) color for the text.\n",
    "            font_scale (float): Font scale factor.\n",
    "            thickness (int): Line thickness for the text.\n",
    "            font (int): OpenCV font type (e.g., cv2.FONT_HERSHEY_SIMPLEX).\n",
    "            time_format (str): The format string using placeholders (HH, MM, SS, MS).\n",
    "                               Placeholders are case-insensitive.\n",
    "        \"\"\"\n",
    "        total_seconds_int = int(time_in_sec)\n",
    "        milliseconds = int((time_in_sec - total_seconds_int) * 1000)\n",
    "        seconds = total_seconds_int % 60\n",
    "        minutes = (total_seconds_int // 60) % 60\n",
    "        hours = total_seconds_int // 3600\n",
    "\n",
    "        replacements = {\n",
    "            \"HH\": f\"{hours:02}\", \"hh\": f\"{hours:02}\",\n",
    "            \"MM\": f\"{minutes:02}\", \"mm\": f\"{minutes:02}\",\n",
    "            \"SS\": f\"{seconds:02}\", \"ss\": f\"{seconds:02}\",\n",
    "            \"MS\": f\"{milliseconds:03}\", \"ms\": f\"{milliseconds:03}\"\n",
    "        }\n",
    "\n",
    "        time_str = time_format\n",
    "        for placeholder, value in replacements.items():\n",
    "            time_str = time_str.replace(placeholder, value)\n",
    "\n",
    "        cv2.putText(frame, time_str, position, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        return frame\n",
    "\n",
    "    def add_image(self, image, duration_sec,\n",
    "                  show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                  time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                  display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds a single image to the video for a specified duration, with optional time display.\n",
    "\n",
    "        Args:\n",
    "            image: The image to add. Can be a file path (string), a Pillow Image object,\n",
    "                   or an OpenCV image (NumPy array).\n",
    "            duration_sec (float): The duration (in seconds) for which the image should be displayed.\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "                                              Defaults to self.default_show_time_text.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the text.\n",
    "                                                  Defaults to self.default_time_text_position.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the text.\n",
    "                                               Defaults to self.default_time_text_color.\n",
    "            time_text_font_scale (float, optional): Font scale factor.\n",
    "                                                    Defaults to self.default_time_text_font_scale.\n",
    "            time_text_thickness (int, optional): Line thickness for the text.\n",
    "                                                 Defaults to self.default_time_text_thickness.\n",
    "            time_text_font (int, optional): OpenCV font type.\n",
    "                                            Defaults to self.default_time_text_font.\n",
    "            display_time_start_offset (float, optional): The starting time (in seconds) to display\n",
    "                                                         on the video for this segment.\n",
    "                                                         Precedence: local param > global param > actual video current time.\n",
    "            time_display_format (str, optional): The format for the time string (e.g., \"HH:MM:SS.MS\").\n",
    "                                                 Placeholders are case-insensitive.\n",
    "                                                 Precedence: local param > default class param.\n",
    "        \"\"\"\n",
    "        frame = self._convert_to_cv2(image)\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "\n",
    "        # Resolve text display parameters\n",
    "        _show_text = self.default_show_time_text if show_time_text is None else show_time_text\n",
    "        _position = self.default_time_text_position if time_text_position is None else time_text_position\n",
    "        _color = self.default_time_text_color if time_text_color is None else time_text_color\n",
    "        _font_scale = self.default_time_text_font_scale if time_text_font_scale is None else time_text_font_scale\n",
    "        _thickness = self.default_time_text_thickness if time_text_thickness is None else time_text_thickness\n",
    "        _font = self.default_time_text_font if time_text_font is None else time_text_font\n",
    "        _time_format = self.default_time_display_format if time_display_format is None else time_display_format\n",
    "\n",
    "        # Determine the base time for display based on precedence\n",
    "        _base_display_time_for_this_segment = self.current_time # Default fallback: actual video time\n",
    "\n",
    "        if display_time_start_offset is not None:\n",
    "            # Local override takes highest precedence\n",
    "            _base_display_time_for_this_segment = display_time_start_offset\n",
    "        elif self.global_display_time_offset_start is not None:\n",
    "            # Global override applies if no local override. Use the continuous display_time_counter.\n",
    "            _base_display_time_for_this_segment = self.display_time_counter\n",
    "\n",
    "        for i in range(frame_count):\n",
    "            # Calculate the time to display for the current frame\n",
    "            current_display_time = _base_display_time_for_this_segment + (i / self.fps)\n",
    "            frame_to_write = frame.copy() # Use a copy to avoid drawing on the original frame for next iteration\n",
    "            if _show_text:\n",
    "                frame_to_write = self._draw_time_on_frame(\n",
    "                    frame_to_write, current_display_time, _position, _color, _font_scale, _thickness, _font, _time_format\n",
    "                )\n",
    "            self.video_writer.write(frame_to_write)\n",
    "\n",
    "        # Always update the actual video time\n",
    "        self.current_time += duration_sec\n",
    "\n",
    "        # Only update display_time_counter if global offset is active AND no local offset was used\n",
    "        if display_time_start_offset is None and self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration_sec\n",
    "\n",
    "\n",
    "    def add_images_from_list(self, images, total_duration_sec,\n",
    "                             show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                             time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                             display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds several images to the video, distributing them evenly over a total duration,\n",
    "        with optional time display on each image.\n",
    "\n",
    "        Args:\n",
    "            images: Can be a string (directory path), a list of strings (image file paths),\n",
    "                    a list of OpenCV images (numpy arrays), or a list of Pillow images.\n",
    "            total_duration_sec (float): The total duration (in seconds) that these images\n",
    "                                        should occupy in the video.\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "                                              Defaults to self.default_show_time_text.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the text.\n",
    "                                                  Defaults to self.default_time_text_position.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the text.\n",
    "                                               Defaults to self.default_time_text_color.\n",
    "            time_text_font_scale (float, optional): Font scale factor.\n",
    "                                                    Defaults to self.default_time_text_font_scale.\n",
    "            time_text_thickness (int, optional): Line thickness for the text.\n",
    "                                                 Defaults to self.default_time_text_thickness.\n",
    "            time_text_font (int, optional): OpenCV font type.\n",
    "                                            Defaults to self.default_time_text_font.\n",
    "            display_time_start_offset (float, optional): The starting time (in seconds) to display\n",
    "                                                         on the video for this segment.\n",
    "                                                         Precedence: local param > global param > actual video current time.\n",
    "            time_display_format (str, optional): The format for the time string (e.g., \"HH:MM:SS.MS\").\n",
    "                                                 Placeholders are case-insensitive.\n",
    "                                                 Precedence: local param > default class param.\n",
    "        \"\"\"\n",
    "        image_list = []\n",
    "\n",
    "        if isinstance(images, str) and os.path.isdir(images):\n",
    "            for filename in sorted(os.listdir(images)):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    image_list.append(os.path.join(images, filename))\n",
    "        elif isinstance(images, list):\n",
    "            if all(isinstance(img, str) for img in images):\n",
    "                image_list = images\n",
    "            elif all(isinstance(img, (np.ndarray, Image.Image)) for img in images):\n",
    "                image_list = images\n",
    "            else:\n",
    "                raise ValueError(\"List must contain only strings (paths), OpenCV images, or Pillow images.\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported 'images' type. Must be a directory path (string), a list of paths, a list of OpenCV images, or a list of Pillow images.\")\n",
    "\n",
    "        if not image_list:\n",
    "            print(\"No images found to add.\")\n",
    "            return\n",
    "\n",
    "        single_image_duration = total_duration_sec / len(image_list)\n",
    "\n",
    "        for i, img in enumerate(image_list):\n",
    "            # The display_time_start_offset passed to add_image needs to be carefully managed.\n",
    "            # If the user provided a display_time_start_offset to add_images_from_list,\n",
    "            # we calculate the offset for each individual image within that batch.\n",
    "            # If not, we pass None, allowing add_image to use the global_display_time_offset_start\n",
    "            # and its internal display_time_counter, which is the desired behavior for continuous time.\n",
    "            effective_display_offset_for_image_batch = None\n",
    "            if display_time_start_offset is not None:\n",
    "                effective_display_offset_for_image_batch = display_time_start_offset + (i * single_image_duration)\n",
    "\n",
    "            self.add_image(img, single_image_duration,\n",
    "                           show_time_text=show_time_text,\n",
    "                           time_text_position=time_text_position,\n",
    "                           time_text_color=time_text_color,\n",
    "                           time_text_font_scale=time_text_font_scale,\n",
    "                           time_text_thickness=time_text_thickness,\n",
    "                           time_text_font=time_text_font,\n",
    "                           display_time_start_offset=effective_display_offset_for_image_batch, # Pass the calculated offset or None\n",
    "                           time_display_format=time_display_format)\n",
    "\n",
    "    def add_text_below_image(self, image, text, duration_sec,\n",
    "                             text_box_height_ratio=0.2,\n",
    "                             background_color=(0, 0, 0),\n",
    "                             text_color=(255, 255, 255),\n",
    "                             text_horizontal_alignment=\"center\",\n",
    "                             text_vertical_alignment=\"center\",\n",
    "                             font_scale=0.7, thickness=2, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                             show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                             time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                             display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds an image with text displayed in a box below it for a given duration.\n",
    "        The combined content fits within the video's frame_size.\n",
    "\n",
    "        Args:\n",
    "            image: The input image (path, Pillow, or NumPy array).\n",
    "            text (str): The text string to display.\n",
    "            duration_sec (float): The duration (in seconds) for which this combined frame should be displayed.\n",
    "            text_box_height_ratio (float): Ratio (0.0 to 1.0) of the frame height allocated to the text box.\n",
    "            background_color (tuple): (B, G, R) color for the text box background.\n",
    "            text_color (tuple): (B, G, R) color for the text.\n",
    "            text_horizontal_alignment (str): Horizontal alignment of text (\"left\", \"center\", \"right\").\n",
    "            text_vertical_alignment (str): Vertical alignment of text (\"top\", \"center\", \"bottom\").\n",
    "            font_scale (float): Font scale factor for the text.\n",
    "            thickness (int): Line thickness for the text.\n",
    "            font (int): OpenCV font type (e.g., cv2.FONT_HERSHEY_SIMPLEX).\n",
    "            # Parameters for optional time display (same as add_image)\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the time text.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the time text.\n",
    "            time_text_font_scale (float, optional): Font scale factor for time text.\n",
    "            time_text_thickness (int, optional): Line thickness for time text.\n",
    "            time_text_font (int, optional): OpenCV font type for time text.\n",
    "            display_time_start_offset (float, optional): The starting time to display.\n",
    "            time_display_format (str, optional): The format for the time string.\n",
    "        \"\"\"\n",
    "        if not (0.0 <= text_box_height_ratio <= 1.0):\n",
    "            raise ValueError(\"text_box_height_ratio must be between 0.0 and 1.0.\")\n",
    "\n",
    "        original_img_frame = self._convert_to_cv2(image)\n",
    "        \n",
    "        video_width, video_height = self.frame_size\n",
    "        text_box_height = int(video_height * text_box_height_ratio)\n",
    "        image_display_height = video_height - text_box_height\n",
    "\n",
    "        if image_display_height <= 0:\n",
    "            raise ValueError(\"Image display height is zero or negative. Reduce text_box_height_ratio or increase frame_size.\")\n",
    "\n",
    "        resized_img_for_top = cv2.resize(original_img_frame, (video_width, image_display_height))\n",
    "\n",
    "        text_section = np.full((text_box_height, video_width, 3), background_color, dtype=np.uint8)\n",
    "\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        text_width, text_height_actual = text_size\n",
    "\n",
    "        text_x, text_y = 0, 0\n",
    "\n",
    "        if text_horizontal_alignment.lower() == \"center\":\n",
    "            text_x = (video_width - text_width) // 2\n",
    "        elif text_horizontal_alignment.lower() == \"right\":\n",
    "            text_x = video_width - text_width - 10\n",
    "        else: # Default to \"left\"\n",
    "            text_x = 10\n",
    "\n",
    "        if text_vertical_alignment.lower() == \"center\":\n",
    "            text_y = (text_box_height + text_height_actual) // 2\n",
    "        elif text_vertical_alignment.lower() == \"bottom\":\n",
    "            text_y = text_box_height - 10\n",
    "        else: # Default to \"top\"\n",
    "            text_y = text_height_actual + 10\n",
    "\n",
    "        cv2.putText(text_section, text, (text_x, text_y), font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        final_frame = np.vstack((resized_img_for_top, text_section))\n",
    "\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "\n",
    "        _show_text = self.default_show_time_text if show_time_text is None else show_time_text\n",
    "        _time_position = self.default_time_text_position if time_text_position is None else time_text_position\n",
    "        _time_color = self.default_time_text_color if time_text_color is None else time_text_color\n",
    "        _time_font_scale = self.default_time_text_font_scale if time_text_font_scale is None else time_text_font_scale\n",
    "        _time_thickness = self.default_time_text_thickness if time_text_thickness is None else time_text_thickness\n",
    "        _time_font = self.default_time_text_font if time_text_font is None else time_text_font\n",
    "        _time_format = self.default_time_display_format if time_display_format is None else time_display_format\n",
    "\n",
    "        _base_display_time_for_this_segment = self.current_time\n",
    "        if display_time_start_offset is not None:\n",
    "            _base_display_time_for_this_segment = display_time_start_offset\n",
    "        elif self.global_display_time_offset_start is not None:\n",
    "            _base_display_time_for_this_segment = self.display_time_counter\n",
    "\n",
    "\n",
    "        for i in range(frame_count):\n",
    "            current_display_time = _base_display_time_for_this_segment + (i / self.fps)\n",
    "            frame_to_write = final_frame.copy()\n",
    "            if _show_text:\n",
    "                frame_to_write = self._draw_time_on_frame(\n",
    "                    frame_to_write, current_display_time, _time_position, _time_color, _time_font_scale, _time_thickness, _time_font, _time_format\n",
    "                )\n",
    "            self.video_writer.write(frame_to_write)\n",
    "        \n",
    "        self.current_time += duration_sec\n",
    "\n",
    "        if display_time_start_offset is None and self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration_sec\n",
    "\n",
    "    def add_video(self, video_path):\n",
    "        \"\"\"\n",
    "        Adds another video clip to the current video. The added video retains its original duration.\n",
    "\n",
    "        Args:\n",
    "            video_path (str): Path to the video file to be added.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise FileNotFoundError(f\"Video file not found or could not be opened: {video_path}\")\n",
    "\n",
    "        video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if video_fps == 0:\n",
    "            video_fps = self.fps\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = self._resize_frame(frame)\n",
    "            self.video_writer.write(frame)\n",
    "\n",
    "        duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / video_fps\n",
    "        cap.release()\n",
    "        self.current_time += duration\n",
    "        # If global display offset is active, and we are adding a video (which doesn't have its own\n",
    "        # display_time_start_offset parameter), we should also advance the display_time_counter.\n",
    "        if self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration\n",
    "\n",
    "\n",
    "    def add_audio(self, audio_path, audio_clip_start=None, audio_clip_end=None, video_start_offset=None):\n",
    "        \"\"\"\n",
    "        Adds an audio clip to the video timeline.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "            audio_clip_start (float, optional): The start time (in seconds) within the audio file itself.\n",
    "                                                Defaults to 0 (beginning of the audio file).\n",
    "            audio_clip_end (float, optional): The end time (in seconds) within the audio file itself.\n",
    "                                              Defaults to the end of the audio clip.\n",
    "            video_start_offset (float, optional): The time (in seconds) on the video timeline where this\n",
    "                                                  audio should start. If None, it starts at the current\n",
    "                                                  end time of the video (`self.current_time`).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not load audio file {audio_path}: {e}\")\n",
    "\n",
    "        if audio_clip_start is not None or audio_clip_end is not None:\n",
    "            if audio_clip_start is not None and audio_clip_end is not None and audio_clip_start > audio_clip_end:\n",
    "                raise ValueError(\"audio_clip_start cannot be greater than audio_clip_end.\")\n",
    "            \n",
    "            start_subclip = audio_clip_start if audio_clip_start is not None else 0\n",
    "            end_subclip = audio_clip_end if audio_clip_end is not None else audio_clip.duration\n",
    "            \n",
    "            audio_clip = audio_clip.subclip(start_subclip, end_subclip)\n",
    "\n",
    "        offset_on_video = video_start_offset if video_start_offset is not None else self.current_time\n",
    "\n",
    "        self.audio_clips.append((audio_clip, offset_on_video))\n",
    "\n",
    "    def get_video_duration(self):\n",
    "        \"\"\"\n",
    "        Returns the current duration of the video content in seconds.\n",
    "        \"\"\"\n",
    "        return self.current_time\n",
    "\n",
    "    def save(self, output_path):\n",
    "        \"\"\"\n",
    "        Finalizes the video and merges audio if present.\n",
    "        \"\"\"\n",
    "        self.video_writer.release()\n",
    "\n",
    "        final_clip = VideoFileClip(self.temp_video_path)\n",
    "\n",
    "        if self.audio_clips:\n",
    "            all_audios = []\n",
    "            for audio, offset in self.audio_clips:\n",
    "                all_audios.append(audio.set_start(offset))\n",
    "            \n",
    "            composite_audio = CompositeAudioClip(all_audios)\n",
    "            \n",
    "            final_clip = final_clip.set_audio(composite_audio)\n",
    "\n",
    "        print(f\"Saving video to {output_path}...\")\n",
    "        final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "        final_clip.close()\n",
    "        os.remove(self.temp_video_path)\n",
    "        print(\"Video saved successfully and temporary file removed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3de60dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SD15ImageGenerator:\n",
    "    def __init__(self, model_id=\"runwayml/stable-diffusion-v1-5\", use_cuda=True, num_inference_steps=25):\n",
    "        \"\"\"\n",
    "        Initialize the Stable Diffusion 1.5 pipeline and inference settings.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.intermediate_images = []\n",
    "\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            safety_checker=None\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _capture_step(self, step, timestep, latents):\n",
    "        \"\"\"\n",
    "        Internal callback to capture the image at each step.\n",
    "        \"\"\"\n",
    "        # Decode latent to image at this step\n",
    "        with torch.no_grad():\n",
    "            image = self.pipe.vae.decode(latents / self.pipe.vae.config.scaling_factor).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image_pil = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "            self.intermediate_images.append(image_pil)\n",
    "\n",
    "    def generate_image(self, prompt, negative_prompt=None, guidance_scale=7.5):\n",
    "        \"\"\"\n",
    "        Generate image and collect intermediate steps.\n",
    "        Returns a list of PIL images (one per step).\n",
    "        \"\"\"\n",
    "        self.intermediate_images = []\n",
    "\n",
    "        with torch.autocast(self.device) if self.device == \"cuda\" else torch.no_grad():\n",
    "            _ = self.pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=self.num_inference_steps,\n",
    "                callback=self._capture_step,\n",
    "                callback_steps=1  # capture every step\n",
    "            )\n",
    "\n",
    "        return self.intermediate_images\n",
    "\n",
    "    def save_image(self, image: Image.Image, output_path: str):\n",
    "        \"\"\"\n",
    "        Save a single PIL image to the specified path.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        image.save(output_path)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "    def save_images(self, images, directory=\"generated\"):\n",
    "        \"\"\"\n",
    "        Save a list of images to the given directory.\n",
    "        \"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        for i, img in enumerate(images):\n",
    "            path = os.path.join(directory, f\"step_{i:02d}.png\")\n",
    "            img.save(path)\n",
    "        print(f\"Saved {len(images)} images to '{directory}/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170c009",
   "metadata": {},
   "source": [
    "# Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae56ed7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d17a177b7e04bc18914975df1dc34e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "generator = SD15ImageGenerator(num_inference_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "222490ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c556367956464284fe597cd93ca100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"Cat\"\n",
    "    \n",
    "images = generator.generate_image(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a5f139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_evenly_distributed_values(data):\n",
    "    \"\"\"\n",
    "    Generates evenly distributed values for each tuple (number, start, end) in a list.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of tuples, where each tuple is (number, start, end).\n",
    "                     'number' is the count of values to generate, 'start' is the\n",
    "                     beginning of the range, and 'end' is the end of the range.\n",
    "\n",
    "    Returns:\n",
    "        list: A single list containing all the generated evenly distributed values.\n",
    "    \"\"\"\n",
    "    all_values = []\n",
    "    for num, start, end in data:\n",
    "        # Generate 'num' evenly distributed values between 'start' and 'end'\n",
    "        # np.linspace includes both start and end points\n",
    "        if num > 0:\n",
    "            generated_values = np.linspace(start, end, num).tolist()\n",
    "            all_values.extend(generated_values)\n",
    "    return all_values\n",
    "\n",
    "# Example Usage:\n",
    "# data1 = [(5, 0, 10), (3, 100, 102)]\n",
    "# result1 = generate_evenly_distributed_values(data1)\n",
    "# print(f\"Result for data1: {result1}\")\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]\n",
    "\n",
    "# data2 = [(1, 5, 5), (4, -2, 2)]\n",
    "# result2 = generate_evenly_distributed_values(data2)\n",
    "# print(f\"Result for data2: {result2}\")\n",
    "# # Expected output for data2: [5.0, -2.0, -0.6666666666666666, 0.6666666666666666, 2.0]\n",
    "\n",
    "# data3 = []\n",
    "# result3 = generate_evenly_distributed_values(data3)\n",
    "# print(f\"Result for data3: {result3}\")\n",
    "# # Expected output for data3: []\n",
    "\n",
    "# data4 = [(0, 1, 10)]\n",
    "# result4 = generate_evenly_distributed_values(data4)\n",
    "# print(f\"Result for data4: {result4}\")\n",
    "# # Expected output for data4: []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "088ce93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for data1: [0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.03333333333333333, 0.04942528735632184, 0.06551724137931034, 0.08160919540229886, 0.09770114942528735, 0.11379310344827587, 0.12988505747126436, 0.14597701149425288, 0.16206896551724137, 0.1781609195402299, 0.19425287356321838, 0.21034482758620687, 0.2264367816091954, 0.2425287356321839, 0.25862068965517243, 0.2747126436781609, 0.2908045977011494, 0.30689655172413793, 0.32298850574712645, 0.3390804597701149, 0.35517241379310344, 0.37126436781609196, 0.3873563218390804, 0.40344827586206894, 0.41954022988505746, 0.435632183908046, 0.4517241379310345, 0.46781609195402296, 0.4839080459770115, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.44166666666666665, 0.3833333333333333, 0.325, 0.26666666666666666, 0.20833333333333331, 0.15000000000000002, 0.09166666666666667, 0.03333333333333333, 3.202]\n",
      "len 100, sum 24.268666666666668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage:\n",
    "speed_stop=0.5\n",
    "#speed_distribution = [(40, 1/30, speed_stop), (40, speed_stop, speed_stop), (19, speed_stop, 1/30)]\n",
    "speed_distribution = [(40, 1/60, 1/60), (30,1/30, speed_stop),(20, speed_stop, speed_stop), (9, speed_stop, 1/30)]\n",
    "result1 = generate_evenly_distributed_values(speed_distribution)\n",
    "result1.append(3.202)\n",
    "print(f\"Result for data1: {result1}\")\n",
    "print(f\"len {len(result1)}, sum {sum(result1)}\" )\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef204e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_image(\n",
    "    image,\n",
    "    text: str,\n",
    "    org: tuple[int, int] = (10, 30),  # Bottom-left corner of the text string\n",
    "    font_face: int = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    font_scale: float = 0.8,\n",
    "    color: tuple[int, int, int] = (255,255,255),  # BGR color (Black by default)\n",
    "    thickness: int = 2,\n",
    "    background_color: tuple[int, int, int] = (0, 0, 0) # White background for new canvas\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds text to an image, extending the image size if the text falls outside\n",
    "    the original boundaries.\n",
    "\n",
    "    Args:\n",
    "        image: The input image. Can be an OpenCV (numpy.ndarray) or Pillow (PIL.Image.Image) image.\n",
    "        text (str): The text string to add.\n",
    "        org (tuple[int, int]): The bottom-left corner of the text string in (x, y) coordinates.\n",
    "                               Defaults to (10, 30).\n",
    "        font_face (int): Font type. See cv2.FONT_HERSHEY_* for options.\n",
    "                         Defaults to cv2.FONT_HERSHEY_SIMPLEX.\n",
    "        font_scale (float): Font scale factor multiplied by the font-specific base size.\n",
    "                            Defaults to 1.0.\n",
    "        color (tuple[int, int, int]): Text color in BGR format. Defaults to (0, 0, 0) (Black).\n",
    "        thickness (int): Thickness of the text lines. Defaults to 2.\n",
    "        background_color (tuple[int, int, int]): Color to fill the extended canvas if the image\n",
    "                                                  needs to be resized. Defaults to (255, 255, 255) (White).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with the added text, in OpenCV (BGR) format.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Handle Image Input: Convert Pillow image to OpenCV format if necessary\n",
    "    if isinstance(image, Image.Image):\n",
    "        # Convert PIL image to NumPy array (OpenCV format - BGR)\n",
    "        # PIL uses RGB, OpenCV uses BGR, so convert color channels\n",
    "        img_np = np.array(image)\n",
    "        if img_np.ndim == 2: # Grayscale image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
    "        elif img_np.shape[2] == 4: # RGBA image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR)\n",
    "        else: # RGB image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        img_cv = image\n",
    "        # Ensure the image is BGR (3 channels) if it's grayscale\n",
    "        if img_cv.ndim == 2:\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_GRAY2BGR)\n",
    "        elif img_cv.shape[2] == 4: # Handle RGBA if passed as numpy array\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGBA2BGR)\n",
    "    else:\n",
    "        raise TypeError(\"Input image must be a PIL Image or a NumPy array (OpenCV format).\")\n",
    "\n",
    "    # Get original image dimensions\n",
    "    h_orig, w_orig = img_cv.shape[:2]\n",
    "\n",
    "    # 2. Calculate Text Size\n",
    "    # getTextSize returns ((width, height), baseline)\n",
    "    # text_w is the width of the text string\n",
    "    # text_h is the height of the text string (from top of ascenders to bottom of descenders)\n",
    "    # baseline is the distance from the bottom-left point (org) to the baseline of the text.\n",
    "    (text_w, text_h), baseline = cv2.getTextSize(text, font_face, font_scale, thickness)\n",
    "\n",
    "    # Initialize new dimensions and offsets for the original image\n",
    "    new_w, new_h = w_orig, h_orig\n",
    "    offset_x, offset_y = 0, 0\n",
    "    current_org_x, current_org_y = org[0], org[1]\n",
    "\n",
    "    # 3. Determine New Image Dimensions (if text is outside)\n",
    "\n",
    "    # Check for left extension (text starts before x=0)\n",
    "    if current_org_x < 0:\n",
    "        offset_x = -current_org_x\n",
    "        new_w += offset_x\n",
    "        current_org_x = 0 # Adjust text origin for the new canvas\n",
    "\n",
    "    # Check for top extension (text top edge is above y=0)\n",
    "    # The top of the text is at org[1] - text_h\n",
    "    if current_org_y - text_h < 0:\n",
    "        offset_y = -(current_org_y - text_h)\n",
    "        new_h += offset_y\n",
    "        current_org_y = text_h # Adjust text origin for the new canvas, so its top is at y=0\n",
    "\n",
    "    # Check for right extension (text extends beyond original width)\n",
    "    if current_org_x + text_w > new_w:\n",
    "        new_w = current_org_x + text_w\n",
    "\n",
    "    # Check for bottom extension (text extends beyond original height)\n",
    "    # The bottom of the text is at org[1]\n",
    "    # We also need to consider the baseline for descenders if text goes below org[1]\n",
    "    # The effective bottom of the text is org[1] + baseline\n",
    "    if current_org_y + baseline > new_h:\n",
    "        new_h = current_org_y + baseline\n",
    "\n",
    "    # 4. Create new canvas if needed and paste original image\n",
    "    if new_w > w_orig or new_h > h_orig:\n",
    "        # Create a new blank canvas with the background color\n",
    "        new_image_canvas = np.full((new_h, new_w, 3), background_color, dtype=np.uint8)\n",
    "\n",
    "        # Paste the original image onto the new canvas at the calculated offset\n",
    "        new_image_canvas[offset_y : offset_y + h_orig, offset_x : offset_x + w_orig] = img_cv\n",
    "        img_cv = new_image_canvas\n",
    "    \n",
    "    # Update the org coordinates to reflect the new canvas if it was extended\n",
    "    # This is important because putText will draw relative to the current image's top-left\n",
    "    adjusted_org = (current_org_x, current_org_y)\n",
    "\n",
    "    # 5. Place Text\n",
    "    cv2.putText(img_cv, text, adjusted_org, font_face, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    # 6. Return Image\n",
    "    return img_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70851b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to output_video1.mp4...\n",
      "Moviepy - Building video output_video1.mp4.\n",
      "MoviePy - Writing audio in output_video1TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video1.mp4\n",
      "Video saved successfully and temporary file removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "editor = VideoEditor(fps=30, frame_size=(512, 512))\n",
    "#generator.save_images(images, directory=\"generated\")\n",
    "#editor.add_images_from_list(images, duration_sec=30)\n",
    "if(len(result1) != len(images   )):\n",
    "    print(\"time_duration and number of images are not matched.\")\n",
    "logo=Image.open(str(assts_dir/\"AiArtStudio.AILogo.png\"))\n",
    "editor.add_image(logo, 3)  # Add logo for 3 seconds\n",
    "\n",
    "editor.set_global_display_time_offset_start(0)  # Set global offset to 0 seconds\n",
    "for index, duration in enumerate(result1):\n",
    "   img = images[index] \n",
    "   img1=add_text_to_image(img, f\"Guess what AI painting now?!\",org=(5,535))\n",
    "   editor.add_image(img1, duration,show_time_text=True,time_display_format=\"SS.MS\",time_text_position=(400, 505),time_text_font_scale=0.8)\n",
    "\n",
    "img=add_text_to_image(images[-1], \"Time Up, It is a:\",org=(5,250))\n",
    "img=add_text_to_image(img, prompt,org=(240,350),color=(0, 0, 255), font_scale=1.2, thickness=3)\n",
    "editor.add_image(img, 5,show_time_text=False)\n",
    "editor.add_audio(str(assts_dir/\"Long Distance.mp3\"),audio_clip_end=editor.get_video_duration(),video_start_offset=0)  # Add audio starting at the beginning of the video\n",
    "#editor.add_image(images[-1],3)  # Add last image for 3 seconds\n",
    "editor.save(\"output_video1.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
