{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bce612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Require Python 3.12.3\n",
    "#%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f53fc",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdbb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "from transformers import CLIPFeatureExtractor\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0eaa39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edc07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assts_dir= Path(os.getcwd())/\"assets\"\n",
    "propmt_dir = Path(os.getcwd())/\"Prompts\"\n",
    "GDrive_dir = Path(\"run/user/1000/gvfs/google-drive:host=gmail.com,user=aiartstudio.ai/0AOT4cSJ5oKlpUk9PVA/1cBJcIkDKKJziO4CPcNyoIBUOQ6n_MshJ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee1f25",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f59686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConceptPromptGenerator:\n",
    "    def __init__(self, model_name=\"microsoft/Phi-4-reasoning-plus\", max_new_tokens=300, device=None):\n",
    "        self.device = device if device is not None else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=self.device,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "\n",
    "        )\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def generate(self, concept: str, category: str) -> dict:\n",
    "        few_shot = (\n",
    "            \"You are an expert in AI art prompting and factual summaries.\\n\"\n",
    "            \"Given a concept and its category, generate:\\n\"\n",
    "            \"1. A vivid prompt for Stable Diffusion 1.5\\n\"\n",
    "            \"2. A negative prompt to avoid rendering issues\\n\"\n",
    "            \"3. A short (around 150 characters) catchy, interesting and educational fact about the concept\\n\\n\"\n",
    "            \"Example:\\n\"\n",
    "            \"Concept: dog\\n\"\n",
    "            \"Category: animal\\n\"\n",
    "            \"Prompt: a happy golden retriever playing in a field, photorealistic, warm sunlight, detailed fur, 4k, realistic anatomy\\n\"\n",
    "            \"Negative Prompt: blurry, extra limbs, distorted, low quality, overexposed, unrealistic eyes\\n\"\n",
    "            \"Fact: Dogs bark to communicate\\n\\n\"\n",
    "            f\"Concept: {concept}\\n\"\n",
    "            f\"Category: {category}\\n\"\n",
    "            \"Prompt:\"\n",
    "        )\n",
    "\n",
    "        output = self.generator(\n",
    "            few_shot,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        # Parse output from where the concept starts\n",
    "        output = output.split(f\"Concept: {concept}\")[1]\n",
    "\n",
    "        result = {\"Prompt\": \"\", \"Negative Prompt\": \"\", \"Fact\": \"\"}\n",
    "        for line in output.splitlines():\n",
    "            if line.startswith(\"Prompt:\"):\n",
    "                result[\"Prompt\"] = line[len(\"Prompt:\"):].strip()\n",
    "            elif line.startswith(\"Negative Prompt:\"):\n",
    "                result[\"Negative Prompt\"] = line[len(\"Negative Prompt:\"):].strip()\n",
    "            elif line.startswith(\"Fact:\"):\n",
    "                result[\"Fact\"] = line[len(\"Fact:\"):].strip()\n",
    "            if all(result.values()):\n",
    "                break\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2713a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoEditor:\n",
    "    def __init__(self, fps=30, fourcc='mp4v', frame_size=(640, 480)):\n",
    "        self.fps = fps\n",
    "        self.frame_size = frame_size\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*fourcc)\n",
    "        self.temp_video_path = tempfile.mktemp(suffix='.mp4')\n",
    "        self.video_writer = cv2.VideoWriter(self.temp_video_path, self.fourcc, self.fps, self.frame_size)\n",
    "        self.current_time = 0  # in seconds, tracks the actual current duration of the video content\n",
    "        self.audio_clips = []\n",
    "\n",
    "        # Default settings for time text overlay\n",
    "        self.default_show_time_text = False\n",
    "        self.default_time_text_position = (50, 50)  # Default top-left corner\n",
    "        self.default_time_text_color = (0, 255, 0)  # Default Green (B, G, R)\n",
    "        self.default_time_text_font_scale = 1.0\n",
    "        self.default_time_text_thickness = 2\n",
    "        self.default_time_text_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        self.default_time_display_format = \"HH:MM:SS.MS\"\n",
    "\n",
    "        # New class-level parameter for global display time offset\n",
    "        # If None, the displayed time will default to the actual video current_time.\n",
    "        # If set, it overrides the default behavior for all subsequent additions\n",
    "        # where display_time_start_offset is not explicitly provided.\n",
    "        self.global_display_time_offset_start = None\n",
    "        # New: Counter for the displayed time when global_display_time_offset_start is active\n",
    "        self.display_time_counter = 0.0\n",
    "\n",
    "    def set_global_display_time_offset_start(self, offset_time):\n",
    "        \"\"\"\n",
    "        Sets a global offset for the time displayed on the video.\n",
    "        Any subsequent images added with show_time_text enabled and without\n",
    "        an explicit display_time_start_offset will use this value as their base.\n",
    "\n",
    "        Args:\n",
    "            offset_time (float): The starting time (in seconds) to display on the video.\n",
    "                                 Set to None to revert to using the actual video's current_time.\n",
    "        \"\"\"\n",
    "        if not isinstance(offset_time, (int, float)) and offset_time is not None:\n",
    "            raise TypeError(\"offset_time must be a number (int or float) or None.\")\n",
    "        if offset_time is not None and offset_time < 0:\n",
    "            raise ValueError(\"offset_time cannot be negative.\")\n",
    "        self.global_display_time_offset_start = offset_time\n",
    "        # Initialize the display_time_counter when the global offset is set\n",
    "        self.display_time_counter = offset_time if offset_time is not None else 0.0\n",
    "\n",
    "\n",
    "    def set_default_time_display_format(self, format_string):\n",
    "        \"\"\"\n",
    "        Sets the default format for displaying time on frames.\n",
    "\n",
    "        Args:\n",
    "            format_string (str): The desired format string using placeholders:\n",
    "                                 HH (hours), MM (minutes), SS (seconds), MS (milliseconds).\n",
    "                                 Placeholders are case-insensitive (e.g., \"hh\", \"mm\", \"ss\", \"ms\" also work).\n",
    "                                 Example: \"HH:MM:SS.MS\", \"ss.ms\", \"MM:SS\".\n",
    "        \"\"\"\n",
    "        self.default_time_display_format = format_string\n",
    "\n",
    "    def _resize_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Resizes an image frame to the video's frame_size.\n",
    "        \"\"\"\n",
    "        return cv2.resize(frame, self.frame_size)\n",
    "\n",
    "    def _convert_to_cv2(self, image):\n",
    "        \"\"\"\n",
    "        Converts various image types (path, Pillow, NumPy array) to an OpenCV image (NumPy array)\n",
    "        and resizes it.\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            img = cv2.imread(image)\n",
    "            if img is None:\n",
    "                raise FileNotFoundError(f\"Image file not found or could not be read: {image}\")\n",
    "        elif isinstance(image, Image.Image):\n",
    "            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            img = image\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image type. Must be a file path (string), Pillow Image, or NumPy array.\")\n",
    "        return self._resize_frame(img)\n",
    "\n",
    "    def _draw_time_on_frame(self, frame, time_in_sec, position, color, font_scale, thickness, font, time_format):\n",
    "        \"\"\"\n",
    "        Draws the current time onto a video frame with a specified format.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): The OpenCV image frame.\n",
    "            time_in_sec (float): The time in seconds to display.\n",
    "            position (tuple): (x, y) coordinates for the text.\n",
    "            color (tuple): (B, G, R) color for the text.\n",
    "            font_scale (float): Font scale factor.\n",
    "            thickness (int): Line thickness for the text.\n",
    "            font (int): OpenCV font type (e.g., cv2.FONT_HERSHEY_SIMPLEX).\n",
    "            time_format (str): The format string using placeholders (HH, MM, SS, MS).\n",
    "                               Placeholders are case-insensitive.\n",
    "        \"\"\"\n",
    "        total_seconds_int = int(time_in_sec)\n",
    "        milliseconds = int((time_in_sec - total_seconds_int) * 10)\n",
    "        seconds = total_seconds_int % 60\n",
    "        minutes = (total_seconds_int // 60) % 60\n",
    "        hours = total_seconds_int // 3600\n",
    "\n",
    "        replacements = {\n",
    "            \"HH\": f\"{hours:02}\", \"hh\": f\"{hours:02}\",\n",
    "            \"MM\": f\"{minutes:02}\", \"mm\": f\"{minutes:02}\",\n",
    "            \"SS\": f\"{seconds:02}\", \"ss\": f\"{seconds:02}\",\n",
    "            \"MS\": f\"{milliseconds:01}\", \"ms\": f\"{milliseconds:01}\"\n",
    "        }\n",
    "\n",
    "        time_str = time_format\n",
    "        for placeholder, value in replacements.items():\n",
    "            time_str = time_str.replace(placeholder, value)\n",
    "\n",
    "        cv2.putText(frame, time_str, position, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        return frame\n",
    "\n",
    "    def add_image(self, image, duration_sec,\n",
    "                  show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                  time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                  display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds a single image to the video for a specified duration, with optional time display.\n",
    "\n",
    "        Args:\n",
    "            image: The image to add. Can be a file path (string), a Pillow Image object,\n",
    "                   or an OpenCV image (NumPy array).\n",
    "            duration_sec (float): The duration (in seconds) for which the image should be displayed.\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "                                              Defaults to self.default_show_time_text.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the text.\n",
    "                                                  Defaults to self.default_time_text_position.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the text.\n",
    "                                               Defaults to self.default_time_text_color.\n",
    "            time_text_font_scale (float, optional): Font scale factor.\n",
    "                                                    Defaults to self.default_time_text_font_scale.\n",
    "            time_text_thickness (int, optional): Line thickness for the text.\n",
    "                                                 Defaults to self.default_time_text_thickness.\n",
    "            time_text_font (int, optional): OpenCV font type.\n",
    "                                            Defaults to self.default_time_text_font.\n",
    "            display_time_start_offset (float, optional): The starting time (in seconds) to display\n",
    "                                                         on the video for this segment.\n",
    "                                                         Precedence: local param > global param > actual video current time.\n",
    "            time_display_format (str, optional): The format for the time string (e.g., \"HH:MM:SS.MS\").\n",
    "                                                 Placeholders are case-insensitive.\n",
    "                                                 Precedence: local param > default class param.\n",
    "        \"\"\"\n",
    "        frame = self._convert_to_cv2(image)\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "\n",
    "        # Resolve text display parameters\n",
    "        _show_text = self.default_show_time_text if show_time_text is None else show_time_text\n",
    "        _position = self.default_time_text_position if time_text_position is None else time_text_position\n",
    "        _color = self.default_time_text_color if time_text_color is None else time_text_color\n",
    "        _font_scale = self.default_time_text_font_scale if time_text_font_scale is None else time_text_font_scale\n",
    "        _thickness = self.default_time_text_thickness if time_text_thickness is None else time_text_thickness\n",
    "        _font = self.default_time_text_font if time_text_font is None else time_text_font\n",
    "        _time_format = self.default_time_display_format if time_display_format is None else time_display_format\n",
    "\n",
    "        # Determine the base time for display based on precedence\n",
    "        _base_display_time_for_this_segment = self.current_time # Default fallback: actual video time\n",
    "\n",
    "        if display_time_start_offset is not None:\n",
    "            # Local override takes highest precedence\n",
    "            _base_display_time_for_this_segment = display_time_start_offset\n",
    "        elif self.global_display_time_offset_start is not None:\n",
    "            # Global override applies if no local override. Use the continuous display_time_counter.\n",
    "            _base_display_time_for_this_segment = self.display_time_counter\n",
    "\n",
    "        for i in range(frame_count):\n",
    "            # Calculate the time to display for the current frame\n",
    "            current_display_time = _base_display_time_for_this_segment + (i / self.fps)\n",
    "            frame_to_write = frame.copy() # Use a copy to avoid drawing on the original frame for next iteration\n",
    "            if _show_text:\n",
    "                frame_to_write = self._draw_time_on_frame(\n",
    "                    frame_to_write, current_display_time, _position, _color, _font_scale, _thickness, _font, _time_format\n",
    "                )\n",
    "            self.video_writer.write(frame_to_write)\n",
    "\n",
    "        # Always update the actual video time\n",
    "        self.current_time += duration_sec\n",
    "\n",
    "        # Only update display_time_counter if global offset is active AND no local offset was used\n",
    "        if display_time_start_offset is None and self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration_sec\n",
    "\n",
    "\n",
    "    def add_images_from_list(self, images, total_duration_sec,\n",
    "                             show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                             time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                             display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds several images to the video, distributing them evenly over a total duration,\n",
    "        with optional time display on each image.\n",
    "\n",
    "        Args:\n",
    "            images: Can be a string (directory path), a list of strings (image file paths),\n",
    "                    a list of OpenCV images (numpy arrays), or a list of Pillow images.\n",
    "            total_duration_sec (float): The total duration (in seconds) that these images\n",
    "                                        should occupy in the video.\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "                                              Defaults to self.default_show_time_text.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the text.\n",
    "                                                  Defaults to self.default_time_text_position.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the text.\n",
    "                                               Defaults to self.default_time_text_color.\n",
    "            time_text_font_scale (float, optional): Font scale factor.\n",
    "                                                    Defaults to self.default_time_text_font_scale.\n",
    "            time_text_thickness (int, optional): Line thickness for the text.\n",
    "                                                 Defaults to self.default_time_text_thickness.\n",
    "            time_text_font (int, optional): OpenCV font type.\n",
    "                                            Defaults to self.default_time_text_font.\n",
    "            display_time_start_offset (float, optional): The starting time (in seconds) to display\n",
    "                                                         on the video for this segment.\n",
    "                                                         Precedence: local param > global param > actual video current time.\n",
    "            time_display_format (str, optional): The format for the time string (e.g., \"HH:MM:SS.MS\").\n",
    "                                                 Placeholders are case-insensitive.\n",
    "                                                 Precedence: local param > default class param.\n",
    "        \"\"\"\n",
    "        image_list = []\n",
    "\n",
    "        if isinstance(images, str) and os.path.isdir(images):\n",
    "            for filename in sorted(os.listdir(images)):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    image_list.append(os.path.join(images, filename))\n",
    "        elif isinstance(images, list):\n",
    "            if all(isinstance(img, str) for img in images):\n",
    "                image_list = images\n",
    "            elif all(isinstance(img, (np.ndarray, Image.Image)) for img in images):\n",
    "                image_list = images\n",
    "            else:\n",
    "                raise ValueError(\"List must contain only strings (paths), OpenCV images, or Pillow images.\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported 'images' type. Must be a directory path (string), a list of paths, a list of OpenCV images, or a list of Pillow images.\")\n",
    "\n",
    "        if not image_list:\n",
    "            print(\"No images found to add.\")\n",
    "            return\n",
    "\n",
    "        single_image_duration = total_duration_sec / len(image_list)\n",
    "\n",
    "        for i, img in enumerate(image_list):\n",
    "            # The display_time_start_offset passed to add_image needs to be carefully managed.\n",
    "            # If the user provided a display_time_start_offset to add_images_from_list,\n",
    "            # we calculate the offset for each individual image within that batch.\n",
    "            # If not, we pass None, allowing add_image to use the global_display_time_offset_start\n",
    "            # and its internal display_time_counter, which is the desired behavior for continuous time.\n",
    "            effective_display_offset_for_image_batch = None\n",
    "            if display_time_start_offset is not None:\n",
    "                effective_display_offset_for_image_batch = display_time_start_offset + (i * single_image_duration)\n",
    "\n",
    "            self.add_image(img, single_image_duration,\n",
    "                           show_time_text=show_time_text,\n",
    "                           time_text_position=time_text_position,\n",
    "                           time_text_color=time_text_color,\n",
    "                           time_text_font_scale=time_text_font_scale,\n",
    "                           time_text_thickness=time_text_thickness,\n",
    "                           time_text_font=time_text_font,\n",
    "                           display_time_start_offset=effective_display_offset_for_image_batch, # Pass the calculated offset or None\n",
    "                           time_display_format=time_display_format)\n",
    "\n",
    "    def add_text_below_image(self, image, text, duration_sec,\n",
    "                             text_box_height_ratio=0.2,\n",
    "                             background_color=(0, 0, 0),\n",
    "                             text_color=(255, 255, 255),\n",
    "                             text_horizontal_alignment=\"center\",\n",
    "                             text_vertical_alignment=\"center\",\n",
    "                             font_scale=0.7, thickness=2, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                             show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                             time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                             display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds an image with text displayed in a box below it for a given duration.\n",
    "        The combined content fits within the video's frame_size.\n",
    "\n",
    "        Args:\n",
    "            image: The input image (path, Pillow, or NumPy array).\n",
    "            text (str): The text string to display.\n",
    "            duration_sec (float): The duration (in seconds) for which this combined frame should be displayed.\n",
    "            text_box_height_ratio (float): Ratio (0.0 to 1.0) of the frame height allocated to the text box.\n",
    "            background_color (tuple): (B, G, R) color for the text box background.\n",
    "            text_color (tuple): (B, G, R) color for the text.\n",
    "            text_horizontal_alignment (str): Horizontal alignment of text (\"left\", \"center\", \"right\").\n",
    "            text_vertical_alignment (str): Vertical alignment of text (\"top\", \"center\", \"bottom\").\n",
    "            font_scale (float): Font scale factor for the text.\n",
    "            thickness (int): Line thickness for the text.\n",
    "            font (int): OpenCV font type (e.g., cv2.FONT_HERSHEY_SIMPLEX).\n",
    "            # Parameters for optional time display (same as add_image)\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the time text.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the time text.\n",
    "            time_text_font_scale (float, optional): Font scale factor for time text.\n",
    "            time_text_thickness (int, optional): Line thickness for time text.\n",
    "            time_text_font (int, optional): OpenCV font type for time text.\n",
    "            display_time_start_offset (float, optional): The starting time to display.\n",
    "            time_display_format (str, optional): The format for the time string.\n",
    "        \"\"\"\n",
    "        if not (0.0 <= text_box_height_ratio <= 1.0):\n",
    "            raise ValueError(\"text_box_height_ratio must be between 0.0 and 1.0.\")\n",
    "\n",
    "        original_img_frame = self._convert_to_cv2(image)\n",
    "        \n",
    "        video_width, video_height = self.frame_size\n",
    "        text_box_height = int(video_height * text_box_height_ratio)\n",
    "        image_display_height = video_height - text_box_height\n",
    "\n",
    "        if image_display_height <= 0:\n",
    "            raise ValueError(\"Image display height is zero or negative. Reduce text_box_height_ratio or increase frame_size.\")\n",
    "\n",
    "        resized_img_for_top = cv2.resize(original_img_frame, (video_width, image_display_height))\n",
    "\n",
    "        text_section = np.full((text_box_height, video_width, 3), background_color, dtype=np.uint8)\n",
    "\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        text_width, text_height_actual = text_size\n",
    "\n",
    "        text_x, text_y = 0, 0\n",
    "\n",
    "        if text_horizontal_alignment.lower() == \"center\":\n",
    "            text_x = (video_width - text_width) // 2\n",
    "        elif text_horizontal_alignment.lower() == \"right\":\n",
    "            text_x = video_width - text_width - 10\n",
    "        else: # Default to \"left\"\n",
    "            text_x = 10\n",
    "\n",
    "        if text_vertical_alignment.lower() == \"center\":\n",
    "            text_y = (text_box_height + text_height_actual) // 2\n",
    "        elif text_vertical_alignment.lower() == \"bottom\":\n",
    "            text_y = text_box_height - 10\n",
    "        else: # Default to \"top\"\n",
    "            text_y = text_height_actual + 10\n",
    "\n",
    "        cv2.putText(text_section, text, (text_x, text_y), font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        final_frame = np.vstack((resized_img_for_top, text_section))\n",
    "\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "\n",
    "        _show_text = self.default_show_time_text if show_time_text is None else show_time_text\n",
    "        _time_position = self.default_time_text_position if time_text_position is None else time_text_position\n",
    "        _time_color = self.default_time_text_color if time_text_color is None else time_text_color\n",
    "        _time_font_scale = self.default_time_text_font_scale if time_text_font_scale is None else time_text_font_scale\n",
    "        _time_thickness = self.default_time_text_thickness if time_text_thickness is None else time_text_thickness\n",
    "        _time_font = self.default_time_text_font if time_text_font is None else time_text_font\n",
    "        _time_format = self.default_time_display_format if time_display_format is None else time_display_format\n",
    "\n",
    "        _base_display_time_for_this_segment = self.current_time\n",
    "        if display_time_start_offset is not None:\n",
    "            _base_display_time_for_this_segment = display_time_start_offset\n",
    "        elif self.global_display_time_offset_start is not None:\n",
    "            _base_display_time_for_this_segment = self.display_time_counter\n",
    "\n",
    "\n",
    "        for i in range(frame_count):\n",
    "            current_display_time = _base_display_time_for_this_segment + (i / self.fps)\n",
    "            frame_to_write = final_frame.copy()\n",
    "            if _show_text:\n",
    "                frame_to_write = self._draw_time_on_frame(\n",
    "                    frame_to_write, current_display_time, _time_position, _time_color, _time_font_scale, _time_thickness, _time_font, _time_format\n",
    "                )\n",
    "            self.video_writer.write(frame_to_write)\n",
    "        \n",
    "        self.current_time += duration_sec\n",
    "\n",
    "        if display_time_start_offset is None and self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration_sec\n",
    "\n",
    "    def add_video(self, video_path):\n",
    "        \"\"\"\n",
    "        Adds another video clip to the current video. The added video retains its original duration.\n",
    "\n",
    "        Args:\n",
    "            video_path (str): Path to the video file to be added.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise FileNotFoundError(f\"Video file not found or could not be opened: {video_path}\")\n",
    "\n",
    "        video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if video_fps == 0:\n",
    "            video_fps = self.fps\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = self._resize_frame(frame)\n",
    "            self.video_writer.write(frame)\n",
    "\n",
    "        duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / video_fps\n",
    "        cap.release()\n",
    "        self.current_time += duration\n",
    "        # If global display offset is active, and we are adding a video (which doesn't have its own\n",
    "        # display_time_start_offset parameter), we should also advance the display_time_counter.\n",
    "        if self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration\n",
    "\n",
    "\n",
    "    def add_audio(self, audio_path, audio_clip_start=None, audio_clip_end=None, video_start_offset=None):\n",
    "        \"\"\"\n",
    "        Adds an audio clip to the video timeline.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "            audio_clip_start (float, optional): The start time (in seconds) within the audio file itself.\n",
    "                                                Defaults to 0 (beginning of the audio file).\n",
    "            audio_clip_end (float, optional): The end time (in seconds) within the audio file itself.\n",
    "                                              Defaults to the end of the audio clip.\n",
    "            video_start_offset (float, optional): The time (in seconds) on the video timeline where this\n",
    "                                                  audio should start. If None, it starts at the current\n",
    "                                                  end time of the video (`self.current_time`).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not load audio file {audio_path}: {e}\")\n",
    "\n",
    "        if audio_clip_start is not None or audio_clip_end is not None:\n",
    "            if audio_clip_start is not None and audio_clip_end is not None and audio_clip_start > audio_clip_end:\n",
    "                raise ValueError(\"audio_clip_start cannot be greater than audio_clip_end.\")\n",
    "            \n",
    "            start_subclip = audio_clip_start if audio_clip_start is not None else 0\n",
    "            end_subclip = audio_clip_end if audio_clip_end is not None else audio_clip.duration\n",
    "            \n",
    "            audio_clip = audio_clip.subclip(start_subclip, end_subclip)\n",
    "\n",
    "        offset_on_video = video_start_offset if video_start_offset is not None else self.current_time\n",
    "\n",
    "        self.audio_clips.append((audio_clip, offset_on_video))\n",
    "\n",
    "    def get_video_duration(self):\n",
    "        \"\"\"\n",
    "        Returns the current duration of the video content in seconds.\n",
    "        \"\"\"\n",
    "        return self.current_time\n",
    "\n",
    "    def save(self, output_path):\n",
    "        \"\"\"\n",
    "        Finalizes the video and merges audio if present.\n",
    "        \"\"\"\n",
    "        self.video_writer.release()\n",
    "\n",
    "        final_clip = VideoFileClip(self.temp_video_path)\n",
    "\n",
    "        if self.audio_clips:\n",
    "            all_audios = []\n",
    "            for audio, offset in self.audio_clips:\n",
    "                all_audios.append(audio.set_start(offset))\n",
    "            \n",
    "            composite_audio = CompositeAudioClip(all_audios)\n",
    "            \n",
    "            final_clip = final_clip.set_audio(composite_audio)\n",
    "\n",
    "        print(f\"Saving video to {output_path}...\")\n",
    "        final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "        final_clip.close()\n",
    "        os.remove(self.temp_video_path)\n",
    "        print(\"Video saved successfully and temporary file removed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de60dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SD15ImageGenerator:\n",
    "    def __init__(self, model_id=\"runwayml/stable-diffusion-v1-5\", use_cuda=True, num_inference_steps=25):\n",
    "        \"\"\"\n",
    "        Initialize the Stable Diffusion 1.5 pipeline and inference settings.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.intermediate_images = []\n",
    "\n",
    "        # Load the safety checker and feature extractor\n",
    "        # You might need to specify the subfolder if they are not at the top level of the model_id\n",
    "        safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        feature_extractor = CLIPFeatureExtractor.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        self.negative_prompt = \"\"\"deformed, distorted, disfigured, bad anatomy, ugly, tiling, poorly drawn hands, poorly drawn face, \n",
    "                                  out of frame, low quality, jpeg artifacts, duplicate, morbid, mutilated, extra fingers, mutated hands,  mutation, blurry, dehydrated, \n",
    "                                  bad proportions, extra limbs, cloned face, gross proportions, malformed limbs, missing arms, missing legs, extra hands, fused fingers, wrong hand, \n",
    "                                  long neck, worst quality, watermark, signature, text, error, cropped, username, logo, lowres, oversaturated, washed out, \n",
    "                                  cloned, bad composition, crosseyed , squint, lazy eye , bad eyes, wrong eyes, missing teeth, bad teeth, ugly teeth, open mouth, too many teeth,\n",
    "                                  extra tongue, wrong mouth, ugly mouth, bad mouth, bad nose, ugly nose, wrong nose, missing nose, bad ear, ugly ear, wrong ear, missing ear,\n",
    "                                  extra ear, double ear, three ears, mutated ear, long ear, short ear, big ear, small ear,\n",
    "                                  bad hair, ugly hair, wrong hair, missing hair, bad skin, ugly skin, wrong skin, \n",
    "                                  missing skin, extra skin, mutated skin, bad clothing, ugly clothing, wrong clothing,missing clothing, mutated clothing, \n",
    "                                  big clothing, small clothing, bad background, ugly background, wrong background, bad lighting, ugly lighting, wrong lighting\"\"\"\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "             safety_checker=safety_checker,\n",
    "            feature_extractor=feature_extractor # Don't forget the feature_extractor\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _capture_step(self, step, timestep, latents):\n",
    "        \"\"\"\n",
    "        Internal callback to capture the image at each step.\n",
    "        \"\"\"\n",
    "        # Decode latent to image at this step\n",
    "        with torch.no_grad():\n",
    "            image = self.pipe.vae.decode(latents / self.pipe.vae.config.scaling_factor).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image_pil = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "            self.intermediate_images.append(image_pil)\n",
    "\n",
    "    def generate_image(self, prompt, negative_prompt=None, guidance_scale=7.5):\n",
    "        \"\"\"\n",
    "        Generate image and collect intermediate steps.\n",
    "        Returns a list of PIL images (one per step).\n",
    "        \"\"\"\n",
    "        self.intermediate_images = []\n",
    "        negative_prompt = negative_prompt or self.negative_prompt\n",
    "\n",
    "        with torch.autocast(self.device) if self.device == \"cuda\" else torch.no_grad():\n",
    "            _ = self.pipe(\n",
    "                prompt=\"high resolution image of: \"+prompt +\" ,8K, best quality, masterpiece, photorealistic, ultra-detailed, sharp focus\",\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=self.num_inference_steps,\n",
    "                callback=self._capture_step,\n",
    "                callback_steps=1  # capture every step\n",
    "            )\n",
    "\n",
    "        return self.intermediate_images\n",
    "\n",
    "    def save_image(self, image: Image.Image, output_path: str):\n",
    "        \"\"\"\n",
    "        Save a single PIL image to the specified path.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        image.save(output_path)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "    def save_images(self, images, directory=\"generated\"):\n",
    "        \"\"\"\n",
    "        Save a list of images to the given directory.\n",
    "        \"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        for i, img in enumerate(images):\n",
    "            path = os.path.join(directory, f\"step_{i:02d}.png\")\n",
    "            img.save(path)\n",
    "        print(f\"Saved {len(images)} images to '{directory}/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7fea56",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5f139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_evenly_distributed_values(data):\n",
    "    \"\"\"\n",
    "    Generates evenly distributed values for each tuple (number, start, end) in a list.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of tuples, where each tuple is (number, start, end).\n",
    "                     'number' is the count of values to generate, 'start' is the\n",
    "                     beginning of the range, and 'end' is the end of the range.\n",
    "\n",
    "    Returns:\n",
    "        list: A single list containing all the generated evenly distributed values.\n",
    "    \"\"\"\n",
    "    all_values = []\n",
    "    for num, start, end in data:\n",
    "        # Generate 'num' evenly distributed values between 'start' and 'end'\n",
    "        # np.linspace includes both start and end points\n",
    "        if num > 0:\n",
    "            generated_values = np.linspace(start, end, num).tolist()\n",
    "            all_values.extend(generated_values)\n",
    "    return all_values\n",
    "\n",
    "# Example Usage:\n",
    "# data1 = [(5, 0, 10), (3, 100, 102)]\n",
    "# result1 = generate_evenly_distributed_values(data1)\n",
    "# print(f\"Result for data1: {result1}\")\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]\n",
    "\n",
    "# data2 = [(1, 5, 5), (4, -2, 2)]\n",
    "# result2 = generate_evenly_distributed_values(data2)\n",
    "# print(f\"Result for data2: {result2}\")\n",
    "# # Expected output for data2: [5.0, -2.0, -0.6666666666666666, 0.6666666666666666, 2.0]\n",
    "\n",
    "# data3 = []\n",
    "# result3 = generate_evenly_distributed_values(data3)\n",
    "# print(f\"Result for data3: {result3}\")\n",
    "# # Expected output for data3: []\n",
    "\n",
    "# data4 = [(0, 1, 10)]\n",
    "# result4 = generate_evenly_distributed_values(data4)\n",
    "# print(f\"Result for data4: {result4}\")\n",
    "# # Expected output for data4: []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf975f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_filename_by_datetime(postfix:str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a filename based on the current date and time with a specified extension.\n",
    "\n",
    "    The format of the filename will be 'YYYY-MM-DD-HH-MM-SS.extension'.\n",
    "\n",
    "    Args:\n",
    "        extension (str): The file extension (e.g., 'mp4', 'txt', 'jpg').\n",
    "                         It should not include the leading dot.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated filename.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # Format the datetime object into a string\n",
    "    # YYYY: Year with century\n",
    "    # MM: Month as a zero-padded decimal number\n",
    "    # DD: Day of the month as a zero-padded decimal number\n",
    "    # HH: Hour (24-hour clock) as a zero-padded decimal number\n",
    "    # MM: Minute as a zero-padded decimal number\n",
    "    # SS: Second as a zero-padded decimal number\n",
    "    timestamp_str = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    # Construct the full filename\n",
    "    filename = f\"{timestamp_str}_{postfix}.{extension}\"\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b626cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_text_to_image(\n",
    "    image,\n",
    "    text: str,\n",
    "    org: tuple[int | None, int | None] = (10, 30),  # Bottom-left corner of the text string\n",
    "    font_face: int = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    font_scale: float = 1.0,\n",
    "    color: tuple[int, int, int] = (0, 0, 0),  # BGR color (Black by default)\n",
    "    thickness: int = 2,\n",
    "    background_color: tuple[int, int, int] = (255, 255, 255), # White background for new canvas\n",
    "    text_background_color: tuple[int, int, int] = (255, 255, 255), # White background for text by default\n",
    "    text_background_transparency: float = 0.8, # 80% transparency by default\n",
    "    padding_x: int = 20, # Horizontal padding for text background\n",
    "    padding_y: int = 20, # Vertical padding for text background\n",
    "    wordwrap: bool = False # New parameter: if True, wraps text to fit image width\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds text to an image, extending the image size if the text falls outside\n",
    "    the original boundaries. Supports word wrapping.\n",
    "\n",
    "    Args:\n",
    "        image: The input image. Can be an OpenCV (numpy.ndarray) or Pillow (PIL.Image.Image) image.\n",
    "        text (str): The text string to add.\n",
    "        org (tuple[int | None, int | None]): The bottom-left corner of the text string in (x, y) coordinates.\n",
    "                               Defaults to (10, 30). If x or y is None, it will be centered in that direction.\n",
    "        font_face (int): Font type. See cv2.FONT_HERSHEY_* for options.\n",
    "                         Defaults to cv2.FONT_HERSHEY_SIMPLEX.\n",
    "        font_scale (float): Font scale factor multiplied by the font-specific base size.\n",
    "                            Defaults to 1.0.\n",
    "        color (tuple[int, int, int]): Text color in BGR format. Defaults to (0, 0, 0) (Black).\n",
    "        thickness (int): Thickness of the text lines. Defaults to 2.\n",
    "        background_color (tuple[int, int, int]): Color to fill the extended canvas if the image\n",
    "                                                  needs to be resized. Defaults to (255, 255, 255) (White).\n",
    "        text_background_color (tuple[int, int, int]): Color of the text's background in BGR format.\n",
    "                                                       Defaults to (255, 255, 255) (White).\n",
    "        text_background_transparency (float): Transparency of the text background.\n",
    "                                              Value between 0.0 (fully transparent) and 1.0 (fully opaque).\n",
    "                                              Defaults to 0.8 (80% transparent).\n",
    "        padding_x (int): Horizontal padding to add around the text background. Defaults to 5 pixels.\n",
    "        padding_y (int): Vertical padding to add around the text background. Defaults to 5 pixels.\n",
    "        wordwrap (bool): If True, wraps text to fit within the image width, breaking at spaces.\n",
    "                         Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with the added text, in OpenCV (BGR) format.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Handle Image Input: Convert Pillow image to OpenCV format if necessary\n",
    "    if isinstance(image, Image.Image):\n",
    "        img_np = np.array(image)\n",
    "        if img_np.ndim == 2: # Grayscale image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
    "        elif img_np.shape[2] == 4: # RGBA image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR)\n",
    "        else: # RGB image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        img_cv = image\n",
    "        # Ensure the image is BGR (3 channels) if it's grayscale\n",
    "        if img_cv.ndim == 2:\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_GRAY2BGR)\n",
    "        elif img_cv.shape[2] == 4: # Handle RGBA if passed as numpy array\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGBA2BGR)\n",
    "    else:\n",
    "        raise TypeError(\"Input image must be a PIL Image or a NumPy array (OpenCV format).\")\n",
    "\n",
    "    # Get original image dimensions\n",
    "    h_orig, w_orig = img_cv.shape[:2]\n",
    "\n",
    "    # Calculate representative text height and baseline for a single line.\n",
    "    # This is used for consistent line spacing and overall text block height calculations.\n",
    "    (text_w_dummy, text_h_single_line), baseline_single_line = cv2.getTextSize(\n",
    "        \"Tg\", font_face, font_scale, thickness\n",
    "    )\n",
    "    # A reasonable spacing between lines, often a percentage of the font height.\n",
    "    line_spacing = int(text_h_single_line * 0.5)\n",
    "\n",
    "    # --- Word Wrapping Logic ---\n",
    "    wrapped_lines_info = [] # This list will store tuples of (line_text, line_width, line_height, line_baseline) for each line.\n",
    "    max_overall_text_width = 0 # Stores the width of the widest line\n",
    "    \n",
    "    if wordwrap:\n",
    "        # Determine the maximum available width for wrapping the text within the image.\n",
    "        # This considers the initial x-position and padding.\n",
    "        if org[0] is None: # If horizontally centered, available width is image width minus double padding.\n",
    "            available_width_for_wrapping = w_orig - (2 * padding_x)\n",
    "        else: # If a specific x-coordinate is provided, available width is from that point to the right edge.\n",
    "            available_width_for_wrapping = w_orig - org[0] - padding_x\n",
    "        \n",
    "        # Ensure the available width is not negative or too small to avoid issues.\n",
    "        available_width_for_wrapping = max(10, available_width_for_wrapping)\n",
    "\n",
    "        words = text.split(' ')\n",
    "        current_line_words = []\n",
    "        current_line_text = \"\"\n",
    "\n",
    "        for word in words:\n",
    "            # Construct a test line by adding the current word (with a space if not the first word).\n",
    "            test_line_text = (current_line_text + \" \" + word).strip()\n",
    "            # Get the size of this potential line.\n",
    "            (test_w, _), _ = cv2.getTextSize(\n",
    "                test_line_text, font_face, font_scale, thickness\n",
    "            )\n",
    "\n",
    "            # If adding the word makes the line too long AND there are already words in the current line,\n",
    "            # then the current line is complete and the new word starts a new line.\n",
    "            if test_w > available_width_for_wrapping and len(current_line_words) > 0:\n",
    "                # Calculate the actual size of the completed line.\n",
    "                (line_w, line_h), line_baseline = cv2.getTextSize(\n",
    "                    current_line_text, font_face, font_scale, thickness\n",
    "                )\n",
    "                wrapped_lines_info.append((current_line_text, line_w, line_h, line_baseline))\n",
    "                max_overall_text_width = max(max_overall_text_width, line_w)\n",
    "\n",
    "                # Start a new line with the current word.\n",
    "                current_line_words = [word]\n",
    "                current_line_text = word\n",
    "            else:\n",
    "                # The word fits, so add it to the current line.\n",
    "                current_line_words.append(word)\n",
    "                current_line_text = \" \".join(current_line_words)\n",
    "        \n",
    "        # After the loop, add any remaining text in the current_line_text as the last line.\n",
    "        if current_line_text:\n",
    "            (line_w, line_h), line_baseline = cv2.getTextSize(\n",
    "                current_line_text, font_face, font_scale, thickness\n",
    "            )\n",
    "            wrapped_lines_info.append((current_line_text, line_w, line_h, line_baseline))\n",
    "            max_overall_text_width = max(max_overall_text_width, line_w)\n",
    "\n",
    "    else: # If word wrapping is not enabled, treat the entire text as a single line.\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(text, font_face, font_scale, thickness)\n",
    "        wrapped_lines_info.append((text, text_w, text_h, baseline))\n",
    "        max_overall_text_width = text_w\n",
    "    \n",
    "    # Calculate the total height required by the entire block of wrapped text.\n",
    "    # This is the height from the top of the first line's ascenders to the bottom of the last line's descenders.\n",
    "    total_text_block_content_height = 0\n",
    "    if wrapped_lines_info:\n",
    "        # Top of the first line relative to its baseline (negative value)\n",
    "        first_line_top_offset_from_baseline = -(wrapped_lines_info[0][2] - wrapped_lines_info[0][3])\n",
    "        \n",
    "        # Baseline of the last line relative to the first line's baseline\n",
    "        last_line_baseline_offset_from_first_baseline = 0\n",
    "        if len(wrapped_lines_info) > 1:\n",
    "            last_line_baseline_offset_from_first_baseline = (len(wrapped_lines_info) - 1) * \\\n",
    "                                                              (text_h_single_line + line_spacing)\n",
    "        \n",
    "        # Bottom of the last line relative to its own baseline\n",
    "        last_line_bottom_offset_from_its_baseline = wrapped_lines_info[-1][3]\n",
    "\n",
    "        # Total content height = (last line's baseline + its bottom offset) - (first line's baseline + its top offset)\n",
    "        # We assume the first line's baseline is at y=0 for this calculation of the *span*.\n",
    "        total_text_block_content_height = (last_line_baseline_offset_from_first_baseline + last_line_bottom_offset_from_its_baseline) - \\\n",
    "                                          (first_line_top_offset_from_baseline)\n",
    "\n",
    "    # --- Determine Canvas Extension (Pre-calculation of text block position for extension check) ---\n",
    "    new_w, new_h = w_orig, h_orig\n",
    "    offset_x_canvas, offset_y_canvas = 0, 0\n",
    "\n",
    "    # Calculate the *desired* top-left corner of the text block's content area (without padding)\n",
    "    # relative to the original image's (0,0) if no canvas extension happens.\n",
    "    temp_text_block_x_content_start = org[0] if org[0] is not None else int((w_orig - max_overall_text_width) / 2)\n",
    "    \n",
    "    # temp_text_block_y_content_top represents the Y-coordinate of the *visual top* of the entire text block.\n",
    "    if org[1] is not None: # org[1] is the baseline of the first line\n",
    "        temp_text_block_y_content_top = org[1] + first_line_top_offset_from_baseline\n",
    "    else: # Vertical centering\n",
    "        temp_text_block_y_content_top = int((h_orig - total_text_block_content_height) / 2)\n",
    "\n",
    "    # Calculate bounding box for text block with padding for extension check\n",
    "    padded_x1 = temp_text_block_x_content_start - padding_x\n",
    "    padded_y1 = temp_text_block_y_content_top - padding_y \n",
    "    padded_x2 = temp_text_block_x_content_start + max_overall_text_width + padding_x\n",
    "    padded_y2 = temp_text_block_y_content_top + total_text_block_content_height + padding_y\n",
    "\n",
    "    # Check for left extension\n",
    "    if padded_x1 < 0:\n",
    "        offset_x_canvas = -padded_x1\n",
    "        new_w += offset_x_canvas\n",
    "\n",
    "    # Check for top extension\n",
    "    if padded_y1 < 0:\n",
    "        offset_y_canvas = -padded_y1\n",
    "        new_h += offset_y_canvas\n",
    "\n",
    "    # Recalculate padded coordinates based on potentially adjusted new_w/new_h\n",
    "    # This is needed to check for right/bottom extension against the *potential* new size.\n",
    "    # The new_w/new_h might have increased due to left/top extensions.\n",
    "    # The text block's position on this *potential* new canvas:\n",
    "    current_text_block_x_on_potential_canvas = temp_text_block_x_content_start + offset_x_canvas\n",
    "    current_text_block_y_top_on_potential_canvas = temp_text_block_y_content_top + offset_y_canvas\n",
    "\n",
    "    padded_x2_after_offset = current_text_block_x_on_potential_canvas + max_overall_text_width + padding_x\n",
    "    padded_y2_after_offset = current_text_block_y_top_on_potential_canvas + total_text_block_content_height + padding_y\n",
    "\n",
    "    # Check for right extension\n",
    "    if padded_x2_after_offset > new_w:\n",
    "        new_w = padded_x2_after_offset\n",
    "\n",
    "    # Check for bottom extension\n",
    "    if padded_y2_after_offset > new_h:\n",
    "        new_h = padded_y2_after_offset\n",
    "\n",
    "    # 4. Create new canvas if needed and paste original image.\n",
    "    if new_w > w_orig or new_h > h_orig:\n",
    "        # Create a new blank canvas with the specified background color.\n",
    "        new_image_canvas = np.full((new_h, new_w, 3), background_color, dtype=np.uint8)\n",
    "        # Paste the original image onto the new canvas at the calculated offset.\n",
    "        new_image_canvas[offset_y_canvas : offset_y_canvas + h_orig,\n",
    "                         offset_x_canvas : offset_x_canvas + w_orig] = img_cv\n",
    "        img_cv = new_image_canvas\n",
    "    \n",
    "    # Calculate the FINAL position of the text block's *content area* top-left corner on the (potentially new) canvas.\n",
    "    final_text_block_x_content_on_canvas = 0\n",
    "    final_text_block_y_content_top_on_canvas = 0\n",
    "\n",
    "    if org[0] is None: # Horizontal centering\n",
    "        # Center the entire text block (based on its widest line) horizontally on the new canvas.\n",
    "        final_text_block_x_content_on_canvas = int((img_cv.shape[1] - max_overall_text_width) / 2)\n",
    "    else: # Specific x-coordinate provided\n",
    "        final_text_block_x_content_on_canvas = temp_text_block_x_content_start + offset_x_canvas\n",
    "\n",
    "    if org[1] is None: # Vertical centering\n",
    "        # Center the entire text block vertically on the new canvas.\n",
    "        final_text_block_y_content_top_on_canvas = int((img_cv.shape[0] - total_text_block_content_height) / 2)\n",
    "    else: # Specific y-coordinate provided (baseline)\n",
    "        final_text_block_y_content_top_on_canvas = temp_text_block_y_content_top + offset_y_canvas\n",
    "\n",
    "    # 5. Place Text Background (before text) for the entire block.\n",
    "    # This is drawn only if transparency is greater than 0 and there is text to draw.\n",
    "    if text_background_transparency > 0 and wrapped_lines_info:\n",
    "        # Calculate the top-left and bottom-right corners of the entire text block's background.\n",
    "        # These are relative to the final position on the canvas.\n",
    "        x1_bg = final_text_block_x_content_on_canvas - padding_x\n",
    "        y1_bg = final_text_block_y_content_top_on_canvas - padding_y\n",
    "        x2_bg = final_text_block_x_content_on_canvas + max_overall_text_width + padding_x\n",
    "        y2_bg = final_text_block_y_content_top_on_canvas + total_text_block_content_height + padding_y\n",
    "\n",
    "        # Ensure background coordinates are within the image bounds to prevent drawing outside.\n",
    "        x1_bg = max(0, x1_bg)\n",
    "        y1_bg = max(0, y1_bg)\n",
    "        x2_bg = min(img_cv.shape[1], x2_bg)\n",
    "        y2_bg = min(img_cv.shape[0], y2_bg)\n",
    "\n",
    "        # Only draw the rectangle if the bounding box is valid (positive width and height).\n",
    "        if x2_bg > x1_bg and y2_bg > y1_bg:\n",
    "            overlay = img_cv.copy() # Create a copy to draw the background on.\n",
    "            # Draw a filled rectangle for the background.\n",
    "            cv2.rectangle(overlay, (x1_bg, y1_bg), (x2_bg, y2_bg), text_background_color, -1)\n",
    "            # Blend the overlay with the original image using the specified transparency.\n",
    "            alpha = text_background_transparency\n",
    "            cv2.addWeighted(overlay, alpha, img_cv, 1 - alpha, 0, img_cv)\n",
    "\n",
    "    # 6. Place Text (loop through each wrapped line).\n",
    "    # Calculate the baseline of the first line from the top of the text content block.\n",
    "    # The baseline of the first line is the top of the text content block + the distance from its top to its baseline.\n",
    "    current_line_y_baseline = final_text_block_y_content_top_on_canvas - first_line_top_offset_from_baseline\n",
    "\n",
    "    for i, (line_text, line_w, line_h, line_baseline) in enumerate(wrapped_lines_info):\n",
    "        line_org_x = final_text_block_x_content_on_canvas # Default to block's left edge\n",
    "        \n",
    "        # If the original request was for horizontal centering, recalculate x-origin for each line\n",
    "        # to ensure each line is individually centered within the *new* canvas.\n",
    "        if org[0] is None:\n",
    "            line_org_x = int((img_cv.shape[1] - line_w) / 2)\n",
    "\n",
    "        # Put the text on the image.\n",
    "        cv2.putText(img_cv, line_text, (line_org_x, current_line_y_baseline),\n",
    "                    font_face, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        # Move the y-coordinate down to the baseline of the next line.\n",
    "        # The height of the line itself is line_h, but we use text_h_single_line for consistent spacing.\n",
    "        if i < len(wrapped_lines_info) - 1: # Don't add spacing after the last line\n",
    "            current_line_y_baseline += (text_h_single_line + line_spacing)\n",
    "\n",
    "    # 7. Return the modified image.\n",
    "    return img_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4333d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_random_prompt(folder_path):\n",
    "    \"\"\"\n",
    "    Looks for all CSV files in a specified folder, processes them, updates a random row \n",
    "    with the minimum count, and returns the subject and filename.\n",
    "    The function assumes that the first row of each CSV file contains headers.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    all_data = []\n",
    "    \n",
    "    # 1. Look for all CSV files in the specified folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        return \"No CSV files found in the specified directory.\"\n",
    "\n",
    "    # 2. Read them into dataframes and process them\n",
    "    for filename in csv_files:\n",
    "        try:\n",
    "            # Read the CSV assuming the first row is the header\n",
    "            df = pd.read_csv(filename)\n",
    "            \n",
    "            # Standardize column names to 'subject' and 'count' for consistency\n",
    "            if 'subject' not in df.columns or 'count' not in df.columns:\n",
    "                print(f\"File {filename} does not have 'subject' and 'count' columns. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 2.b If count is blank, set it to 0 and ensure the column is int type\n",
    "            df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "            # 3. Remove rows where the first character of 'subject' is '#'\n",
    "            df = df[~df['subject'].astype(str).str.startswith('#')]\n",
    "            \n",
    "            # Add a column to store the original filename\n",
    "            df['original_file'] = filename\n",
    "            \n",
    "            all_data.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data:\n",
    "        return \"No valid data could be read from the CSV files.\"\n",
    "\n",
    "    # 4. Merge all dataframes into one\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # 5. Sort them based on the count column\n",
    "    merged_df = merged_df.sort_values(by='count')\n",
    "\n",
    "    # 6. Select the set of rows that has the minimum value of count\n",
    "    min_count = merged_df['count'].min()\n",
    "    min_count_rows = merged_df[merged_df['count'] == min_count]\n",
    "    \n",
    "    if min_count_rows.empty:\n",
    "        return \"No rows with a minimum count were found.\"\n",
    "\n",
    "    # 7. Use a random row from this list\n",
    "    selected_row = min_count_rows.sample(n=1).iloc[0]\n",
    "    selected_subject = selected_row['subject']\n",
    "    selected_file = selected_row['original_file']\n",
    "\n",
    "    # 8. Update the original CSV value for this row and add one to its count\n",
    "    # 9. Save the original CSV\n",
    "    try:\n",
    "        # Read the original file again to ensure we are modifying the correct data\n",
    "        original_df = pd.read_csv(selected_file)\n",
    "        \n",
    "        # FIX: Ensure the count column is numeric with blank values as 0, \n",
    "        # so the comparison with min_count works correctly.\n",
    "        original_df['count'] = pd.to_numeric(original_df['count'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        # Find the row to update\n",
    "        # Using a more robust way to find the row to avoid issues with duplicates\n",
    "        # We'll match on both subject and the count before updating\n",
    "        original_df.loc[\n",
    "            (original_df['subject'] == selected_subject) & (original_df['count'] == min_count),\n",
    "            'count'\n",
    "        ] += 1\n",
    "        \n",
    "        # Save the updated dataframe back to the original CSV file\n",
    "        original_df.to_csv(selected_file, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error updating and saving file {selected_file}: {e}\"\n",
    "\n",
    "    # 10. Return the subject and the filename (without extension)\n",
    "    filename_without_ext = os.path.splitext(os.path.basename(selected_file))[0]\n",
    "    return  filename_without_ext,selected_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e754a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_increment(file_path=\"counter.txt\"):\n",
    "    \"\"\"\n",
    "    Reads a number from a file, returns it, and updates the file content with the number incremented by one.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the number.\n",
    "\n",
    "    Returns:\n",
    "        int: The number read from the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the current number from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            number = int(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, start with 0\n",
    "        number = 0\n",
    "    except ValueError:\n",
    "        # If the file content is invalid, raise an error\n",
    "        raise ValueError(f\"The file {file_path} does not contain a valid integer.\")\n",
    "\n",
    "    # Increment the number\n",
    "    incremented_number = number + 1\n",
    "\n",
    "    # Write the incremented number back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(str(incremented_number))\n",
    "\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logo=Image.open(str(assts_dir/\"aiartstudio_logo.png\"))\n",
    "# puzzel_no_text = f\"Puzzle No: {read_and_increment()}\"\n",
    "# logo1 = add_text_to_image(logo, puzzel_no_text, org=(None, 1800), font_scale=4, color=(0, 0, 255), thickness=10, wordwrap=True)\n",
    "# logo1_image = Image.fromarray(cv2.cvtColor(logo1, cv2.COLOR_BGR2RGB))\n",
    "# logo1_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b44ab",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae56ed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392e4b21f2f54c5d98d7a322184b6895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed a non-standard module StableDiffusionSafetyChecker(\n",
      "  (vision_model): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(257, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
      "). We cannot verify whether it has the correct type\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8986857b1f9d42a1bb42812062002228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = SD15ImageGenerator(num_inference_steps=100)\n",
    "\n",
    "conceptPromptGenerator=ConceptPromptGenerator(device=\"cpu\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088ce93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for data1: [0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.04942528735632184, 0.06551724137931034, 0.08160919540229886, 0.09770114942528735, 0.11379310344827587, 0.12988505747126436, 0.14597701149425288, 0.16206896551724137, 0.1781609195402299, 0.19425287356321838, 0.21034482758620687, 0.2264367816091954, 0.2425287356321839, 0.25862068965517243, 0.2747126436781609, 0.2908045977011494, 0.30689655172413793, 0.32298850574712645, 0.3390804597701149, 0.35517241379310344, 0.37126436781609196, 0.3873563218390804, 0.40344827586206894, 0.41954022988505746, 0.435632183908046, 0.4517241379310345, 0.46781609195402296, 0.4839080459770115, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.44166666666666665, 0.3833333333333333, 0.325, 0.26666666666666666, 0.20833333333333331, 0.15000000000000002, 0.09166666666666667, 0.03333333333333333, 5.202]\n",
      "len 100, sum 31.602\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage:\n",
    "speed_stop=0.5\n",
    "#speed_distribution = [(40, 1/30, speed_stop), (40, speed_stop, speed_stop), (19, speed_stop, 1/30)]\n",
    "speed_distribution = [(30, 1/30, 1/30), (30,1/30, speed_stop),(30, speed_stop, speed_stop), (9, speed_stop, 1/30)]\n",
    "result1 = generate_evenly_distributed_values(speed_distribution)\n",
    "result1.append(5.202)\n",
    "print(f\"Result for data1: {result1}\")\n",
    "print(f\"len {len(result1)}, sum {sum(result1)}\" )\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70851b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_one_video():\n",
    "    subscribe_msg= \"Think you're a fast problem-solver? Subscribe for fresh puzzles and fascinating facts! Cracked the code? Prove it! Share your answer AND your solve time in the comments below!\"\n",
    "    Subscribe_position = (None, 580)  # Position for the subscribe message\n",
    "    category,subject = get_random_prompt(propmt_dir)\n",
    "    #category=\"testing\"\n",
    "    #subject=\"Louvre Abu Dhabi a longsubject name that is very long and should be wrapped properly in the image\"\n",
    "    print(category, subject)\n",
    "    #prompt_dic={'Prompt': 'a detailed view of Louvre Abu Dhabi, architectural marvel, modern design, glass dome, sun rays, reflection, desert backdrop', 'Negative Prompt': 'no details, wrong architecture, generic buildings, low resolution, no dome', 'Fact': 'Louvre Abu Dhabi fuses French art with Middle Eastern influences.'}\n",
    "    prompt_dic=conceptPromptGenerator.generate(subject,category)\n",
    "    print(prompt_dic)    \n",
    "    images = generator.generate_image(prompt_dic['Prompt'],prompt_dic['Negative Prompt'] , guidance_scale=7.5)\n",
    "    editor = VideoEditor(fps=30, frame_size=(512, 512))\n",
    "    #generator.save_images(images, directory=\"generated\")\n",
    "    #editor.add_images_from_list(images, duration_sec=30)\n",
    "    if(len(result1) != len(images   )):\n",
    "        print(\"time_duration and number of images are not matched.\")\n",
    "    logo=Image.open(str(assts_dir/\"aiartstudio_logo.png\"))\n",
    "    puzzel_no_text = f\"Puzzle No: {read_and_increment()}\"\n",
    "    logo1 = add_text_to_image(logo, puzzel_no_text, org=(None, 1800), font_scale=4, color=(0, 0, 255), thickness=10, wordwrap=True)\n",
    "    editor.add_image(logo1, 3)  # Add logo for 3 seconds\n",
    "\n",
    "    editor.set_global_display_time_offset_start(0)  # Set global offset to 0 seconds\n",
    "    total_duration = 0\n",
    "    for index, duration in enumerate(result1):\n",
    "        img = images[index] \n",
    "        img1=add_text_to_image(img, f\"Guess what AI painting now?!\",org=(25,540),font_scale=0.8)\n",
    "        total_duration += duration\n",
    "        if int(total_duration)%2 == 0:\n",
    "            img1 = add_text_to_image(img1, subscribe_msg, org=Subscribe_position, font_scale=0.8,color=(0, 0, 255),wordwrap=True)\n",
    "        else:\n",
    "            img1 = add_text_to_image(img1, subscribe_msg, org=Subscribe_position, font_scale=0.8,color=(255, 0, 255),wordwrap=True)\n",
    "        editor.add_image(img1, duration,show_time_text=True,time_display_format=\"SS.MS\",time_text_position=(400, 373),time_text_font_scale=0.8)\n",
    "    img_final = add_text_to_image(images[-1], subscribe_msg, org=Subscribe_position, font_scale=0.8,color=(0, 0, 255),wordwrap=True)\n",
    "    img=add_text_to_image(img_final.copy(), \"Time Up, AI draw:\",org=(None,250))\n",
    "    img=add_text_to_image(img, subject,org=(None,350),color=(0, 0, 255), font_scale=1.2, thickness=3,wordwrap=True)\n",
    "    editor.add_image(img, 5,show_time_text=False)\n",
    "\n",
    "    img=add_text_to_image(img_final, \"Fact:\",org=(None,50))\n",
    "    img=add_text_to_image(img,prompt_dic['Fact'] ,org=(None,150),color=(0, 0, 255), font_scale=1.2, thickness=3,wordwrap=True)\n",
    "    editor.add_image(img, 10,show_time_text=False)\n",
    "    editor.add_audio(str(assts_dir/\"Long Distance.mp3\"),audio_clip_end=editor.get_video_duration(),video_start_offset=0)  # Add audio starting at the beginning of the video\n",
    "    #editor.add_image(images[-1],3)  # Add last image for 3 seconds\n",
    "    fileName=generate_filename_by_datetime(category, \"mp4\")\n",
    "    #full_path_filename = str(GDrive_dir/fileName)\n",
    "    #print(full_path_filename)\n",
    "    editor.save(f\"videos/{fileName}\")\n",
    "\n",
    "#gen_one_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "222490ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celeb Lady Gaga\n",
      "{'Prompt': 'Lady Gaga in a dramatic stage outfit, intense makeup, bold colors, dynamic pose, concert stage, high energy', 'Negative Prompt': 'cartoonish, low detail, simplified, low resolution, unrealistic proportions, messy hair', 'Fact': 'Lady Gaga is known for her innovative fashion and music.'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d4f1c3074b4eefae795663951aac0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-08-04-13-19-39_Celeb.mp4...\n",
      "Moviepy - Building video videos/2025-08-04-13-19-39_Celeb.mp4.\n",
      "MoviePy - Writing audio in 2025-08-04-13-19-39_CelebTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-08-04-13-19-39_Celeb.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-08-04-13-19-39_Celeb.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Celeb Friedrich Nietzsche\n",
      "{'Prompt': 'a thoughtful portrait of Friedrich Nietzsche, stoic expression, deep eyes, surrounded by books and candlelight, high contrast, realistic style', 'Negative Prompt': 'cartoonish, modern attire, unrealistic facial features, cartoon style, overly dramatic lighting, dishezzled hair', 'Fact': \"Nietzsche's philosophy influenced existentialism and modern thought.\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5dd282f0ad47e1934fed6f39c40645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-08-04-13-27-16_Celeb.mp4...\n",
      "Moviepy - Building video videos/2025-08-04-13-27-16_Celeb.mp4.\n",
      "MoviePy - Writing audio in 2025-08-04-13-27-16_CelebTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-08-04-13-27-16_Celeb.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-08-04-13-27-16_Celeb.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Object Clock\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mgen_one_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mgen_one_video\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(category, subject)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#prompt_dic={'Prompt': 'a detailed view of Louvre Abu Dhabi, architectural marvel, modern design, glass dome, sun rays, reflection, desert backdrop', 'Negative Prompt': 'no details, wrong architecture, generic buildings, low resolution, no dome', 'Fact': 'Louvre Abu Dhabi fuses French art with Middle Eastern influences.'}\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m prompt_dic=\u001b[43mconceptPromptGenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(prompt_dic)    \n\u001b[32m     11\u001b[39m images = generator.generate_image(prompt_dic[\u001b[33m'\u001b[39m\u001b[33mPrompt\u001b[39m\u001b[33m'\u001b[39m],prompt_dic[\u001b[33m'\u001b[39m\u001b[33mNegative Prompt\u001b[39m\u001b[33m'\u001b[39m] , guidance_scale=\u001b[32m7.5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mConceptPromptGenerator.generate\u001b[39m\u001b[34m(self, concept, category)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, concept: \u001b[38;5;28mstr\u001b[39m, category: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     22\u001b[39m     few_shot = (\n\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are an expert in AI art prompting and factual summaries.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     24\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGiven a concept and its category, generate:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPrompt:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfew_shot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Parse output from where the concept starts\u001b[39;00m\n\u001b[32m     48\u001b[39m     output = output.split(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConcept: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconcept\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:316\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    315\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1464\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1457\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1458\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1461\u001b[39m         )\n\u001b[32m   1462\u001b[39m     )\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1470\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1471\u001b[39m     model_outputs = \u001b[38;5;28mself\u001b[39m.forward(model_inputs, **forward_params)\n\u001b[32m   1472\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:354\u001b[39m, in \u001b[36mTextGenerationPipeline.preprocess\u001b[39m\u001b[34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[39m\n\u001b[32m    345\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer.apply_chat_template(\n\u001b[32m    346\u001b[39m         prompt_text.messages,\n\u001b[32m    347\u001b[39m         add_generation_prompt=\u001b[38;5;129;01mnot\u001b[39;00m continue_final_message,\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m         **tokenizer_kwargs,\n\u001b[32m    352\u001b[39m     )\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m inputs[\u001b[33m\"\u001b[39m\u001b[33mprompt_text\u001b[39m\u001b[33m\"\u001b[39m] = prompt_text\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle_long_generation == \u001b[33m\"\u001b[39m\u001b[33mhole\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2855\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2854\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2855\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2857\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2965\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2943\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2944\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   2945\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2962\u001b[39m         **kwargs,\n\u001b[32m   2963\u001b[39m     )\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2968\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3040\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3011\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3012\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3013\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3028\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3029\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3031\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3032\u001b[39m     padding=padding,\n\u001b[32m   3033\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3037\u001b[39m     **kwargs,\n\u001b[32m   3038\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3040\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3043\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:126\u001b[39m, in \u001b[36mGPT2TokenizerFast._encode_plus\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m is_split_into_words = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mis_split_into_words\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[32m    122\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with add_prefix_space=True \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mto use it with pretokenized inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:627\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    604\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    605\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m     **kwargs,\n\u001b[32m    625\u001b[39m ) -> BatchEncoding:\n\u001b[32m    626\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:116\u001b[39m, in \u001b[36mGPT2TokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m is_split_into_words = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mis_split_into_words\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[32m    112\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with add_prefix_space=True \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mto use it with pretokenized inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    114\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/r/Data/AI/AiArtStudio.Puzzle/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:553\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    551\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    565\u001b[39m tokens_and_encodings = [\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    567\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    577\u001b[39m ]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    gen_one_video()\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
