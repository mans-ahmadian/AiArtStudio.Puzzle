{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92bce612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Require Python 3.12.3\n",
    "#%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f53fc",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "from transformers import CLIPFeatureExtractor,pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import ollama\n",
    "import json\n",
    "import time\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "from transformers import CLIPTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0eaa39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5edc07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir= Path(os.getcwd())\n",
    "assts_dir= working_dir/\"assets\"\n",
    "propmt_dir = working_dir/\"Prompts\"\n",
    "#GDrive_dir = Path(\"run/user/1000/gvfs/google-drive:host=gmail.com,user=aiartstudio.ai/0AOT4cSJ5oKlpUk9PVA/1cBJcIkDKKJziO4CPcNyoIBUOQ6n_MshJ\")\n",
    "videos_dir= working_dir/\"Videos\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee1f25",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36448523",
   "metadata": {},
   "source": [
    "## ImageCanvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f2443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ImageCanvas:\n",
    "    \"\"\"\n",
    "    A class to create a canvas and add images and text to it with various options.\n",
    "\n",
    "    Attributes:\n",
    "        size (tuple): A tuple (width, height) representing the canvas size.\n",
    "        canvas (Image): The PIL Image object that serves as the canvas.\n",
    "        padding_top (int): The top padding in pixels.\n",
    "        padding_bottom (int): The bottom padding in pixels.\n",
    "        padding_left (int): The left padding in pixels.\n",
    "        padding_right (int): The right padding in pixels.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(1080, 1920), padding_top=50, padding_bottom=50, padding_left=50, padding_right=50, background_color='gray'):\n",
    "        \"\"\"\n",
    "        Initializes the ImageCanvas with a specified size, padding, and background color.\n",
    "\n",
    "        Args:\n",
    "            size (tuple, optional): The width and height of the canvas. Defaults to (1080, 1920).\n",
    "            padding_top (int, optional): Top padding in pixels. Defaults to 250.\n",
    "            padding_bottom (int, optional): Bottom padding in pixels. Defaults to 250.\n",
    "            padding_left (int, optional): Left padding in pixels. Defaults to 200.\n",
    "            padding_right (int, optional): Right padding in pixels. Defaults to 200.\n",
    "            background_color (str or tuple, optional): The background color of the canvas.\n",
    "                                                      Can be a color name (e.g., 'gray'), a hex code (e.g., '#808080'),\n",
    "                                                      or an RGB tuple (e.g., (128, 128, 128)). Defaults to 'gray'.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        # Create a canvas with the specified background color\n",
    "        self.canvas = Image.new('RGB', self.size, background_color)\n",
    "        self.padding_top = padding_top\n",
    "        self.padding_bottom = padding_bottom\n",
    "        self.padding_left = padding_left\n",
    "        self.padding_right = padding_right\n",
    "        print(f\"Image canvas created with size {self.size}, specified padding, and background color '{background_color}'.\")\n",
    "\n",
    "    def add_image(self, input_image, position=(0, 0), size=None, resize_option='original'):\n",
    "        \"\"\"\n",
    "        Adds an image to the canvas at a specific position with a given size and resize option.\n",
    "\n",
    "        Args:\n",
    "            input_image (str, Image, or np.ndarray): The file path to the image, a PIL Image object,\n",
    "                                                    or a NumPy array (e.g., from OpenCV).\n",
    "            position (tuple, optional): The (x, y) coordinates for the top-left corner\n",
    "                                        of the added image. Defaults to (0, 0).\n",
    "            size (tuple, optional): A tuple (width, height) for the fixed size option.\n",
    "                                    Required if resize_option is 'fixed_size'.\n",
    "            resize_option (str, optional): The resizing strategy to use.\n",
    "                                           Options are 'original', 'fill_width',\n",
    "                                           'fill_height', or 'fixed_size'.\n",
    "                                           Defaults to 'original'.\n",
    "        Raises:\n",
    "            ValueError: If an invalid resize_option is provided.\n",
    "            ValueError: If size is None for the 'fixed_size' option.\n",
    "            TypeError: If the input_image is not a supported type.\n",
    "        \"\"\"\n",
    "        input_image_obj = None\n",
    "\n",
    "        if isinstance(input_image, str):\n",
    "            # Input is a file path\n",
    "            if not os.path.exists(input_image):\n",
    "                print(f\"Error: The image file path '{input_image}' does not exist.\")\n",
    "                return\n",
    "            try:\n",
    "                input_image_obj = Image.open(input_image).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"Error: Could not open the image file. {e}\")\n",
    "                return\n",
    "        elif isinstance(input_image, Image.Image):\n",
    "            # Input is a PIL Image object\n",
    "            input_image_obj = input_image.convert('RGB')\n",
    "        elif isinstance(input_image, np.ndarray):\n",
    "            # Input is a NumPy array (e.g., from OpenCV)\n",
    "            try:\n",
    "                # OpenCV uses BGR, convert to RGB for Pillow\n",
    "                rgb_image = input_image[:, :, ::-1]\n",
    "                input_image_obj = Image.fromarray(rgb_image).convert('RGB')\n",
    "                print(\"Converted OpenCV (NumPy) image to PIL Image.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: Could not convert NumPy array to PIL Image. {e}\")\n",
    "                return\n",
    "        else:\n",
    "            raise TypeError(\"Invalid input_image type. Must be a file path (str), a PIL Image object, or a NumPy array (np.ndarray).\")\n",
    "\n",
    "        # Use input_image_obj for all subsequent operations\n",
    "        image_width, image_height = input_image_obj.size\n",
    "        \n",
    "        # Calculate the available drawing area based on padding\n",
    "        available_width = self.size[0] - self.padding_left - self.padding_right\n",
    "        available_height = self.size[1] - self.padding_top - self.padding_bottom\n",
    "        x_offset = self.padding_left\n",
    "        y_offset = self.padding_top\n",
    "\n",
    "        if resize_option == 'original':\n",
    "            # Use the original size of the input image\n",
    "            new_width, new_height = image_width, image_height\n",
    "        elif resize_option == 'fill_width':\n",
    "            # Resize to fill the available width while maintaining aspect ratio\n",
    "            aspect_ratio = image_height / image_width\n",
    "            new_width = available_width\n",
    "            new_height = int(new_width * aspect_ratio)\n",
    "        elif resize_option == 'fill_height':\n",
    "            # Resize to fill the available height while maintaining aspect ratio\n",
    "            aspect_ratio = image_width / image_height\n",
    "            new_height = available_height\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "        elif resize_option == 'fixed_size':\n",
    "            # Resize to a specific size provided in the 'size' parameter\n",
    "            if size is None or not isinstance(size, tuple) or len(size) != 2:\n",
    "                raise ValueError(\"For 'fixed_size' option, a valid 'size' tuple (width, height) must be provided.\")\n",
    "            new_width, new_height = size\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resize_option. Choose from 'original', 'fill_width', 'fill_height', or 'fixed_size'.\")\n",
    "\n",
    "        # Resize the image using the calculated dimensions\n",
    "        resized_image = input_image_obj.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Calculate the final paste position\n",
    "        paste_x = x_offset + position[0]\n",
    "        paste_y = y_offset + position[1]\n",
    "\n",
    "        # Paste the resized image onto the canvas. The bounding box ensures it stays within the canvas.\n",
    "        self.canvas.paste(resized_image, (paste_x, paste_y))\n",
    "        print(f\"Image added with '{resize_option}' resizing.\")\n",
    "\n",
    "    def add_text(self, text, position=(0, 0), font_path=None, font_size=50, text_color='black', background_color=None, align='left'):\n",
    "        \"\"\"\n",
    "        Adds text to the canvas with wrapping, font, and color options.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be added to the image.\n",
    "            position (tuple, optional): The (x, y) coordinates for the top-left corner\n",
    "                                        of the text box. Defaults to (0, 0).\n",
    "            font_path (str, optional): The path to the font file (e.g., 'arial.ttf').\n",
    "                                        If None, a default font will be used. Defaults to None.\n",
    "            font_size (int, optional): The size of the font. Defaults to 50.\n",
    "            text_color (str or tuple, optional): The color of the text. Defaults to 'black'.\n",
    "            background_color (str or tuple, optional): The background color for the text.\n",
    "                                                      Defaults to None (transparent).\n",
    "            align (str, optional): The alignment of the text. Options are 'left', 'center', or 'right'.\n",
    "                                   Defaults to 'left'.\n",
    "        \"\"\"\n",
    "        draw = ImageDraw.Draw(self.canvas)\n",
    "        \n",
    "        font = None\n",
    "        # Attempt to load the user-provided font first\n",
    "        if font_path and os.path.exists(font_path):\n",
    "            try:\n",
    "                font = ImageFont.truetype(font_path, font_size)\n",
    "            except IOError:\n",
    "                print(f\"Warning: Could not load font from '{font_path}'. Falling back to system fonts.\")\n",
    "\n",
    "        # If a font hasn't been loaded, try common system fonts, including CJK support\n",
    "        if not font:\n",
    "            fallback_fonts = [\n",
    "                'arial.ttf', 'DejaVuSans.ttf', 'times.ttf', 'ipaexg.ttf', # Common CJK font\n",
    "                'msmincho.ttf', # Common Windows Japanese font\n",
    "                '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',  # Linux path\n",
    "                '/Library/Fonts/Arial.ttf'  # macOS path\n",
    "            ]\n",
    "            for f_path in fallback_fonts:\n",
    "                try:\n",
    "                    font = ImageFont.truetype(f_path, font_size)\n",
    "                    print(f\"Using fallback font: {f_path}\")\n",
    "                    break\n",
    "                except (IOError, OSError):\n",
    "                    continue\n",
    "        \n",
    "        # If all else fails, use the default Pillow font and warn the user\n",
    "        if not font:\n",
    "            print(\"Warning: No valid TrueType font found. Using default Pillow font. Font size and special characters may not work.\")\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # Clean the text: keep only characters the font can render\n",
    "        cleaned_text = \"\"\n",
    "        for char in text:\n",
    "            # Check for spaces, as they don't have a getmask() but are valid characters\n",
    "            if char.isspace():\n",
    "                cleaned_text += char\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Get the bounding box of the character's mask. If it's None, the glyph doesn't exist.\n",
    "                bbox = font.getmask(char, mode=\"L\").getbbox()\n",
    "                if bbox:\n",
    "                    cleaned_text += char\n",
    "                else:\n",
    "                    print(f\"\\033[91mWarning: Character '{char}' could not be rendered with the selected font. It has been removed.\\033[0m\")\n",
    "            except Exception:\n",
    "                 print(f\"\\033[91mWarning: Character '{char}' could not be rendered with the selected font. It has been removed.\\033[0m\")\n",
    "        \n",
    "        # Define the available space for text based on padding\n",
    "        max_width = self.size[0] - self.padding_left - self.padding_right - position[0]\n",
    "        \n",
    "        # Word wrapping logic - a more robust approach\n",
    "        lines = []\n",
    "        words = cleaned_text.split()\n",
    "        if not words:\n",
    "            return\n",
    "        \n",
    "        current_line = words[0]\n",
    "        for word in words[1:]:\n",
    "            bbox = draw.textbbox((0, 0), current_line + ' ' + word, font=font)\n",
    "            line_width = bbox[2] - bbox[0]\n",
    "            if line_width < max_width:\n",
    "                current_line += ' ' + word\n",
    "            else:\n",
    "                lines.append(current_line)\n",
    "                current_line = word\n",
    "        lines.append(current_line)\n",
    "\n",
    "        # Calculate the total height of the text block\n",
    "        text_height = 0\n",
    "        for line in lines:\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            text_height += (bbox[3] - bbox[1])\n",
    "        \n",
    "        # Calculate final paste position with padding offset\n",
    "        paste_x = self.padding_left + position[0]\n",
    "        paste_y = self.padding_top + position[1]\n",
    "\n",
    "        # Draw background if specified\n",
    "        if background_color:\n",
    "            text_box_width = max(draw.textbbox((0, 0), line, font=font)[2] - draw.textbbox((0, 0), line, font=font)[0] for line in lines)\n",
    "            draw.rectangle(\n",
    "                (paste_x, paste_y, paste_x + text_box_width, paste_y + text_height),\n",
    "                fill=background_color\n",
    "            )\n",
    "\n",
    "        # Draw each line of text\n",
    "        y_text = paste_y\n",
    "        for line in lines:\n",
    "            # Calculate the x-coordinate for alignment\n",
    "            line_width = draw.textbbox((0, 0), line, font=font)[2] - draw.textbbox((0, 0), line, font=font)[0]\n",
    "            x_text = paste_x\n",
    "\n",
    "            if align == 'center':\n",
    "                x_text = paste_x + (max_width - line_width) / 2\n",
    "            elif align == 'right':\n",
    "                x_text = paste_x + (max_width - line_width)\n",
    "\n",
    "            # Draw the line of text\n",
    "            draw.text((x_text, y_text), line, font=font, fill=text_color)\n",
    "            \n",
    "            # Calculate the height of the current line to advance y_text\n",
    "            bbox = draw.textbbox((0, 0), line, font=font)\n",
    "            line_height = bbox[3] - bbox[1]\n",
    "            y_text += line_height\n",
    "            \n",
    "        print(f\"Text added to canvas: '{cleaned_text[:20]}...'\")\n",
    "\n",
    "    def save(self, output_path):\n",
    "        \"\"\"\n",
    "        Saves the final canvas image to the specified path.\n",
    "\n",
    "        Args:\n",
    "            output_path (str): The file path to save the image to.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.canvas.save(output_path)\n",
    "            print(f\"Canvas saved successfully to '{output_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Could not save the canvas. {e}\")\n",
    "            \n",
    "    def get_image(self):\n",
    "        \"\"\"\n",
    "        Returns the current PIL Image object of the canvas.\n",
    "\n",
    "        Returns:\n",
    "            Image: The PIL Image object of the canvas.\n",
    "        \"\"\"\n",
    "        return self.canvas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042e3b6",
   "metadata": {},
   "source": [
    "## ConceptPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0566db85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Connecting to local Ollama server ---\n",
      "An error occurred: 'Client' object has no attribute 'host'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Define a Pydantic BaseModel with a max_length constraint\n",
    "class ConceptPrompt(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for the model's structured output.\n",
    "    The 'fact' field is constrained to a maximum of 150 characters.\n",
    "    \"\"\"\n",
    "    prompt: str\n",
    "    negative_prompt: str\n",
    "    fact: str = Field(max_length=150)\n",
    "\n",
    "class OllamaConceptPromptGenerator:\n",
    "    \"\"\"\n",
    "    A class to generate prompts and facts using an Ollama-hosted language model.\n",
    "    This version uses the `ollama.chat` function with a Pydantic schema to\n",
    "    enforce reliable JSON output, and post-processes the output to be ASCII-only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"gemma3:12b\", host=None):\n",
    "        \"\"\"\n",
    "        Initializes the Ollama client and sets up model parameters.\n",
    "        Automatically downloads the specified model if it's not found on the server.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the model to use from Ollama.\n",
    "                              Defaults to \"gemma3:12b\".\n",
    "            host (str, optional): The URL of the Ollama server to connect to.\n",
    "                                  e.g., 'http://192.168.1.100:11434'. If None,\n",
    "                                  it connects to the default local server.\n",
    "        \"\"\"\n",
    "        self.client = ollama.Client(host=host)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        try:\n",
    "            # FIX: The ollama.list() method returns an object, not a dictionary.\n",
    "            # We must access its 'models' attribute and then the 'model' attribute of each item.\n",
    "            available_models = [m.model for m in self.client.list().models]\n",
    "            \n",
    "            if self.model_name not in available_models:\n",
    "                print(f\"Model '{self.model_name}' not found on the server. Downloading...\")\n",
    "                \n",
    "                is_terminal = sys.stdout.isatty()\n",
    "                for progress_info in self.client.pull(self.model_name, stream=True):\n",
    "                    if is_terminal and 'total' in progress_info and 'completed' in progress_info:\n",
    "                        total = progress_info['total']\n",
    "                        completed = progress_info['completed']\n",
    "                        if total > 0:\n",
    "                            percent = int(completed / total * 100)\n",
    "                            print(f\"\\rDownloading... {percent}% ({completed}/{total})\", end=\"\", flush=True)\n",
    "                    elif is_terminal and 'status' in progress_info:\n",
    "                        print(f\"\\rStatus: {progress_info['status']}\", end=\"\", flush=True)\n",
    "                    \n",
    "                    time.sleep(0.1)\n",
    "                \n",
    "                print(\"\\nDownload complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Could not connect to Ollama server at '{self.client.host}'.\")\n",
    "            print(f\"Details: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _sanitize_to_ascii(self, text: str) -> str:\n",
    "        \"\"\"Removes any non-ASCII characters from a string.\"\"\"\n",
    "        return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    def generate(self, concept: str, category: str) -> dict:\n",
    "        \"\"\"\n",
    "        Generates a Stable Diffusion prompt, a negative prompt, and a new, random fact \n",
    "        based on a given concept and category using the Ollama model.\n",
    "        The output is guaranteed to be new with each call due to the `temperature` setting,\n",
    "        a random seed, and a random prompt modifier.\n",
    "\n",
    "        Args:\n",
    "            concept (str): The main concept for the generation.\n",
    "            category (str): The category of the concept.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the generated \"prompt\", \"negative_prompt\",\n",
    "                  and \"fact\", with all characters guaranteed to be ASCII.\n",
    "        \"\"\"\n",
    "        modifiers = [\n",
    "            \"in a watercolor style\",\n",
    "            \"in the style of a vintage painting\",\n",
    "            \"at dawn with a soft, ethereal glow\",\n",
    "            \"in a cinematic, dramatic lighting\",\n",
    "            \"as a vibrant digital painting\",\n",
    "            \"in a surrealist style with a dreamlike quality\",\n",
    "            \"with a clean, minimalist aesthetic\",\n",
    "            \"in a cyberpunk style with neon highlights\",\n",
    "            \"at night under a full moon\",\n",
    "            \"with a painterly, oil-on-canvas texture\"\n",
    "        ]\n",
    "        \n",
    "        random_modifier = random.choice(modifiers)\n",
    "        random_seed = random.randint(0, 100000000)\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"Given the concept '{concept}' and category '{category}', generate a vivid prompt for Stable Diffusion 1.5, a negative prompt, and a new, unique, short fact (not more than 150 characters) about the concept, {random_modifier}.\",\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        options = {\n",
    "            \"temperature\": 0.8,\n",
    "            \"top_p\": 0.9,\n",
    "            \"seed\": random_seed\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                format=ConceptPrompt.model_json_schema(),\n",
    "                options=options\n",
    "            )\n",
    "            \n",
    "            generated_data = ConceptPrompt.model_validate_json(response['message']['content'])\n",
    "            \n",
    "            sanitized_output = {\n",
    "                \"prompt\": self._sanitize_to_ascii(generated_data.prompt),\n",
    "                \"negative_prompt\": self._sanitize_to_ascii(generated_data.negative_prompt),\n",
    "                \"fact\": self._sanitize_to_ascii(generated_data.fact)\n",
    "            }\n",
    "            \n",
    "            return sanitized_output\n",
    "        \n",
    "        except ValidationError as e:\n",
    "            print(f\"Error: Model output did not match the Pydantic schema. Details: {e}\")\n",
    "            print(f\"Problematic output: {response['message']['content']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation or parsing. Details: {e}\")\n",
    "        \n",
    "        return {\"prompt\": \"\", \"negative_prompt\": \"\", \"fact\": \"\"}\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"--- Connecting to local Ollama server ---\")\n",
    "        generator = OllamaConceptPromptGenerator(model_name=\"gemma3:12b\")\n",
    "        \n",
    "        concept_to_generate = \"cherry blossom tree\"\n",
    "        category_of_concept = \"plant\"\n",
    "        \n",
    "        print(\"\\n--- Generating First Output with a random seed and style ---\")\n",
    "        generated_content_1 = generator.generate(concept_to_generate, category_of_concept)\n",
    "        print(f\"\\nGenerated content for '{concept_to_generate}':\")\n",
    "        print(json.dumps(generated_content_1, indent=2))\n",
    "        \n",
    "        print(\"\\n--- Generating Second Output (should be significantly different) ---\")\n",
    "        generated_content_2 = generator.generate(concept_to_generate, category_of_concept)\n",
    "        print(f\"\\nGenerated content for '{concept_to_generate}':\")\n",
    "        print(json.dumps(generated_content_2, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10d0b5",
   "metadata": {},
   "source": [
    "## VideoEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2713a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoEditor:\n",
    "    def __init__(self, fps=30, fourcc='mp4v', frame_size=(640, 480)):\n",
    "        self.fps = fps\n",
    "        self.frame_size = frame_size\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*fourcc)\n",
    "        self.temp_video_path = tempfile.mktemp(suffix='.mp4')\n",
    "        self.video_writer = cv2.VideoWriter(self.temp_video_path, self.fourcc, self.fps, self.frame_size)\n",
    "        self.current_time = 0  # in seconds, tracks the actual current duration of the video content\n",
    "        self.audio_clips = []\n",
    "\n",
    "        # Default settings for time text overlay\n",
    "        self.default_show_time_text = False\n",
    "        self.default_time_text_position = (50, 50)  # Default top-left corner\n",
    "        self.default_time_text_color = (0, 255, 0)  # Default Green (B, G, R)\n",
    "        self.default_time_text_font_scale = 1.0\n",
    "        self.default_time_text_thickness = 2\n",
    "        self.default_time_text_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        self.default_time_display_format = \"HH:MM:SS.MS\"\n",
    "\n",
    "        # New class-level parameter for global display time offset\n",
    "        # If None, the displayed time will default to the actual video current_time.\n",
    "        # If set, it overrides the default behavior for all subsequent additions\n",
    "        # where display_time_start_offset is not explicitly provided.\n",
    "        self.global_display_time_offset_start = None\n",
    "        # New: Counter for the displayed time when global_display_time_offset_start is active\n",
    "        self.display_time_counter = 0.0\n",
    "\n",
    "    def set_global_display_time_offset_start(self, offset_time):\n",
    "        \"\"\"\n",
    "        Sets a global offset for the time displayed on the video.\n",
    "        Any subsequent images added with show_time_text enabled and without\n",
    "        an explicit display_time_start_offset will use this value as their base.\n",
    "\n",
    "        Args:\n",
    "            offset_time (float): The starting time (in seconds) to display on the video.\n",
    "                                 Set to None to revert to using the actual video's current_time.\n",
    "        \"\"\"\n",
    "        if not isinstance(offset_time, (int, float)) and offset_time is not None:\n",
    "            raise TypeError(\"offset_time must be a number (int or float) or None.\")\n",
    "        if offset_time is not None and offset_time < 0:\n",
    "            raise ValueError(\"offset_time cannot be negative.\")\n",
    "        self.global_display_time_offset_start = offset_time\n",
    "        # Initialize the display_time_counter when the global offset is set\n",
    "        self.display_time_counter = offset_time if offset_time is not None else 0.0\n",
    "\n",
    "\n",
    "    def set_default_time_display_format(self, format_string):\n",
    "        \"\"\"\n",
    "        Sets the default format for displaying time on frames.\n",
    "\n",
    "        Args:\n",
    "            format_string (str): The desired format string using placeholders:\n",
    "                                 HH (hours), MM (minutes), SS (seconds), MS (milliseconds).\n",
    "                                 Placeholders are case-insensitive (e.g., \"hh\", \"mm\", \"ss\", \"ms\" also work).\n",
    "                                 Example: \"HH:MM:SS.MS\", \"ss.ms\", \"MM:SS\".\n",
    "        \"\"\"\n",
    "        self.default_time_display_format = format_string\n",
    "\n",
    "    def _resize_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Resizes an image frame to the video's frame_size.\n",
    "        \"\"\"\n",
    "        return cv2.resize(frame, self.frame_size)\n",
    "\n",
    "    def _convert_to_cv2(self, image):\n",
    "        \"\"\"\n",
    "        Converts various image types (path, Pillow, NumPy array) to an OpenCV image (NumPy array)\n",
    "        and resizes it.\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            img = cv2.imread(image)\n",
    "            if img is None:\n",
    "                raise FileNotFoundError(f\"Image file not found or could not be read: {image}\")\n",
    "        elif isinstance(image, Image.Image):\n",
    "            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            img = image\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image type. Must be a file path (string), Pillow Image, or NumPy array.\")\n",
    "        return self._resize_frame(img)\n",
    "\n",
    "    def _draw_time_on_frame(self, frame, time_in_sec, position, color, font_scale, thickness, font, time_format):\n",
    "        \"\"\"\n",
    "        Draws the current time onto a video frame with a specified format.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): The OpenCV image frame.\n",
    "            time_in_sec (float): The time in seconds to display.\n",
    "            position (tuple): (x, y) coordinates for the text.\n",
    "            color (tuple): (B, G, R) color for the text.\n",
    "            font_scale (float): Font scale factor.\n",
    "            thickness (int): Line thickness for the text.\n",
    "            font (int): OpenCV font type (e.g., cv2.FONT_HERSHEY_SIMPLEX).\n",
    "            time_format (str): The format string using placeholders (HH, MM, SS, MS).\n",
    "                               Placeholders are case-insensitive.\n",
    "        \"\"\"\n",
    "        total_seconds_int = int(time_in_sec)\n",
    "        milliseconds = int((time_in_sec - total_seconds_int) * 10)\n",
    "        seconds = total_seconds_int % 60\n",
    "        minutes = (total_seconds_int // 60) % 60\n",
    "        hours = total_seconds_int // 3600\n",
    "\n",
    "        replacements = {\n",
    "            \"HH\": f\"{hours:02}\", \"hh\": f\"{hours:02}\",\n",
    "            \"MM\": f\"{minutes:02}\", \"mm\": f\"{minutes:02}\",\n",
    "            \"SS\": f\"{seconds:02}\", \"ss\": f\"{seconds:02}\",\n",
    "            \"MS\": f\"{milliseconds:01}\", \"ms\": f\"{milliseconds:01}\"\n",
    "        }\n",
    "\n",
    "        time_str = time_format\n",
    "        for placeholder, value in replacements.items():\n",
    "            time_str = time_str.replace(placeholder, value)\n",
    "\n",
    "        cv2.putText(frame, time_str, position, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        return frame\n",
    "\n",
    "    def add_image(self, image, duration_sec,\n",
    "                  show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                  time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                  display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds a single image to the video for a specified duration, with optional time display.\n",
    "\n",
    "        Args:\n",
    "            image: The image to add. Can be a file path (string), a Pillow Image object,\n",
    "                   or an OpenCV image (NumPy array).\n",
    "            duration_sec (float): The duration (in seconds) for which the image should be displayed.\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "                                              Defaults to self.default_show_time_text.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the text.\n",
    "                                                  Defaults to self.default_time_text_position.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the text.\n",
    "                                               Defaults to self.default_time_text_color.\n",
    "            time_text_font_scale (float, optional): Font scale factor.\n",
    "                                                    Defaults to self.default_time_text_font_scale.\n",
    "            time_text_thickness (int, optional): Line thickness for the text.\n",
    "                                                 Defaults to self.default_time_text_thickness.\n",
    "            time_text_font (int, optional): OpenCV font type.\n",
    "                                            Defaults to self.default_time_text_font.\n",
    "            display_time_start_offset (float, optional): The starting time (in seconds) to display\n",
    "                                                         on the video for this segment.\n",
    "                                                         Precedence: local param > global param > actual video current time.\n",
    "            time_display_format (str, optional): The format for the time string (e.g., \"HH:MM:SS.MS\").\n",
    "                                                 Placeholders are case-insensitive.\n",
    "                                                 Precedence: local param > default class param.\n",
    "        \"\"\"\n",
    "        frame = self._convert_to_cv2(image)\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "\n",
    "        # Resolve text display parameters\n",
    "        _show_text = self.default_show_time_text if show_time_text is None else show_time_text\n",
    "        _position = self.default_time_text_position if time_text_position is None else time_text_position\n",
    "        _color = self.default_time_text_color if time_text_color is None else time_text_color\n",
    "        _font_scale = self.default_time_text_font_scale if time_text_font_scale is None else time_text_font_scale\n",
    "        _thickness = self.default_time_text_thickness if time_text_thickness is None else time_text_thickness\n",
    "        _font = self.default_time_text_font if time_text_font is None else time_text_font\n",
    "        _time_format = self.default_time_display_format if time_display_format is None else time_display_format\n",
    "\n",
    "        # Determine the base time for display based on precedence\n",
    "        _base_display_time_for_this_segment = self.current_time # Default fallback: actual video time\n",
    "\n",
    "        if display_time_start_offset is not None:\n",
    "            # Local override takes highest precedence\n",
    "            _base_display_time_for_this_segment = display_time_start_offset\n",
    "        elif self.global_display_time_offset_start is not None:\n",
    "            # Global override applies if no local override. Use the continuous display_time_counter.\n",
    "            _base_display_time_for_this_segment = self.display_time_counter\n",
    "\n",
    "        for i in range(frame_count):\n",
    "            # Calculate the time to display for the current frame\n",
    "            current_display_time = _base_display_time_for_this_segment + (i / self.fps)\n",
    "            frame_to_write = frame.copy() # Use a copy to avoid drawing on the original frame for next iteration\n",
    "            if _show_text:\n",
    "                frame_to_write = self._draw_time_on_frame(\n",
    "                    frame_to_write, current_display_time, _position, _color, _font_scale, _thickness, _font, _time_format\n",
    "                )\n",
    "            self.video_writer.write(frame_to_write)\n",
    "\n",
    "        # Always update the actual video time\n",
    "        self.current_time += duration_sec\n",
    "\n",
    "        # Only update display_time_counter if global offset is active AND no local offset was used\n",
    "        if display_time_start_offset is None and self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration_sec\n",
    "\n",
    "\n",
    "    def add_images_from_list(self, images, total_duration_sec,\n",
    "                             show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                             time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                             display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds several images to the video, distributing them evenly over a total duration,\n",
    "        with optional time display on each image.\n",
    "\n",
    "        Args:\n",
    "            images: Can be a string (directory path), a list of strings (image file paths),\n",
    "                    a list of OpenCV images (numpy arrays), or a list of Pillow images.\n",
    "            total_duration_sec (float): The total duration (in seconds) that these images\n",
    "                                        should occupy in the video.\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "                                              Defaults to self.default_show_time_text.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the text.\n",
    "                                                  Defaults to self.default_time_text_position.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the text.\n",
    "                                               Defaults to self.default_time_text_color.\n",
    "            time_text_font_scale (float, optional): Font scale factor.\n",
    "                                                    Defaults to self.default_time_text_font_scale.\n",
    "            time_text_thickness (int, optional): Line thickness for the text.\n",
    "                                                 Defaults to self.default_time_text_thickness.\n",
    "            time_text_font (int, optional): OpenCV font type.\n",
    "                                            Defaults to self.default_time_text_font.\n",
    "            display_time_start_offset (float, optional): The starting time (in seconds) to display\n",
    "                                                         on the video for this segment.\n",
    "                                                         Precedence: local param > global param > actual video current time.\n",
    "            time_display_format (str, optional): The format for the time string (e.g., \"HH:MM:SS.MS\").\n",
    "                                                 Placeholders are case-insensitive.\n",
    "                                                 Precedence: local param > default class param.\n",
    "        \"\"\"\n",
    "        image_list = []\n",
    "\n",
    "        if isinstance(images, str) and os.path.isdir(images):\n",
    "            for filename in sorted(os.listdir(images)):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    image_list.append(os.path.join(images, filename))\n",
    "        elif isinstance(images, list):\n",
    "            if all(isinstance(img, str) for img in images):\n",
    "                image_list = images\n",
    "            elif all(isinstance(img, (np.ndarray, Image.Image)) for img in images):\n",
    "                image_list = images\n",
    "            else:\n",
    "                raise ValueError(\"List must contain only strings (paths), OpenCV images, or Pillow images.\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported 'images' type. Must be a directory path (string), a list of paths, a list of OpenCV images, or a list of Pillow images.\")\n",
    "\n",
    "        if not image_list:\n",
    "            print(\"No images found to add.\")\n",
    "            return\n",
    "\n",
    "        single_image_duration = total_duration_sec / len(image_list)\n",
    "\n",
    "        for i, img in enumerate(image_list):\n",
    "            # The display_time_start_offset passed to add_image needs to be carefully managed.\n",
    "            # If the user provided a display_time_start_offset to add_images_from_list,\n",
    "            # we calculate the offset for each individual image within that batch.\n",
    "            # If not, we pass None, allowing add_image to use the global_display_time_offset_start\n",
    "            # and its internal display_time_counter, which is the desired behavior for continuous time.\n",
    "            effective_display_offset_for_image_batch = None\n",
    "            if display_time_start_offset is not None:\n",
    "                effective_display_offset_for_image_batch = display_time_start_offset + (i * single_image_duration)\n",
    "\n",
    "            self.add_image(img, single_image_duration,\n",
    "                           show_time_text=show_time_text,\n",
    "                           time_text_position=time_text_position,\n",
    "                           time_text_color=time_text_color,\n",
    "                           time_text_font_scale=time_text_font_scale,\n",
    "                           time_text_thickness=time_text_thickness,\n",
    "                           time_text_font=time_text_font,\n",
    "                           display_time_start_offset=effective_display_offset_for_image_batch, # Pass the calculated offset or None\n",
    "                           time_display_format=time_display_format)\n",
    "\n",
    "    def add_text_below_image(self, image, text, duration_sec,\n",
    "                             text_box_height_ratio=0.2,\n",
    "                             background_color=(0, 0, 0),\n",
    "                             text_color=(255, 255, 255),\n",
    "                             text_horizontal_alignment=\"center\",\n",
    "                             text_vertical_alignment=\"center\",\n",
    "                             font_scale=0.7, thickness=2, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                             show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                             time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                             display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds an image with text displayed in a box below it for a given duration.\n",
    "        The combined content fits within the video's frame_size.\n",
    "\n",
    "        Args:\n",
    "            image: The input image (path, Pillow, or NumPy array).\n",
    "            text (str): The text string to display.\n",
    "            duration_sec (float): The duration (in seconds) for which this combined frame should be displayed.\n",
    "            text_box_height_ratio (float): Ratio (0.0 to 1.0) of the frame height allocated to the text box.\n",
    "            background_color (tuple): (B, G, R) color for the text box background.\n",
    "            text_color (tuple): (B, G, R) color for the text.\n",
    "            text_horizontal_alignment (str): Horizontal alignment of text (\"left\", \"center\", \"right\").\n",
    "            text_vertical_alignment (str): Vertical alignment of text (\"top\", \"center\", \"bottom\").\n",
    "            font_scale (float): Font scale factor for the text.\n",
    "            thickness (int): Line thickness for the text.\n",
    "            font (int): OpenCV font type (e.g., cv2.FONT_HERSHEY_SIMPLEX).\n",
    "            # Parameters for optional time display (same as add_image)\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the time text.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the time text.\n",
    "            time_text_font_scale (float, optional): Font scale factor for time text.\n",
    "            time_text_thickness (int, optional): Line thickness for time text.\n",
    "            time_text_font (int, optional): OpenCV font type for time text.\n",
    "            display_time_start_offset (float, optional): The starting time to display.\n",
    "            time_display_format (str, optional): The format for the time string.\n",
    "        \"\"\"\n",
    "        if not (0.0 <= text_box_height_ratio <= 1.0):\n",
    "            raise ValueError(\"text_box_height_ratio must be between 0.0 and 1.0.\")\n",
    "\n",
    "        original_img_frame = self._convert_to_cv2(image)\n",
    "        \n",
    "        video_width, video_height = self.frame_size\n",
    "        text_box_height = int(video_height * text_box_height_ratio)\n",
    "        image_display_height = video_height - text_box_height\n",
    "\n",
    "        if image_display_height <= 0:\n",
    "            raise ValueError(\"Image display height is zero or negative. Reduce text_box_height_ratio or increase frame_size.\")\n",
    "\n",
    "        resized_img_for_top = cv2.resize(original_img_frame, (video_width, image_display_height))\n",
    "\n",
    "        text_section = np.full((text_box_height, video_width, 3), background_color, dtype=np.uint8)\n",
    "\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        text_width, text_height_actual = text_size\n",
    "\n",
    "        text_x, text_y = 0, 0\n",
    "\n",
    "        if text_horizontal_alignment.lower() == \"center\":\n",
    "            text_x = (video_width - text_width) // 2\n",
    "        elif text_horizontal_alignment.lower() == \"right\":\n",
    "            text_x = video_width - text_width - 10\n",
    "        else: # Default to \"left\"\n",
    "            text_x = 10\n",
    "\n",
    "        if text_vertical_alignment.lower() == \"center\":\n",
    "            text_y = (text_box_height + text_height_actual) // 2\n",
    "        elif text_vertical_alignment.lower() == \"bottom\":\n",
    "            text_y = text_box_height - 10\n",
    "        else: # Default to \"top\"\n",
    "            text_y = text_height_actual + 10\n",
    "\n",
    "        cv2.putText(text_section, text, (text_x, text_y), font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        final_frame = np.vstack((resized_img_for_top, text_section))\n",
    "\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "\n",
    "        _show_text = self.default_show_time_text if show_time_text is None else show_time_text\n",
    "        _time_position = self.default_time_text_position if time_text_position is None else time_text_position\n",
    "        _time_color = self.default_time_text_color if time_text_color is None else time_text_color\n",
    "        _time_font_scale = self.default_time_text_font_scale if time_text_font_scale is None else time_text_font_scale\n",
    "        _time_thickness = self.default_time_text_thickness if time_text_thickness is None else time_text_thickness\n",
    "        _time_font = self.default_time_text_font if time_text_font is None else time_text_font\n",
    "        _time_format = self.default_time_display_format if time_display_format is None else time_display_format\n",
    "\n",
    "        _base_display_time_for_this_segment = self.current_time\n",
    "        if display_time_start_offset is not None:\n",
    "            _base_display_time_for_this_segment = display_time_start_offset\n",
    "        elif self.global_display_time_offset_start is not None:\n",
    "            _base_display_time_for_this_segment = self.display_time_counter\n",
    "\n",
    "\n",
    "        for i in range(frame_count):\n",
    "            current_display_time = _base_display_time_for_this_segment + (i / self.fps)\n",
    "            frame_to_write = final_frame.copy()\n",
    "            if _show_text:\n",
    "                frame_to_write = self._draw_time_on_frame(\n",
    "                    frame_to_write, current_display_time, _time_position, _time_color, _time_font_scale, _time_thickness, _time_font, _time_format\n",
    "                )\n",
    "            self.video_writer.write(frame_to_write)\n",
    "        \n",
    "        self.current_time += duration_sec\n",
    "\n",
    "        if display_time_start_offset is None and self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration_sec\n",
    "\n",
    "    def add_video(self, video_path):\n",
    "        \"\"\"\n",
    "        Adds another video clip to the current video. The added video retains its original duration.\n",
    "\n",
    "        Args:\n",
    "            video_path (str): Path to the video file to be added.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise FileNotFoundError(f\"Video file not found or could not be opened: {video_path}\")\n",
    "\n",
    "        video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if video_fps == 0:\n",
    "            video_fps = self.fps\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = self._resize_frame(frame)\n",
    "            self.video_writer.write(frame)\n",
    "\n",
    "        duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / video_fps\n",
    "        cap.release()\n",
    "        self.current_time += duration\n",
    "        # If global display offset is active, and we are adding a video (which doesn't have its own\n",
    "        # display_time_start_offset parameter), we should also advance the display_time_counter.\n",
    "        if self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration\n",
    "\n",
    "\n",
    "    def add_audio(self, audio_path, audio_clip_start=None, audio_clip_end=None, video_start_offset=None):\n",
    "        \"\"\"\n",
    "        Adds an audio clip to the video timeline.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "            audio_clip_start (float, optional): The start time (in seconds) within the audio file itself.\n",
    "                                                Defaults to 0 (beginning of the audio file).\n",
    "            audio_clip_end (float, optional): The end time (in seconds) within the audio file itself.\n",
    "                                              Defaults to the end of the audio clip.\n",
    "            video_start_offset (float, optional): The time (in seconds) on the video timeline where this\n",
    "                                                  audio should start. If None, it starts at the current\n",
    "                                                  end time of the video (`self.current_time`).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not load audio file {audio_path}: {e}\")\n",
    "\n",
    "        if audio_clip_start is not None or audio_clip_end is not None:\n",
    "            if audio_clip_start is not None and audio_clip_end is not None and audio_clip_start > audio_clip_end:\n",
    "                raise ValueError(\"audio_clip_start cannot be greater than audio_clip_end.\")\n",
    "            \n",
    "            start_subclip = audio_clip_start if audio_clip_start is not None else 0\n",
    "            end_subclip = audio_clip_end if audio_clip_end is not None else audio_clip.duration\n",
    "            \n",
    "            audio_clip = audio_clip.subclip(start_subclip, end_subclip)\n",
    "\n",
    "        offset_on_video = video_start_offset if video_start_offset is not None else self.current_time\n",
    "\n",
    "        self.audio_clips.append((audio_clip, offset_on_video))\n",
    "\n",
    "    def get_video_duration(self):\n",
    "        \"\"\"\n",
    "        Returns the current duration of the video content in seconds.\n",
    "        \"\"\"\n",
    "        return self.current_time\n",
    "\n",
    "    def save(self, output_path):\n",
    "        \"\"\"\n",
    "        Finalizes the video and merges audio if present.\n",
    "        \"\"\"\n",
    "        self.video_writer.release()\n",
    "\n",
    "        final_clip = VideoFileClip(self.temp_video_path)\n",
    "\n",
    "        if self.audio_clips:\n",
    "            all_audios = []\n",
    "            for audio, offset in self.audio_clips:\n",
    "                all_audios.append(audio.set_start(offset))\n",
    "            \n",
    "            composite_audio = CompositeAudioClip(all_audios)\n",
    "            \n",
    "            final_clip = final_clip.set_audio(composite_audio)\n",
    "\n",
    "        print(f\"Saving video to {output_path}...\")\n",
    "        final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "        final_clip.close()\n",
    "        os.remove(self.temp_video_path)\n",
    "        print(\"Video saved successfully and temporary file removed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03befd",
   "metadata": {},
   "source": [
    "## SD15ImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3de60dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SD15ImageGenerator:\n",
    "    def __init__(self, model_id=\"runwayml/stable-diffusion-v1-5\", use_cuda=True, num_inference_steps=25):\n",
    "        \"\"\"\n",
    "        Initialize the Stable Diffusion 1.5 pipeline and inference settings.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.intermediate_images = []\n",
    "\n",
    "        # Load the safety checker and feature extractor\n",
    "        # You might need to specify the subfolder if they are not at the top level of the model_id\n",
    "        safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        feature_extractor = CLIPFeatureExtractor.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        self.negative_prompt = \"\"\"deformed, distorted, disfigured, bad anatomy, ugly, tiling, poorly drawn hands, poorly drawn face, \n",
    "                                  out of frame, low quality, jpeg artifacts, duplicate, morbid, mutilated, extra fingers, mutated hands,  mutation, blurry, dehydrated, \n",
    "                                  bad proportions, extra limbs, cloned face, gross proportions, malformed limbs, missing arms, missing legs, extra hands, fused fingers, wrong hand, \n",
    "                                  long neck, worst quality, watermark, signature, text, error, cropped, username, logo, lowres, oversaturated, washed out, \n",
    "                                  cloned, bad composition, crosseyed , squint, lazy eye , bad eyes, wrong eyes, missing teeth, bad teeth, ugly teeth, open mouth, too many teeth,\n",
    "                                  extra tongue, wrong mouth, ugly mouth, bad mouth, bad nose, ugly nose, wrong nose, missing nose, bad ear, ugly ear, wrong ear, missing ear,\n",
    "                                  extra ear, double ear, three ears, mutated ear, long ear, short ear, big ear, small ear,\n",
    "                                  bad hair, ugly hair, wrong hair, missing hair, bad skin, ugly skin, wrong skin, \n",
    "                                  missing skin, extra skin, mutated skin, bad clothing, ugly clothing, wrong clothing,missing clothing, mutated clothing, \n",
    "                                  big clothing, small clothing, bad background, ugly background, wrong background, bad lighting, ugly lighting, wrong lighting\"\"\"\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "             safety_checker=safety_checker,\n",
    "            feature_extractor=feature_extractor # Don't forget the feature_extractor\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _capture_step(self, step, timestep, latents):\n",
    "        \"\"\"\n",
    "        Internal callback to capture the image at each step.\n",
    "        \"\"\"\n",
    "        # Decode latent to image at this step\n",
    "        with torch.no_grad():\n",
    "            image = self.pipe.vae.decode(latents / self.pipe.vae.config.scaling_factor).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image_pil = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "            self.intermediate_images.append(image_pil)\n",
    "\n",
    "    def generate_image(self, prompt, negative_prompt=None, guidance_scale=7.5):\n",
    "        \"\"\"\n",
    "        Generate image and collect intermediate steps.\n",
    "        Returns a list of PIL images (one per step).\n",
    "        \"\"\"\n",
    "        self.intermediate_images = []\n",
    "        negative_prompt = negative_prompt or self.negative_prompt\n",
    "\n",
    "        with torch.autocast(self.device) if self.device == \"cuda\" else torch.no_grad():\n",
    "            _ = self.pipe(\n",
    "                prompt=\"high resolution image of: \"+prompt +\" ,8K, best quality, masterpiece, photorealistic, ultra-detailed, sharp focus\",\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=self.num_inference_steps,\n",
    "                callback=self._capture_step,\n",
    "                callback_steps=1  # capture every step\n",
    "            )\n",
    "\n",
    "        return self.intermediate_images\n",
    "\n",
    "    def save_image(self, image: Image.Image, output_path: str):\n",
    "        \"\"\"\n",
    "        Save a single PIL image to the specified path.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        image.save(output_path)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "    def save_images(self, images, directory=\"generated\"):\n",
    "        \"\"\"\n",
    "        Save a list of images to the given directory.\n",
    "        \"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        for i, img in enumerate(images):\n",
    "            path = os.path.join(directory, f\"step_{i:02d}.png\")\n",
    "            img.save(path)\n",
    "        print(f\"Saved {len(images)} images to '{directory}/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SDXL35ImageGenerator:\n",
    "    def __init__(self, model_id=\"stabilityai/stable-diffusion-xl-base-1.0\", use_cuda=True, num_inference_steps=25):\n",
    "        \"\"\"\n",
    "        Initialize the Stable Diffusion XL 3.5 Medium pipeline and inference settings.\n",
    "        Note: SDXL 3.5 Medium would use a specific model ID when available.\n",
    "        Using SDXL base as template - replace with actual SDXL 3.5 Medium model ID when released.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.intermediate_images = []\n",
    "        \n",
    "        # Comprehensive negative prompt optimized for SDXL\n",
    "        self.negative_prompt = \"\"\"deformed, distorted, disfigured, bad anatomy, ugly, tiling, poorly drawn hands, poorly drawn face, \n",
    "                                  out of frame, low quality, jpeg artifacts, duplicate, morbid, mutilated, extra fingers, mutated hands, \n",
    "                                  mutation, blurry, dehydrated, bad proportions, extra limbs, cloned face, gross proportions, \n",
    "                                  malformed limbs, missing arms, missing legs, extra hands, fused fingers, wrong hand, long neck, \n",
    "                                  worst quality, watermark, signature, text, error, cropped, username, logo, lowres, oversaturated, \n",
    "                                  washed out, cloned, bad composition, crosseyed, squint, lazy eye, bad eyes, wrong eyes, missing teeth, \n",
    "                                  bad teeth, ugly teeth, open mouth, too many teeth, extra tongue, wrong mouth, ugly mouth, bad mouth, \n",
    "                                  bad nose, ugly nose, wrong nose, missing nose, bad ear, ugly ear, wrong ear, missing ear, extra ear, \n",
    "                                  double ear, three ears, mutated ear, long ear, short ear, big ear, small ear, bad hair, ugly hair, \n",
    "                                  wrong hair, missing hair, bad skin, ugly skin, wrong skin, missing skin, extra skin, mutated skin, \n",
    "                                  bad clothing, ugly clothing, wrong clothing, missing clothing, mutated clothing, big clothing, \n",
    "                                  small clothing, bad background, ugly background, wrong background, bad lighting, ugly lighting, \n",
    "                                  wrong lighting, plastic, artificial, cartoon, anime, painting, sketch, drawing\"\"\"\n",
    "\n",
    "        # Load SDXL pipeline\n",
    "        self.pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\" if self.device == \"cuda\" else None\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Enable memory efficient attention if using CUDA\n",
    "        if self.device == \"cuda\":\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "            \n",
    "        # Initialize tokenizer for prompt processing\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"\n",
    "        Count the number of tokens in a text string.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        return len(tokens)\n",
    "    \n",
    "    def split_long_prompt(self, prompt, max_tokens=75):\n",
    "        \"\"\"\n",
    "        Split a long prompt into chunks that fit within token limits.\n",
    "        Preserves sentence structure when possible.\n",
    "        \"\"\"\n",
    "        if self.count_tokens(prompt) <= max_tokens:\n",
    "            return [prompt]\n",
    "        \n",
    "        # First try splitting by sentences\n",
    "        sentences = re.split(r'[.!?]+', prompt)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_chunk = current_chunk + (\", \" if current_chunk else \"\") + sentence\n",
    "            \n",
    "            if self.count_tokens(test_chunk) <= max_tokens:\n",
    "                current_chunk = test_chunk\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = sentence\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        # If sentences are still too long, split by commas\n",
    "        final_chunks = []\n",
    "        for chunk in chunks:\n",
    "            if self.count_tokens(chunk) <= max_tokens:\n",
    "                final_chunks.append(chunk)\n",
    "            else:\n",
    "                # Split by commas\n",
    "                parts = chunk.split(',')\n",
    "                current_part = \"\"\n",
    "                \n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    test_part = current_part + (\", \" if current_part else \"\") + part\n",
    "                    \n",
    "                    if self.count_tokens(test_part) <= max_tokens:\n",
    "                        current_part = test_part\n",
    "                    else:\n",
    "                        if current_part:\n",
    "                            final_chunks.append(current_part)\n",
    "                        current_part = part\n",
    "                \n",
    "                if current_part:\n",
    "                    final_chunks.append(current_part)\n",
    "        \n",
    "        return final_chunks\n",
    "    \n",
    "    def compress_prompt(self, prompt, max_tokens=75):\n",
    "        \"\"\"\n",
    "        Compress a long prompt by removing less important words while preserving meaning.\n",
    "        \"\"\"\n",
    "        # Words to remove first (less important)\n",
    "        filler_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'very', 'really', 'quite', 'somewhat']\n",
    "        \n",
    "        if self.count_tokens(prompt) <= max_tokens:\n",
    "            return prompt\n",
    "        \n",
    "        words = prompt.split()\n",
    "        \n",
    "        # Remove filler words first\n",
    "        filtered_words = [word for word in words if word.lower().rstrip('.,!?;:') not in filler_words]\n",
    "        compressed = ' '.join(filtered_words)\n",
    "        \n",
    "        if self.count_tokens(compressed) <= max_tokens:\n",
    "            return compressed\n",
    "        \n",
    "        # If still too long, keep only the most important words (nouns, adjectives, verbs)\n",
    "        # This is a simple heuristic - you might want to use a proper NLP library\n",
    "        important_words = []\n",
    "        for word in filtered_words:\n",
    "            clean_word = word.rstrip('.,!?;:')\n",
    "            # Keep capitalized words (likely important), longer words, and quoted phrases\n",
    "            if (len(clean_word) > 3 or \n",
    "                clean_word[0].isupper() or \n",
    "                '\"' in word or \n",
    "                \"'\" in word):\n",
    "                important_words.append(word)\n",
    "        \n",
    "        return ' '.join(important_words)\n",
    "    \n",
    "    def prioritize_prompt_elements(self, prompt):\n",
    "        \"\"\"\n",
    "        Extract and prioritize the most important elements from a long prompt.\n",
    "        Returns a shortened version focusing on key visual elements.\n",
    "        \"\"\"\n",
    "        # Extract quoted phrases (usually important)\n",
    "        quoted_phrases = re.findall(r'\"([^\"]*)\"', prompt)\n",
    "        \n",
    "        # Extract capitalized words/phrases (likely important subjects)\n",
    "        capitalized = re.findall(r'\\b[A-Z][a-zA-Z]+(?:\\s+[A-Z][a-zA-Z]+)*\\b', prompt)\n",
    "        \n",
    "        # Common important visual keywords\n",
    "        visual_keywords = [\n",
    "            'portrait', 'landscape', 'close-up', 'wide shot', 'macro', 'aerial',\n",
    "            'lighting', 'dramatic', 'soft', 'harsh', 'golden hour', 'sunset', 'sunrise',\n",
    "            'color', 'vibrant', 'muted', 'monochrome', 'black and white',\n",
    "            'style', 'realistic', 'photorealistic', 'artistic', 'painting', 'sketch',\n",
    "            'quality', 'high resolution', '4K', '8K', 'masterpiece', 'detailed'\n",
    "        ]\n",
    "        \n",
    "        # Extract visual keywords from prompt\n",
    "        found_keywords = []\n",
    "        prompt_lower = prompt.lower()\n",
    "        for keyword in visual_keywords:\n",
    "            if keyword in prompt_lower:\n",
    "                found_keywords.append(keyword)\n",
    "        \n",
    "        # Build prioritized prompt\n",
    "        priority_elements = []\n",
    "        \n",
    "        # Add quoted phrases (highest priority)\n",
    "        priority_elements.extend(quoted_phrases)\n",
    "        \n",
    "        # Add capitalized terms\n",
    "        priority_elements.extend(capitalized[:3])  # Limit to first 3\n",
    "        \n",
    "        # Add important visual keywords\n",
    "        priority_elements.extend(found_keywords[:5])  # Limit to first 5\n",
    "        \n",
    "        # Combine with main subject (first few words of original prompt)\n",
    "        main_subject = ' '.join(prompt.split()[:10])\n",
    "        \n",
    "        final_prompt = main_subject\n",
    "        if priority_elements:\n",
    "            final_prompt += ', ' + ', '.join(set(priority_elements))\n",
    "        \n",
    "        return final_prompt\n",
    "        \"\"\"\n",
    "        Internal callback to capture the image at each step.\n",
    "        \"\"\"\n",
    "        # Decode latent to image at this step\n",
    "        with torch.no_grad():\n",
    "            # SDXL uses a different VAE scaling factor\n",
    "            image = self.pipe.vae.decode(latents / self.pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0]\n",
    "            image_pil = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "            self.intermediate_images.append(image_pil)\n",
    "\n",
    "    def _capture_step(self, step, timestep, latents):\n",
    "        \"\"\"\n",
    "        Internal callback to capture the image at each step.\n",
    "        \"\"\"\n",
    "        # Decode latent to image at this step\n",
    "        with torch.no_grad():\n",
    "            # SDXL uses a different VAE scaling factor\n",
    "            image = self.pipe.vae.decode(latents / self.pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0]\n",
    "            image_pil = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "            self.intermediate_images.append(image_pil)\n",
    "\n",
    "    def generate_image_long_prompt(self, long_prompt, negative_prompt=None, guidance_scale=7.5, width=1024, height=1024, method=\"compress\"):\n",
    "        \"\"\"\n",
    "        Generate image from a long prompt (>77 tokens) using various handling methods.\n",
    "        \n",
    "        Args:\n",
    "            long_prompt (str): Long text prompt\n",
    "            negative_prompt (str, optional): Negative prompt\n",
    "            guidance_scale (float): Guidance scale\n",
    "            width (int): Image width\n",
    "            height (int): Image height\n",
    "            method (str): Method to handle long prompt:\n",
    "                - \"compress\": Intelligently compress the prompt\n",
    "                - \"prioritize\": Extract most important elements\n",
    "                - \"split\": Split into chunks and use first chunk\n",
    "                - \"truncate\": Simple truncation to 77 tokens\n",
    "        \"\"\"\n",
    "        print(f\"Original prompt length: {self.count_tokens(long_prompt)} tokens\")\n",
    "        \n",
    "        if method == \"compress\":\n",
    "            processed_prompt = self.compress_prompt(long_prompt)\n",
    "        elif method == \"prioritize\":\n",
    "            processed_prompt = self.prioritize_prompt_elements(long_prompt)\n",
    "        elif method == \"split\":\n",
    "            chunks = self.split_long_prompt(long_prompt)\n",
    "            processed_prompt = chunks[0]  # Use first chunk\n",
    "            if len(chunks) > 1:\n",
    "                print(f\"Prompt split into {len(chunks)} chunks. Using first chunk.\")\n",
    "        elif method == \"truncate\":\n",
    "            words = long_prompt.split()\n",
    "            processed_prompt = ' '.join(words[:50])  # Simple word truncation\n",
    "        else:\n",
    "            processed_prompt = long_prompt\n",
    "        \n",
    "        print(f\"Processed prompt length: {self.count_tokens(processed_prompt)} tokens\")\n",
    "        print(f\"Processed prompt: {processed_prompt[:100]}{'...' if len(processed_prompt) > 100 else ''}\")\n",
    "        \n",
    "        return self.generate_image(processed_prompt, negative_prompt, guidance_scale, width, height)\n",
    "    \n",
    "    def generate_multi_prompt_blend(self, long_prompt, negative_prompt=None, guidance_scale=7.5, width=1024, height=1024):\n",
    "        \"\"\"\n",
    "        Handle long prompts by splitting into chunks and generating multiple images,\n",
    "        then you can choose the best one or blend them.\n",
    "        \"\"\"\n",
    "        chunks = self.split_long_prompt(long_prompt, max_tokens=75)\n",
    "        print(f\"Split long prompt into {len(chunks)} chunks:\")\n",
    "        \n",
    "        all_images = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Chunk {i+1}: {chunk}\")\n",
    "            images = self.generate_image(chunk, negative_prompt, guidance_scale, width, height)\n",
    "            # Take the final image from each chunk\n",
    "            all_images.append(images[-1])\n",
    "        \n",
    "        return all_images\n",
    "    def generate_image(self, prompt, negative_prompt=None, guidance_scale=7.5, width=1024, height=1024, prompt_strength=1.0):\n",
    "        \"\"\"\n",
    "        Generate image and collect intermediate steps.\n",
    "        Returns a list of PIL images (one per step).\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Text prompt for image generation\n",
    "            negative_prompt (str, optional): Negative prompt to avoid unwanted elements\n",
    "            guidance_scale (float): How closely to follow the prompt (1.0 to 20.0)\n",
    "            width (int): Image width (default 1024 for SDXL)\n",
    "            height (int): Image height (default 1024 for SDXL)\n",
    "            prompt_strength (float): Multiplier for prompt emphasis (1.0 = normal, 2.0 = double emphasis)\n",
    "        \"\"\"\n",
    "        # Check if prompt is too long\n",
    "        if self.count_tokens(prompt) > 75:\n",
    "            print(f\"Warning: Prompt is {self.count_tokens(prompt)} tokens. SDXL limit is 77 tokens.\")\n",
    "            print(\"Consider using generate_image_long_prompt() method instead.\")\n",
    "        \n",
    "        self.intermediate_images = []\n",
    "        negative_prompt = negative_prompt or self.negative_prompt\n",
    "\n",
    "        # Enhanced prompt for better quality with optional strength multiplier\n",
    "        base_prompt = f\"high resolution image of: {prompt}, 8K, best quality, masterpiece, photorealistic, ultra-detailed, sharp focus, professional photography\"\n",
    "        \n",
    "        # Apply prompt strength by repeating key elements\n",
    "        if prompt_strength > 1.0:\n",
    "            strength_multiplier = int(prompt_strength)\n",
    "            repeated_prompt = \", \".join([prompt] * strength_multiplier)\n",
    "            enhanced_prompt = f\"high resolution image of: {repeated_prompt}, 8K, best quality, masterpiece, photorealistic, ultra-detailed, sharp focus, professional photography\"\n",
    "        else:\n",
    "            enhanced_prompt = base_prompt\n",
    "\n",
    "        with torch.autocast(self.device) if self.device == \"cuda\" else torch.no_grad():\n",
    "            _ = self.pipe(\n",
    "                prompt=enhanced_prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=self.num_inference_steps,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                callback=self._capture_step,\n",
    "                callback_steps=1  # capture every step\n",
    "            )\n",
    "\n",
    "        return self.intermediate_images\n",
    "\n",
    "    def generate_batch(self, prompts, negative_prompt=None, guidance_scale=7.5, width=1024, height=1024):\n",
    "        \"\"\"\n",
    "        Generate multiple images from a list of prompts.\n",
    "        \n",
    "        Args:\n",
    "            prompts (list): List of text prompts\n",
    "            negative_prompt (str, optional): Negative prompt to avoid unwanted elements\n",
    "            guidance_scale (float): How closely to follow the prompt\n",
    "            width (int): Image width\n",
    "            height (int): Image height\n",
    "            \n",
    "        Returns:\n",
    "            list: List of PIL images\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        negative_prompt = negative_prompt or self.negative_prompt\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            enhanced_prompt = f\"high resolution image of: {prompt}, 8K, best quality, masterpiece, photorealistic, ultra-detailed, sharp focus, professional photography\"\n",
    "            \n",
    "            with torch.autocast(self.device) if self.device == \"cuda\" else torch.no_grad():\n",
    "                result = self.pipe(\n",
    "                    prompt=enhanced_prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    num_inference_steps=self.num_inference_steps,\n",
    "                    width=width,\n",
    "                    height=height\n",
    "                )\n",
    "                images.extend(result.images)\n",
    "        \n",
    "        return images\n",
    "\n",
    "    def save_image(self, image: Image.Image, output_path: str):\n",
    "        \"\"\"\n",
    "        Save a single PIL image to the specified path.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        image.save(output_path, quality=95)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "\n",
    "    def save_images(self, images, directory=\"generated_sdxl\"):\n",
    "        \"\"\"\n",
    "        Save a list of images to the given directory.\n",
    "        \"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        for i, img in enumerate(images):\n",
    "            path = os.path.join(directory, f\"step_{i:02d}.png\")\n",
    "            img.save(path, quality=95)\n",
    "        print(f\"Saved {len(images)} images to '{directory}/'\")\n",
    "\n",
    "    def create_grid(self, images, rows=2, cols=2):\n",
    "        \"\"\"\n",
    "        Create a grid from multiple images.\n",
    "        \n",
    "        Args:\n",
    "            images (list): List of PIL images\n",
    "            rows (int): Number of rows in grid\n",
    "            cols (int): Number of columns in grid\n",
    "            \n",
    "        Returns:\n",
    "            PIL.Image: Grid image\n",
    "        \"\"\"\n",
    "        return make_image_grid(images[:rows*cols], rows=rows, cols=cols)\n",
    "\n",
    "    def enable_cpu_offload(self):\n",
    "        \"\"\"\n",
    "        Enable CPU offloading to save GPU memory.\n",
    "        \"\"\"\n",
    "        if self.device == \"cuda\":\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "\n",
    "    def enable_attention_slicing(self, slice_size=\"auto\"):\n",
    "        \"\"\"\n",
    "        Enable attention slicing to reduce memory usage.\n",
    "        \"\"\"\n",
    "        self.pipe.enable_attention_slicing(slice_size)\n",
    "\n",
    "    def emphasize_prompt(self, prompt, emphasis_level=1):\n",
    "        \"\"\"\n",
    "        Add emphasis to specific parts of a prompt using attention syntax.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Original prompt\n",
    "            emphasis_level (int): Level of emphasis (1-3)\n",
    "            \n",
    "        Returns:\n",
    "            str: Emphasized prompt\n",
    "        \"\"\"\n",
    "        if emphasis_level == 1:\n",
    "            return f\"({prompt})\"\n",
    "        elif emphasis_level == 2:\n",
    "            return f\"(({prompt}))\"\n",
    "        elif emphasis_level == 3:\n",
    "            return f\"((({prompt})))\"\n",
    "        else:\n",
    "            return prompt\n",
    "\n",
    "    def create_weighted_prompt(self, prompt_parts):\n",
    "        \"\"\"\n",
    "        Create a prompt with weighted parts.\n",
    "        \n",
    "        Args:\n",
    "            prompt_parts (dict): Dictionary with prompts as keys and weights as values\n",
    "            Example: {\"beautiful woman\": 1.2, \"red dress\": 1.5, \"sunset background\": 0.8}\n",
    "            \n",
    "        Returns:\n",
    "            str: Weighted prompt string\n",
    "        \"\"\"\n",
    "        weighted_parts = []\n",
    "        for part, weight in prompt_parts.items():\n",
    "            if weight == 1.0:\n",
    "                weighted_parts.append(part)\n",
    "            else:\n",
    "                weighted_parts.append(f\"({part}:{weight})\")\n",
    "        \n",
    "        return \", \".join(weighted_parts)\n",
    "\n",
    "    def enable_vae_slicing(self):\n",
    "        \"\"\"\n",
    "        Enable VAE slicing to reduce memory usage during decoding.\n",
    "        \"\"\"\n",
    "        self.pipe.enable_vae_slicing()\n",
    "\n",
    "# Example usage with long prompts:\n",
    "\"\"\"\n",
    "# Initialize the generator\n",
    "generator = SDXL35ImageGenerator(use_cuda=True, num_inference_steps=30)\n",
    "\n",
    "# Your 150-word long prompt example\n",
    "long_prompt = '''A breathtakingly beautiful and ethereal fantasy landscape featuring a majestic ancient castle perched atop a floating island suspended in mid-air among swirling clouds and mist. The castle should have intricate Gothic architecture with tall spires, ornate windows, and weathered stone walls covered in ivy. Below the floating island, a vast enchanted forest stretches to the horizon, filled with bioluminescent plants and magical creatures. The sky should display a stunning aurora borealis with vibrant colors of green, purple, and blue dancing across the heavens. In the foreground, a crystal-clear lake reflects the magical scene above, with lily pads and glowing water flowers floating on its surface. Ancient runes should be carved into stone monuments around the lake's edge, glowing with mystical energy. The overall atmosphere should be mysterious and otherworldly, with soft, dreamlike lighting that creates a sense of wonder and enchantment.'''\n",
    "\n",
    "# Method 1: Compress the prompt intelligently (RECOMMENDED)\n",
    "images = generator.generate_image_long_prompt(\n",
    "    long_prompt, \n",
    "    method=\"compress\",\n",
    "    guidance_scale=10.0\n",
    ")\n",
    "\n",
    "# Method 2: Extract and prioritize most important elements\n",
    "images = generator.generate_image_long_prompt(\n",
    "    long_prompt, \n",
    "    method=\"prioritize\",\n",
    "    guidance_scale=10.0\n",
    ")\n",
    "\n",
    "# Method 3: Split into multiple chunks and generate multiple images\n",
    "chunk_images = generator.generate_multi_prompt_blend(\n",
    "    long_prompt,\n",
    "    guidance_scale=10.0\n",
    ")\n",
    "\n",
    "# Method 4: Check token count first\n",
    "token_count = generator.count_tokens(long_prompt)\n",
    "print(f\"Your prompt has {token_count} tokens (limit is 77)\")\n",
    "\n",
    "# Method 5: Manual compression by removing filler words\n",
    "compressed = generator.compress_prompt(long_prompt, max_tokens=75)\n",
    "images = generator.generate_image(compressed)\n",
    "\n",
    "# Save the results\n",
    "generator.save_images(images, \"long_prompt_results\")\n",
    "if 'chunk_images' in locals():\n",
    "    for i, img in enumerate(chunk_images):\n",
    "        generator.save_image(img, f\"long_prompt_results/chunk_{i}.png\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7fea56",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc74ba",
   "metadata": {},
   "source": [
    "## generate_evenly_distributed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a5f139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_evenly_distributed_values(data):\n",
    "    \"\"\"\n",
    "    Generates evenly distributed values for each tuple (number, start, end) in a list.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of tuples, where each tuple is (number, start, end).\n",
    "                     'number' is the count of values to generate, 'start' is the\n",
    "                     beginning of the range, and 'end' is the end of the range.\n",
    "\n",
    "    Returns:\n",
    "        list: A single list containing all the generated evenly distributed values.\n",
    "    \"\"\"\n",
    "    all_values = []\n",
    "    for num, start, end in data:\n",
    "        # Generate 'num' evenly distributed values between 'start' and 'end'\n",
    "        # np.linspace includes both start and end points\n",
    "        if num > 0:\n",
    "            generated_values = np.linspace(start, end, num).tolist()\n",
    "            all_values.extend(generated_values)\n",
    "    return all_values\n",
    "\n",
    "# Example Usage:\n",
    "# data1 = [(5, 0, 10), (3, 100, 102)]\n",
    "# result1 = generate_evenly_distributed_values(data1)\n",
    "# print(f\"Result for data1: {result1}\")\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]\n",
    "\n",
    "# data2 = [(1, 5, 5), (4, -2, 2)]\n",
    "# result2 = generate_evenly_distributed_values(data2)\n",
    "# print(f\"Result for data2: {result2}\")\n",
    "# # Expected output for data2: [5.0, -2.0, -0.6666666666666666, 0.6666666666666666, 2.0]\n",
    "\n",
    "# data3 = []\n",
    "# result3 = generate_evenly_distributed_values(data3)\n",
    "# print(f\"Result for data3: {result3}\")\n",
    "# # Expected output for data3: []\n",
    "\n",
    "# data4 = [(0, 1, 10)]\n",
    "# result4 = generate_evenly_distributed_values(data4)\n",
    "# print(f\"Result for data4: {result4}\")\n",
    "# # Expected output for data4: []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9e336",
   "metadata": {},
   "source": [
    "## generate_filename_by_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cf975f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_filename_by_datetime(postfix:str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a filename based on the current date and time with a specified extension.\n",
    "\n",
    "    The format of the filename will be 'YYYY-MM-DD-HH-MM-SS.extension'.\n",
    "\n",
    "    Args:\n",
    "        extension (str): The file extension (e.g., 'mp4', 'txt', 'jpg').\n",
    "                         It should not include the leading dot.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated filename.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # Format the datetime object into a string\n",
    "    # YYYY: Year with century\n",
    "    # MM: Month as a zero-padded decimal number\n",
    "    # DD: Day of the month as a zero-padded decimal number\n",
    "    # HH: Hour (24-hour clock) as a zero-padded decimal number\n",
    "    # MM: Minute as a zero-padded decimal number\n",
    "    # SS: Second as a zero-padded decimal number\n",
    "    timestamp_str = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    # Construct the full filename\n",
    "    filename = f\"{timestamp_str}_{postfix}.{extension}\"\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c21ee",
   "metadata": {},
   "source": [
    "## add_text_to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11b626cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_text_to_image(\n",
    "    image,\n",
    "    text: str,\n",
    "    org: tuple[int | None, int | None] = (10, 30),  # Bottom-left corner of the text string\n",
    "    font_face: int = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    font_scale: float = 1.0,\n",
    "    color: tuple[int, int, int] = (0, 0, 0),  # BGR color (Black by default)\n",
    "    thickness: int = 2,\n",
    "    background_color: tuple[int, int, int] = (255, 255, 255), # White background for new canvas\n",
    "    text_background_color: tuple[int, int, int] = (255, 255, 255), # White background for text by default\n",
    "    text_background_transparency: float = 0.8, # 80% transparency by default\n",
    "    padding_x: int = 20, # Horizontal padding for text background\n",
    "    padding_y: int = 20, # Vertical padding for text background\n",
    "    wordwrap: bool = False # New parameter: if True, wraps text to fit image width\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds text to an image, extending the image size if the text falls outside\n",
    "    the original boundaries. Supports word wrapping.\n",
    "\n",
    "    Args:\n",
    "        image: The input image. Can be an OpenCV (numpy.ndarray) or Pillow (PIL.Image.Image) image.\n",
    "        text (str): The text string to add.\n",
    "        org (tuple[int | None, int | None]): The bottom-left corner of the text string in (x, y) coordinates.\n",
    "                               Defaults to (10, 30). If x or y is None, it will be centered in that direction.\n",
    "        font_face (int): Font type. See cv2.FONT_HERSHEY_* for options.\n",
    "                         Defaults to cv2.FONT_HERSHEY_SIMPLEX.\n",
    "        font_scale (float): Font scale factor multiplied by the font-specific base size.\n",
    "                            Defaults to 1.0.\n",
    "        color (tuple[int, int, int]): Text color in BGR format. Defaults to (0, 0, 0) (Black).\n",
    "        thickness (int): Thickness of the text lines. Defaults to 2.\n",
    "        background_color (tuple[int, int, int]): Color to fill the extended canvas if the image\n",
    "                                                  needs to be resized. Defaults to (255, 255, 255) (White).\n",
    "        text_background_color (tuple[int, int, int]): Color of the text's background in BGR format.\n",
    "                                                       Defaults to (255, 255, 255) (White).\n",
    "        text_background_transparency (float): Transparency of the text background.\n",
    "                                              Value between 0.0 (fully transparent) and 1.0 (fully opaque).\n",
    "                                              Defaults to 0.8 (80% transparent).\n",
    "        padding_x (int): Horizontal padding to add around the text background. Defaults to 5 pixels.\n",
    "        padding_y (int): Vertical padding to add around the text background. Defaults to 5 pixels.\n",
    "        wordwrap (bool): If True, wraps text to fit within the image width, breaking at spaces.\n",
    "                         Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with the added text, in OpenCV (BGR) format.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Handle Image Input: Convert Pillow image to OpenCV format if necessary\n",
    "    if isinstance(image, Image.Image):\n",
    "        img_np = np.array(image)\n",
    "        if img_np.ndim == 2: # Grayscale image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
    "        elif img_np.shape[2] == 4: # RGBA image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR)\n",
    "        else: # RGB image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        img_cv = image\n",
    "        # Ensure the image is BGR (3 channels) if it's grayscale\n",
    "        if img_cv.ndim == 2:\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_GRAY2BGR)\n",
    "        elif img_cv.shape[2] == 4: # Handle RGBA if passed as numpy array\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGBA2BGR)\n",
    "    else:\n",
    "        raise TypeError(\"Input image must be a PIL Image or a NumPy array (OpenCV format).\")\n",
    "\n",
    "    # Get original image dimensions\n",
    "    h_orig, w_orig = img_cv.shape[:2]\n",
    "\n",
    "    # Calculate representative text height and baseline for a single line.\n",
    "    # This is used for consistent line spacing and overall text block height calculations.\n",
    "    (text_w_dummy, text_h_single_line), baseline_single_line = cv2.getTextSize(\n",
    "        \"Tg\", font_face, font_scale, thickness\n",
    "    )\n",
    "    # A reasonable spacing between lines, often a percentage of the font height.\n",
    "    line_spacing = int(text_h_single_line * 0.5)\n",
    "\n",
    "    # --- Word Wrapping Logic ---\n",
    "    wrapped_lines_info = [] # This list will store tuples of (line_text, line_width, line_height, line_baseline) for each line.\n",
    "    max_overall_text_width = 0 # Stores the width of the widest line\n",
    "    \n",
    "    if wordwrap:\n",
    "        # Determine the maximum available width for wrapping the text within the image.\n",
    "        # This considers the initial x-position and padding.\n",
    "        if org[0] is None: # If horizontally centered, available width is image width minus double padding.\n",
    "            available_width_for_wrapping = w_orig - (2 * padding_x)\n",
    "        else: # If a specific x-coordinate is provided, available width is from that point to the right edge.\n",
    "            available_width_for_wrapping = w_orig - org[0] - padding_x\n",
    "        \n",
    "        # Ensure the available width is not negative or too small to avoid issues.\n",
    "        available_width_for_wrapping = max(10, available_width_for_wrapping)\n",
    "\n",
    "        words = text.split(' ')\n",
    "        current_line_words = []\n",
    "        current_line_text = \"\"\n",
    "\n",
    "        for word in words:\n",
    "            # Construct a test line by adding the current word (with a space if not the first word).\n",
    "            test_line_text = (current_line_text + \" \" + word).strip()\n",
    "            # Get the size of this potential line.\n",
    "            (test_w, _), _ = cv2.getTextSize(\n",
    "                test_line_text, font_face, font_scale, thickness\n",
    "            )\n",
    "\n",
    "            # If adding the word makes the line too long AND there are already words in the current line,\n",
    "            # then the current line is complete and the new word starts a new line.\n",
    "            if test_w > available_width_for_wrapping and len(current_line_words) > 0:\n",
    "                # Calculate the actual size of the completed line.\n",
    "                (line_w, line_h), line_baseline = cv2.getTextSize(\n",
    "                    current_line_text, font_face, font_scale, thickness\n",
    "                )\n",
    "                wrapped_lines_info.append((current_line_text, line_w, line_h, line_baseline))\n",
    "                max_overall_text_width = max(max_overall_text_width, line_w)\n",
    "\n",
    "                # Start a new line with the current word.\n",
    "                current_line_words = [word]\n",
    "                current_line_text = word\n",
    "            else:\n",
    "                # The word fits, so add it to the current line.\n",
    "                current_line_words.append(word)\n",
    "                current_line_text = \" \".join(current_line_words)\n",
    "        \n",
    "        # After the loop, add any remaining text in the current_line_text as the last line.\n",
    "        if current_line_text:\n",
    "            (line_w, line_h), line_baseline = cv2.getTextSize(\n",
    "                current_line_text, font_face, font_scale, thickness\n",
    "            )\n",
    "            wrapped_lines_info.append((current_line_text, line_w, line_h, line_baseline))\n",
    "            max_overall_text_width = max(max_overall_text_width, line_w)\n",
    "\n",
    "    else: # If word wrapping is not enabled, treat the entire text as a single line.\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(text, font_face, font_scale, thickness)\n",
    "        wrapped_lines_info.append((text, text_w, text_h, baseline))\n",
    "        max_overall_text_width = text_w\n",
    "    \n",
    "    # Calculate the total height required by the entire block of wrapped text.\n",
    "    # This is the height from the top of the first line's ascenders to the bottom of the last line's descenders.\n",
    "    total_text_block_content_height = 0\n",
    "    if wrapped_lines_info:\n",
    "        # Top of the first line relative to its baseline (negative value)\n",
    "        first_line_top_offset_from_baseline = -(wrapped_lines_info[0][2] - wrapped_lines_info[0][3])\n",
    "        \n",
    "        # Baseline of the last line relative to the first line's baseline\n",
    "        last_line_baseline_offset_from_first_baseline = 0\n",
    "        if len(wrapped_lines_info) > 1:\n",
    "            last_line_baseline_offset_from_first_baseline = (len(wrapped_lines_info) - 1) * \\\n",
    "                                                              (text_h_single_line + line_spacing)\n",
    "        \n",
    "        # Bottom of the last line relative to its own baseline\n",
    "        last_line_bottom_offset_from_its_baseline = wrapped_lines_info[-1][3]\n",
    "\n",
    "        # Total content height = (last line's baseline + its bottom offset) - (first line's baseline + its top offset)\n",
    "        # We assume the first line's baseline is at y=0 for this calculation of the *span*.\n",
    "        total_text_block_content_height = (last_line_baseline_offset_from_first_baseline + last_line_bottom_offset_from_its_baseline) - \\\n",
    "                                          (first_line_top_offset_from_baseline)\n",
    "\n",
    "    # --- Determine Canvas Extension (Pre-calculation of text block position for extension check) ---\n",
    "    new_w, new_h = w_orig, h_orig\n",
    "    offset_x_canvas, offset_y_canvas = 0, 0\n",
    "\n",
    "    # Calculate the *desired* top-left corner of the text block's content area (without padding)\n",
    "    # relative to the original image's (0,0) if no canvas extension happens.\n",
    "    temp_text_block_x_content_start = org[0] if org[0] is not None else int((w_orig - max_overall_text_width) / 2)\n",
    "    \n",
    "    # temp_text_block_y_content_top represents the Y-coordinate of the *visual top* of the entire text block.\n",
    "    if org[1] is not None: # org[1] is the baseline of the first line\n",
    "        temp_text_block_y_content_top = org[1] + first_line_top_offset_from_baseline\n",
    "    else: # Vertical centering\n",
    "        temp_text_block_y_content_top = int((h_orig - total_text_block_content_height) / 2)\n",
    "\n",
    "    # Calculate bounding box for text block with padding for extension check\n",
    "    padded_x1 = temp_text_block_x_content_start - padding_x\n",
    "    padded_y1 = temp_text_block_y_content_top - padding_y \n",
    "    padded_x2 = temp_text_block_x_content_start + max_overall_text_width + padding_x\n",
    "    padded_y2 = temp_text_block_y_content_top + total_text_block_content_height + padding_y\n",
    "\n",
    "    # Check for left extension\n",
    "    if padded_x1 < 0:\n",
    "        offset_x_canvas = -padded_x1\n",
    "        new_w += offset_x_canvas\n",
    "\n",
    "    # Check for top extension\n",
    "    if padded_y1 < 0:\n",
    "        offset_y_canvas = -padded_y1\n",
    "        new_h += offset_y_canvas\n",
    "\n",
    "    # Recalculate padded coordinates based on potentially adjusted new_w/new_h\n",
    "    # This is needed to check for right/bottom extension against the *potential* new size.\n",
    "    # The new_w/new_h might have increased due to left/top extensions.\n",
    "    # The text block's position on this *potential* new canvas:\n",
    "    current_text_block_x_on_potential_canvas = temp_text_block_x_content_start + offset_x_canvas\n",
    "    current_text_block_y_top_on_potential_canvas = temp_text_block_y_content_top + offset_y_canvas\n",
    "\n",
    "    padded_x2_after_offset = current_text_block_x_on_potential_canvas + max_overall_text_width + padding_x\n",
    "    padded_y2_after_offset = current_text_block_y_top_on_potential_canvas + total_text_block_content_height + padding_y\n",
    "\n",
    "    # Check for right extension\n",
    "    if padded_x2_after_offset > new_w:\n",
    "        new_w = padded_x2_after_offset\n",
    "\n",
    "    # Check for bottom extension\n",
    "    if padded_y2_after_offset > new_h:\n",
    "        new_h = padded_y2_after_offset\n",
    "\n",
    "    # 4. Create new canvas if needed and paste original image.\n",
    "    if new_w > w_orig or new_h > h_orig:\n",
    "        # Create a new blank canvas with the specified background color.\n",
    "        new_image_canvas = np.full((new_h, new_w, 3), background_color, dtype=np.uint8)\n",
    "        # Paste the original image onto the new canvas at the calculated offset.\n",
    "        new_image_canvas[offset_y_canvas : offset_y_canvas + h_orig,\n",
    "                         offset_x_canvas : offset_x_canvas + w_orig] = img_cv\n",
    "        img_cv = new_image_canvas\n",
    "    \n",
    "    # Calculate the FINAL position of the text block's *content area* top-left corner on the (potentially new) canvas.\n",
    "    final_text_block_x_content_on_canvas = 0\n",
    "    final_text_block_y_content_top_on_canvas = 0\n",
    "\n",
    "    if org[0] is None: # Horizontal centering\n",
    "        # Center the entire text block (based on its widest line) horizontally on the new canvas.\n",
    "        final_text_block_x_content_on_canvas = int((img_cv.shape[1] - max_overall_text_width) / 2)\n",
    "    else: # Specific x-coordinate provided\n",
    "        final_text_block_x_content_on_canvas = temp_text_block_x_content_start + offset_x_canvas\n",
    "\n",
    "    if org[1] is None: # Vertical centering\n",
    "        # Center the entire text block vertically on the new canvas.\n",
    "        final_text_block_y_content_top_on_canvas = int((img_cv.shape[0] - total_text_block_content_height) / 2)\n",
    "    else: # Specific y-coordinate provided (baseline)\n",
    "        final_text_block_y_content_top_on_canvas = temp_text_block_y_content_top + offset_y_canvas\n",
    "\n",
    "    # 5. Place Text Background (before text) for the entire block.\n",
    "    # This is drawn only if transparency is greater than 0 and there is text to draw.\n",
    "    if text_background_transparency > 0 and wrapped_lines_info:\n",
    "        # Calculate the top-left and bottom-right corners of the entire text block's background.\n",
    "        # These are relative to the final position on the canvas.\n",
    "        x1_bg = final_text_block_x_content_on_canvas - padding_x\n",
    "        y1_bg = final_text_block_y_content_top_on_canvas - padding_y\n",
    "        x2_bg = final_text_block_x_content_on_canvas + max_overall_text_width + padding_x\n",
    "        y2_bg = final_text_block_y_content_top_on_canvas + total_text_block_content_height + padding_y\n",
    "\n",
    "        # Ensure background coordinates are within the image bounds to prevent drawing outside.\n",
    "        x1_bg = max(0, x1_bg)\n",
    "        y1_bg = max(0, y1_bg)\n",
    "        x2_bg = min(img_cv.shape[1], x2_bg)\n",
    "        y2_bg = min(img_cv.shape[0], y2_bg)\n",
    "\n",
    "        # Only draw the rectangle if the bounding box is valid (positive width and height).\n",
    "        if x2_bg > x1_bg and y2_bg > y1_bg:\n",
    "            overlay = img_cv.copy() # Create a copy to draw the background on.\n",
    "            # Draw a filled rectangle for the background.\n",
    "            cv2.rectangle(overlay, (x1_bg, y1_bg), (x2_bg, y2_bg), text_background_color, -1)\n",
    "            # Blend the overlay with the original image using the specified transparency.\n",
    "            alpha = text_background_transparency\n",
    "            cv2.addWeighted(overlay, alpha, img_cv, 1 - alpha, 0, img_cv)\n",
    "\n",
    "    # 6. Place Text (loop through each wrapped line).\n",
    "    # Calculate the baseline of the first line from the top of the text content block.\n",
    "    # The baseline of the first line is the top of the text content block + the distance from its top to its baseline.\n",
    "    current_line_y_baseline = final_text_block_y_content_top_on_canvas - first_line_top_offset_from_baseline\n",
    "\n",
    "    for i, (line_text, line_w, line_h, line_baseline) in enumerate(wrapped_lines_info):\n",
    "        line_org_x = final_text_block_x_content_on_canvas # Default to block's left edge\n",
    "        \n",
    "        # If the original request was for horizontal centering, recalculate x-origin for each line\n",
    "        # to ensure each line is individually centered within the *new* canvas.\n",
    "        if org[0] is None:\n",
    "            line_org_x = int((img_cv.shape[1] - line_w) / 2)\n",
    "\n",
    "        # Put the text on the image.\n",
    "        cv2.putText(img_cv, line_text, (line_org_x, current_line_y_baseline),\n",
    "                    font_face, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        # Move the y-coordinate down to the baseline of the next line.\n",
    "        # The height of the line itself is line_h, but we use text_h_single_line for consistent spacing.\n",
    "        if i < len(wrapped_lines_info) - 1: # Don't add spacing after the last line\n",
    "            current_line_y_baseline += (text_h_single_line + line_spacing)\n",
    "\n",
    "    # 7. Return the modified image.\n",
    "    return img_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88d42c",
   "metadata": {},
   "source": [
    "## get_random_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a4333d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_random_prompt(folder_path):\n",
    "    \"\"\"\n",
    "    Looks for all CSV files in a specified folder, processes them, updates a random row \n",
    "    with the minimum count, and returns the subject and filename.\n",
    "    The function assumes that the first row of each CSV file contains headers.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    all_data = []\n",
    "    \n",
    "    # 1. Look for all CSV files in the specified folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        return \"No CSV files found in the specified directory.\"\n",
    "\n",
    "    # 2. Read them into dataframes and process them\n",
    "    for filename in csv_files:\n",
    "        try:\n",
    "            # Read the CSV assuming the first row is the header\n",
    "            df = pd.read_csv(filename)\n",
    "            \n",
    "            # Standardize column names to 'subject' and 'count' for consistency\n",
    "            if 'subject' not in df.columns or 'count' not in df.columns:\n",
    "                print(f\"File {filename} does not have 'subject' and 'count' columns. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 2.b If count is blank, set it to 0 and ensure the column is int type\n",
    "            df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "            # 3. Remove rows where the first character of 'subject' is '#'\n",
    "            df = df[~df['subject'].astype(str).str.startswith('#')]\n",
    "            \n",
    "            # Add a column to store the original filename\n",
    "            df['original_file'] = filename\n",
    "            \n",
    "            all_data.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data:\n",
    "        return \"No valid data could be read from the CSV files.\"\n",
    "\n",
    "    # 4. Merge all dataframes into one\n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # 5. Sort them based on the count column\n",
    "    merged_df = merged_df.sort_values(by='count')\n",
    "\n",
    "    # 6. Select the set of rows that has the minimum value of count\n",
    "    min_count = merged_df['count'].min()\n",
    "    min_count_rows = merged_df[merged_df['count'] == min_count]\n",
    "    \n",
    "    if min_count_rows.empty:\n",
    "        return \"No rows with a minimum count were found.\"\n",
    "\n",
    "    # 7. Use a random row from this list\n",
    "    selected_row = min_count_rows.sample(n=1).iloc[0]\n",
    "    selected_subject = selected_row['subject']\n",
    "    selected_file = selected_row['original_file']\n",
    "\n",
    "    # 8. Update the original CSV value for this row and add one to its count\n",
    "    # 9. Save the original CSV\n",
    "    try:\n",
    "        # Read the original file again to ensure we are modifying the correct data\n",
    "        original_df = pd.read_csv(selected_file)\n",
    "        \n",
    "        # FIX: Ensure the count column is numeric with blank values as 0, \n",
    "        # so the comparison with min_count works correctly.\n",
    "        original_df['count'] = pd.to_numeric(original_df['count'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        # Find the row to update\n",
    "        # Using a more robust way to find the row to avoid issues with duplicates\n",
    "        # We'll match on both subject and the count before updating\n",
    "        original_df.loc[\n",
    "            (original_df['subject'] == selected_subject) & (original_df['count'] == min_count),\n",
    "            'count'\n",
    "        ] += 1\n",
    "        \n",
    "        # Save the updated dataframe back to the original CSV file\n",
    "        original_df.to_csv(selected_file, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error updating and saving file {selected_file}: {e}\"\n",
    "\n",
    "    # 10. Return the subject and the filename (without extension)\n",
    "    filename_without_ext = os.path.splitext(os.path.basename(selected_file))[0]\n",
    "    return  filename_without_ext,selected_subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2bf1c",
   "metadata": {},
   "source": [
    "## read_and_increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e754a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_increment(file_path=\"counter.txt\"):\n",
    "    \"\"\"\n",
    "    Reads a number from a file, returns it, and updates the file content with the number incremented by one.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the number.\n",
    "\n",
    "    Returns:\n",
    "        int: The number read from the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the current number from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            number = int(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, start with 0\n",
    "        number = 0\n",
    "    except ValueError:\n",
    "        # If the file content is invalid, raise an error\n",
    "        raise ValueError(f\"The file {file_path} does not contain a valid integer.\")\n",
    "\n",
    "    # Increment the number\n",
    "    incremented_number = number + 1\n",
    "\n",
    "    # Write the incremented number back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(str(incremented_number))\n",
    "\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4550df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logo=Image.open(str(assts_dir/\"aiartstudio_logo.png\"))\n",
    "# puzzel_no_text = f\"Puzzle No: {read_and_increment()}\"\n",
    "# logo1 = add_text_to_image(logo, puzzel_no_text, org=(None, 1800), font_scale=4, color=(0, 0, 255), thickness=10, wordwrap=True)\n",
    "# logo1_image = Image.fromarray(cv2.cvtColor(logo1, cv2.COLOR_BGR2RGB))\n",
    "# logo1_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b44ab",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae56ed7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d0e8eea14d45eeb71076256a67ff51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cd4b62dea443b6982f58b622fd90b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = SD15ImageGenerator(num_inference_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639094d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conceptPromptGenerator=OllamaConceptPromptGenerator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ce93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example Usage:\n",
    "speed_stop=0.5\n",
    "#speed_distribution = [(40, 1/30, speed_stop), (40, speed_stop, speed_stop), (19, speed_stop, 1/30)]\n",
    "speed_distribution = [(30, 1/30, 1/30), (30,1/30, speed_stop),(30, speed_stop, speed_stop), (9, speed_stop, 1/30)]\n",
    "Image_times = generate_evenly_distributed_values(speed_distribution)\n",
    "Image_times.append(5.202)\n",
    "#print(f\"Result for data1: {Image_times}\")\n",
    "print(f\"len {len(Image_times)}, sum {sum(Image_times)}\" )\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_images(number_of_images=100):\n",
    "    image_paths = working_dir/'TestFiles'/'sample.png'\n",
    "    image= Image.open(image_paths)\n",
    "    return [image] * number_of_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70851b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def gen_one_video():\n",
    "    # get prompt\n",
    "\n",
    "category=\"testing\"\n",
    "subject=\"Louvre Abu Dhabi (in a country)\"\n",
    "#category,subject = get_random_prompt(propmt_dir)\n",
    "print(category, subject)\n",
    "\n",
    "#get prompt detal ferom LLM\n",
    "prompt_dic={'Prompt': 'a detailed view of Louvre Abu Dubai, architectural marvel, modern design, glass dome, sun rays, reflection, desert backdrop', 'Negative Prompt': 'no details, wrong architecture, generic buildings, low resolution, no dome', 'fact': 'Louvre Abu Dhabi fuses French art with Middle Eastern influences.'}\n",
    "#prompt_dic=conceptPromptGenerator.generate(subject,category)\n",
    "print(prompt_dic)    \n",
    "\n",
    "#Generate images\n",
    "\n",
    "#images = generator.generate_image(prompt_dic['prompt'],prompt_dic['negative_prompt'] , guidance_scale=7.5)\n",
    "images=get_sample_images()\n",
    "\n",
    "\n",
    "\n",
    "#Create video\n",
    "\n",
    "editor = VideoEditor(fps=30, frame_size=(1080, 1920))\n",
    "#generator.save_images(images, directory=\"generated\")\n",
    "#editor.add_images_from_list(images, duration_sec=30)\n",
    "\n",
    "if(len(Image_times) != len(images   )):\n",
    "    print(\"time_duration and number of images are not matched.\")\n",
    "\n",
    "# add logo image    \n",
    "logo_canvas=ImageCanvas()\n",
    "logo_image=Image.open(str(assts_dir/\"aiartstudio_logo.png\"))\n",
    "logo_canvas.add_image(logo_image, resize_option='fill_width')  # Add logo for 3 seconds\n",
    "logo_canvas.add_text(f\"Puzzle No: {read_and_increment()}\",position=(0,1000),text_color=(2550, 0,0),align='center')\n",
    "logo_canvas.add_text(f\"Subscribe to train your brain with AI art\",position=(0,1150),text_color=(255,125,0),align='center')\n",
    "editor.add_image(logo_canvas.get_image(), 3)  # Add logo for 3 seconds\n",
    "\n",
    "# add AI generated images\n",
    "editor.set_global_display_time_offset_start(0)  # Set global offset to 0 seconds\n",
    "total_duration = 0\n",
    "\n",
    "\n",
    "\n",
    "subscribe_msg= \"Cracked it fast? Drop your time in the comments!\"\n",
    "Subscribe_position = (0, 1200)  # Position for the subscribe message\n",
    "\n",
    "for index, duration in enumerate(Image_times):\n",
    "    img=ImageCanvas()\n",
    "    img.add_image(images[index], resize_option='fill_width')  # Add image with fill width\n",
    "    img.add_text(f\"Guess what AI is drawing now?!\",position=(25,1000),align='left')\n",
    "    total_duration += duration\n",
    "    if int(total_duration)%2 == 0:\n",
    "        img.add_text(text=subscribe_msg, position=Subscribe_position, text_color=(255, 0, 0),align='center',font_size=75)\n",
    "    else:\n",
    "         img.add_text(text=subscribe_msg, position=Subscribe_position, text_color=(255, 0, 255),align='center',font_size=75)\n",
    "    editor.add_image(img.get_image(), duration,show_time_text=True,time_display_format=\"SS.MS\",time_text_position=(800, 1085),time_text_font_scale=2)\n",
    "\n",
    "\n",
    "\n",
    "final_image= ImageCanvas()\n",
    "final_image.add_image(images[-1], resize_option='fill_width')  # Add last image with fill width\n",
    "final_image.add_text(text=\"Times Up! , AI draw\",position=(0,1000), align='center',font_size=75)\n",
    "final_image.add_text(text=subject,position=(0,1100), align='center',font_size=75,text_color=(255, 0, 0))\n",
    "editor.add_image(final_image.get_image(), 5,show_time_text=False)\n",
    "\n",
    "final_image.add_text(text=\"Fact\",position=(0,1300), align='center',font_size=75)\n",
    "final_image.add_text(text=prompt_dic['fact'],position=(0,1400), align='center',font_size=75,text_color=(255, 0, 0))\n",
    "\n",
    "editor.add_image(final_image.get_image(), 10,show_time_text=False)\n",
    "\n",
    "\n",
    "\n",
    "editor.add_audio(str(assts_dir/\"Long Distance.mp3\"),audio_clip_end=editor.get_video_duration(),video_start_offset=0)  # Add audio starting at the beginning of the video\n",
    "#editor.add_image(images[-1],3)  # Add last image for 3 seconds\n",
    "fileName=generate_filename_by_datetime(category, \"mp4\")\n",
    "#full_path_filename = str(GDrive_dir/fileName)\n",
    "#print(full_path_filename)\n",
    "editor.save(str(videos_dir/fileName))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bf699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "img_final = add_text_to_image(images[-1], subscribe_msg, org=Subscribe_position, font_scale=0.8,color=(0, 0, 255),wordwrap=True)\n",
    "img=add_text_to_image(img_final.copy(), \"Time Up, AI draw:\",org=(None,250))\n",
    "img=add_text_to_image(img, subject,org=(None,350),color=(0, 0, 255), font_scale=1.2, thickness=3,wordwrap=True)\n",
    "editor.add_image(img, 5,show_time_text=False)\n",
    "\n",
    "img=add_text_to_image(img_final, \"Fact:\",org=(None,50))\n",
    "img=add_text_to_image(img,prompt_dic['fact'] ,org=(None,150),color=(0, 0, 255), font_scale=1.2, thickness=3,wordwrap=True)\n",
    "editor.add_image(img, 10,show_time_text=False)\n",
    "\n",
    "\n",
    "#gen_one_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222490ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    #gen_one_video()\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
