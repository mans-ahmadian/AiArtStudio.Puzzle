{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bce612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Require Python 3.12.3\n",
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f53fc",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdbb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edc07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assts_dir= Path(os.getcwd())/\"assets\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee1f25",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2713a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoEditor:\n",
    "    def __init__(self, fps=30, fourcc='mp4v', frame_size=(640, 480)):\n",
    "        self.fps = fps\n",
    "        self.frame_size = frame_size\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*fourcc)\n",
    "        self.temp_video_path = tempfile.mktemp(suffix='.mp4')\n",
    "        self.video_writer = cv2.VideoWriter(self.temp_video_path, self.fourcc, self.fps, self.frame_size)\n",
    "        self.current_time = 0  # in seconds, tracks the current duration of the video content\n",
    "        self.audio_clips = []\n",
    "\n",
    "    def _resize_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Resizes an image frame to the video's frame_size.\n",
    "        \"\"\"\n",
    "        return cv2.resize(frame, self.frame_size)\n",
    "\n",
    "    def _convert_to_cv2(self, image):\n",
    "        \"\"\"\n",
    "        Converts various image types (path, Pillow, NumPy array) to an OpenCV image (NumPy array)\n",
    "        and resizes it.\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            # Read image from file path\n",
    "            img = cv2.imread(image)\n",
    "            if img is None:\n",
    "                raise FileNotFoundError(f\"Image file not found or could not be read: {image}\")\n",
    "        elif isinstance(image, Image.Image):\n",
    "            # Convert Pillow Image to OpenCV format (RGB to BGR)\n",
    "            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            # Assume it's already an OpenCV image (NumPy array)\n",
    "            img = image\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image type. Must be a file path (string), Pillow Image, or NumPy array.\")\n",
    "        return self._resize_frame(img)\n",
    "\n",
    "    def add_image(self, image, duration_sec):\n",
    "        \"\"\"\n",
    "        Adds a single image to the video for a specified duration.\n",
    "\n",
    "        Args:\n",
    "            image: The image to add. Can be a file path (string), a Pillow Image object,\n",
    "                   or an OpenCV image (NumPy array).\n",
    "            duration_sec (float): The duration (in seconds) for which the image should be displayed.\n",
    "        \"\"\"\n",
    "        frame = self._convert_to_cv2(image)\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "        for _ in range(frame_count):\n",
    "            self.video_writer.write(frame)\n",
    "        self.current_time += duration_sec\n",
    "\n",
    "    def add_images_from_list(self, images, total_duration_sec):\n",
    "        \"\"\"\n",
    "        Adds several images to the video, distributing them evenly over a total duration.\n",
    "\n",
    "        Args:\n",
    "            images: Can be a string (directory path), a list of strings (image file paths),\n",
    "                    a list of OpenCV images (numpy arrays), or a list of Pillow images.\n",
    "            total_duration_sec (float): The total duration (in seconds) that these images\n",
    "                                        should occupy in the video.\n",
    "        \"\"\"\n",
    "        image_list = []\n",
    "\n",
    "        if isinstance(images, str) and os.path.isdir(images):\n",
    "            # If a directory is provided, read all supported image files from it\n",
    "            for filename in sorted(os.listdir(images)):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    image_list.append(os.path.join(images, filename))\n",
    "        elif isinstance(images, list):\n",
    "            # If a list is provided, check its contents\n",
    "            if all(isinstance(img, str) for img in images):\n",
    "                # List of image file paths\n",
    "                image_list = images\n",
    "            elif all(isinstance(img, (np.ndarray, Image.Image)) for img in images):\n",
    "                # List of OpenCV or Pillow image objects\n",
    "                image_list = images\n",
    "            else:\n",
    "                raise ValueError(\"List must contain only strings (paths), OpenCV images, or Pillow images.\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported 'images' type. Must be a directory path (string), a list of paths, a list of OpenCV images, or a list of Pillow images.\")\n",
    "\n",
    "        if not image_list:\n",
    "            print(\"No images found to add.\")\n",
    "            return\n",
    "\n",
    "        # Calculate duration for each individual image\n",
    "        single_image_duration = total_duration_sec / len(image_list)\n",
    "\n",
    "        # Add each image using the existing add_image method\n",
    "        for img in image_list:\n",
    "            self.add_image(img, single_image_duration)\n",
    "\n",
    "    def add_video(self, video_path):\n",
    "        \"\"\"\n",
    "        Adds another video clip to the current video. The added video retains its original duration.\n",
    "\n",
    "        Args:\n",
    "            video_path (str): Path to the video file to be added.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise FileNotFoundError(f\"Video file not found or could not be opened: {video_path}\")\n",
    "\n",
    "        video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if video_fps == 0: # Handle case where FPS might be reported as 0\n",
    "            video_fps = self.fps\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = self._resize_frame(frame)\n",
    "            self.video_writer.write(frame)\n",
    "\n",
    "        duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / video_fps\n",
    "        cap.release()\n",
    "        self.current_time += duration\n",
    "\n",
    "    def add_audio(self, audio_path, audio_clip_start=None, audio_clip_end=None, video_start_offset=None):\n",
    "        \"\"\"\n",
    "        Adds an audio clip to the video timeline.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "            audio_clip_start (float, optional): The start time (in seconds) within the audio file itself.\n",
    "                                                Defaults to 0 (beginning of the audio file).\n",
    "            audio_clip_end (float, optional): The end time (in seconds) within the audio file itself.\n",
    "                                              Defaults to the end of the audio clip.\n",
    "            video_start_offset (float, optional): The time (in seconds) on the video timeline where this\n",
    "                                                  audio should start. If None, it starts at the current\n",
    "                                                  end time of the video (`self.current_time`).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not load audio file {audio_path}: {e}\")\n",
    "\n",
    "        # Subclip the audio file based on audio_clip_start and audio_clip_end\n",
    "        if audio_clip_start is not None or audio_clip_end is not None:\n",
    "            # Ensure start is not greater than end if both are provided\n",
    "            if audio_clip_start is not None and audio_clip_end is not None and audio_clip_start > audio_clip_end:\n",
    "                raise ValueError(\"audio_clip_start cannot be greater than audio_clip_end.\")\n",
    "            \n",
    "            # Use 0 if audio_clip_start is None, and audio_clip.duration if audio_clip_end is None\n",
    "            start_subclip = audio_clip_start if audio_clip_start is not None else 0\n",
    "            end_subclip = audio_clip_end if audio_clip_end is not None else audio_clip.duration\n",
    "            \n",
    "            audio_clip = audio_clip.subclip(start_subclip, end_subclip)\n",
    "\n",
    "        # Determine the offset on the video timeline\n",
    "        offset_on_video = video_start_offset if video_start_offset is not None else self.current_time\n",
    "\n",
    "        self.audio_clips.append((audio_clip, offset_on_video))\n",
    "\n",
    "    def get_video_duration(self):\n",
    "        \"\"\"\n",
    "        Returns the current duration of the video content in seconds.\n",
    "        \"\"\"\n",
    "        return self.current_time\n",
    "\n",
    "    def save(self, output_path):\n",
    "        \"\"\"\n",
    "        Finalizes the video and merges audio if present.\n",
    "        \"\"\"\n",
    "        self.video_writer.release()\n",
    "\n",
    "        final_clip = VideoFileClip(self.temp_video_path)\n",
    "\n",
    "        if self.audio_clips:\n",
    "            all_audios = []\n",
    "            for audio, offset in self.audio_clips:\n",
    "                all_audios.append(audio.set_start(offset))\n",
    "            \n",
    "            # Create a composite audio clip, ensuring its duration doesn't exceed the video's duration\n",
    "            composite_audio = CompositeAudioClip(all_audios)\n",
    "            \n",
    "            # Set the audio to the final video clip. MoviePy will automatically trim the audio\n",
    "            # to the length of the video clip if the audio is longer.\n",
    "            final_clip = final_clip.set_audio(composite_audio)\n",
    "\n",
    "        print(f\"Saving video to {output_path}...\")\n",
    "        final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "        final_clip.close()\n",
    "        os.remove(self.temp_video_path)\n",
    "        print(\"Video saved successfully and temporary file removed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de60dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SD15ImageGenerator:\n",
    "    def __init__(self, model_id=\"runwayml/stable-diffusion-v1-5\", use_cuda=True, num_inference_steps=25):\n",
    "        \"\"\"\n",
    "        Initialize the Stable Diffusion 1.5 pipeline and inference settings.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.intermediate_images = []\n",
    "\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            safety_checker=None\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _capture_step(self, step, timestep, latents):\n",
    "        \"\"\"\n",
    "        Internal callback to capture the image at each step.\n",
    "        \"\"\"\n",
    "        # Decode latent to image at this step\n",
    "        with torch.no_grad():\n",
    "            image = self.pipe.vae.decode(latents / self.pipe.vae.config.scaling_factor).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image_pil = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "            self.intermediate_images.append(image_pil)\n",
    "\n",
    "    def generate_image(self, prompt, negative_prompt=None, guidance_scale=7.5):\n",
    "        \"\"\"\n",
    "        Generate image and collect intermediate steps.\n",
    "        Returns a list of PIL images (one per step).\n",
    "        \"\"\"\n",
    "        self.intermediate_images = []\n",
    "\n",
    "        with torch.autocast(self.device) if self.device == \"cuda\" else torch.no_grad():\n",
    "            _ = self.pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=self.num_inference_steps,\n",
    "                callback=self._capture_step,\n",
    "                callback_steps=1  # capture every step\n",
    "            )\n",
    "\n",
    "        return self.intermediate_images\n",
    "\n",
    "    def save_image(self, image: Image.Image, output_path: str):\n",
    "        \"\"\"\n",
    "        Save a single PIL image to the specified path.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        image.save(output_path)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "    def save_images(self, images, directory=\"generated\"):\n",
    "        \"\"\"\n",
    "        Save a list of images to the given directory.\n",
    "        \"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        for i, img in enumerate(images):\n",
    "            path = os.path.join(directory, f\"step_{i:02d}.png\")\n",
    "            img.save(path)\n",
    "        print(f\"Saved {len(images)} images to '{directory}/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170c009",
   "metadata": {},
   "source": [
    "# Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae56ed7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b95a452acb4d1d861361d403261c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "generator = SD15ImageGenerator(num_inference_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222490ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f883f2e650400cb6d9e251e26c608e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"a fantasy castle floating in the sky, vivid colors, highly detailed\"\n",
    "    \n",
    "images = generator.generate_image(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5f139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_evenly_distributed_values(data):\n",
    "    \"\"\"\n",
    "    Generates evenly distributed values for each tuple (number, start, end) in a list.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of tuples, where each tuple is (number, start, end).\n",
    "                     'number' is the count of values to generate, 'start' is the\n",
    "                     beginning of the range, and 'end' is the end of the range.\n",
    "\n",
    "    Returns:\n",
    "        list: A single list containing all the generated evenly distributed values.\n",
    "    \"\"\"\n",
    "    all_values = []\n",
    "    for num, start, end in data:\n",
    "        # Generate 'num' evenly distributed values between 'start' and 'end'\n",
    "        # np.linspace includes both start and end points\n",
    "        if num > 0:\n",
    "            generated_values = np.linspace(start, end, num).tolist()\n",
    "            all_values.extend(generated_values)\n",
    "    return all_values\n",
    "\n",
    "# Example Usage:\n",
    "# data1 = [(5, 0, 10), (3, 100, 102)]\n",
    "# result1 = generate_evenly_distributed_values(data1)\n",
    "# print(f\"Result for data1: {result1}\")\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]\n",
    "\n",
    "# data2 = [(1, 5, 5), (4, -2, 2)]\n",
    "# result2 = generate_evenly_distributed_values(data2)\n",
    "# print(f\"Result for data2: {result2}\")\n",
    "# # Expected output for data2: [5.0, -2.0, -0.6666666666666666, 0.6666666666666666, 2.0]\n",
    "\n",
    "# data3 = []\n",
    "# result3 = generate_evenly_distributed_values(data3)\n",
    "# print(f\"Result for data3: {result3}\")\n",
    "# # Expected output for data3: []\n",
    "\n",
    "# data4 = [(0, 1, 10)]\n",
    "# result4 = generate_evenly_distributed_values(data4)\n",
    "# print(f\"Result for data4: {result4}\")\n",
    "# # Expected output for data4: []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088ce93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for data1: [0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.016666666666666666, 0.03333333333333333, 0.04942528735632184, 0.06551724137931034, 0.08160919540229886, 0.09770114942528735, 0.11379310344827587, 0.12988505747126436, 0.14597701149425288, 0.16206896551724137, 0.1781609195402299, 0.19425287356321838, 0.21034482758620687, 0.2264367816091954, 0.2425287356321839, 0.25862068965517243, 0.2747126436781609, 0.2908045977011494, 0.30689655172413793, 0.32298850574712645, 0.3390804597701149, 0.35517241379310344, 0.37126436781609196, 0.3873563218390804, 0.40344827586206894, 0.41954022988505746, 0.435632183908046, 0.4517241379310345, 0.46781609195402296, 0.4839080459770115, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.44166666666666665, 0.3833333333333333, 0.325, 0.26666666666666666, 0.20833333333333331, 0.15000000000000002, 0.09166666666666667, 0.03333333333333333, 3]\n",
      "len 100, sum 24.066666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage:\n",
    "speed_stop=0.5\n",
    "#speed_distribution = [(40, 1/30, speed_stop), (40, speed_stop, speed_stop), (19, speed_stop, 1/30)]\n",
    "speed_distribution = [(40, 1/60, 1/60), (30,1/30, speed_stop),(20, speed_stop, speed_stop), (9, speed_stop, 1/30)]\n",
    "result1 = generate_evenly_distributed_values(speed_distribution)\n",
    "result1.append(3)\n",
    "print(f\"Result for data1: {result1}\")\n",
    "print(f\"len {len(result1)}, sum {sum(result1)}\" )\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70851b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 images to 'generated/'\n",
      "Saving video to output_video1.mp4...\n",
      "Moviepy - Building video output_video1.mp4.\n",
      "MoviePy - Writing audio in output_video1TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video1.mp4\n",
      "Video saved successfully and temporary file removed.\n"
     ]
    }
   ],
   "source": [
    "editor = VideoEditor(fps=30, frame_size=(512, 512))\n",
    "generator.save_images(images, directory=\"generated\")\n",
    "#editor.add_images_from_list(images, duration_sec=30)\n",
    "if(len(result1) != len(images   )):\n",
    "    print(\"time_duration and number of images are not matched.\")\n",
    "logo=Image.open(str(assts_dir/\"AiArtStudio.AILogo.png\"))\n",
    "editor.add_image(logo, 3)  # Add logo for 3 seconds\n",
    "\n",
    "for index, duration in enumerate(result1):\n",
    "   img = images[index] \n",
    "   editor.add_image(img, duration)\n",
    "\n",
    "editor.add_audio(str(assts_dir/\"Long Distance.mp3\"),audio_clip_end=editor.get_video_duration(),video_start_offset=0)  # Add audio starting at the beginning of the video\n",
    "#editor.add_image(images[-1],3)  # Add last image for 3 seconds\n",
    "editor.save(\"output_video1.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
