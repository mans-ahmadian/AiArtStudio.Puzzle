{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bce612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Require Python 3.12.3\n",
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f53fc",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdbb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "from transformers import CLIPFeatureExtractor\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0eaa39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edc07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assts_dir= Path(os.getcwd())/\"assets\"\n",
    "propmt_dir = Path(os.getcwd())/\"Prompts\"\n",
    "GDrive_dir = Path(\"run/user/1000/gvfs/google-drive:host=gmail.com,user=aiartstudio.ai/0AOT4cSJ5oKlpUk9PVA/1cBJcIkDKKJziO4CPcNyoIBUOQ6n_MshJ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee1f25",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2713a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoEditor:\n",
    "    def __init__(self, fps=30, fourcc='mp4v', frame_size=(640, 480)):\n",
    "        self.fps = fps\n",
    "        self.frame_size = frame_size\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*fourcc)\n",
    "        self.temp_video_path = tempfile.mktemp(suffix='.mp4')\n",
    "        self.video_writer = cv2.VideoWriter(self.temp_video_path, self.fourcc, self.fps, self.frame_size)\n",
    "        self.current_time = 0  # in seconds, tracks the actual current duration of the video content\n",
    "        self.audio_clips = []\n",
    "\n",
    "        # Default settings for time text overlay\n",
    "        self.default_show_time_text = False\n",
    "        self.default_time_text_position = (50, 50)  # Default top-left corner\n",
    "        self.default_time_text_color = (0, 255, 0)  # Default Green (B, G, R)\n",
    "        self.default_time_text_font_scale = 1.0\n",
    "        self.default_time_text_thickness = 2\n",
    "        self.default_time_text_font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        self.default_time_display_format = \"HH:MM:SS.MS\"\n",
    "\n",
    "        # New class-level parameter for global display time offset\n",
    "        # If None, the displayed time will default to the actual video current_time.\n",
    "        # If set, it overrides the default behavior for all subsequent additions\n",
    "        # where display_time_start_offset is not explicitly provided.\n",
    "        self.global_display_time_offset_start = None\n",
    "        # New: Counter for the displayed time when global_display_time_offset_start is active\n",
    "        self.display_time_counter = 0.0\n",
    "\n",
    "    def set_global_display_time_offset_start(self, offset_time):\n",
    "        \"\"\"\n",
    "        Sets a global offset for the time displayed on the video.\n",
    "        Any subsequent images added with show_time_text enabled and without\n",
    "        an explicit display_time_start_offset will use this value as their base.\n",
    "\n",
    "        Args:\n",
    "            offset_time (float): The starting time (in seconds) to display on the video.\n",
    "                                 Set to None to revert to using the actual video's current_time.\n",
    "        \"\"\"\n",
    "        if not isinstance(offset_time, (int, float)) and offset_time is not None:\n",
    "            raise TypeError(\"offset_time must be a number (int or float) or None.\")\n",
    "        if offset_time is not None and offset_time < 0:\n",
    "            raise ValueError(\"offset_time cannot be negative.\")\n",
    "        self.global_display_time_offset_start = offset_time\n",
    "        # Initialize the display_time_counter when the global offset is set\n",
    "        self.display_time_counter = offset_time if offset_time is not None else 0.0\n",
    "\n",
    "\n",
    "    def set_default_time_display_format(self, format_string):\n",
    "        \"\"\"\n",
    "        Sets the default format for displaying time on frames.\n",
    "\n",
    "        Args:\n",
    "            format_string (str): The desired format string using placeholders:\n",
    "                                 HH (hours), MM (minutes), SS (seconds), MS (milliseconds).\n",
    "                                 Placeholders are case-insensitive (e.g., \"hh\", \"mm\", \"ss\", \"ms\" also work).\n",
    "                                 Example: \"HH:MM:SS.MS\", \"ss.ms\", \"MM:SS\".\n",
    "        \"\"\"\n",
    "        self.default_time_display_format = format_string\n",
    "\n",
    "    def _resize_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Resizes an image frame to the video's frame_size.\n",
    "        \"\"\"\n",
    "        return cv2.resize(frame, self.frame_size)\n",
    "\n",
    "    def _convert_to_cv2(self, image):\n",
    "        \"\"\"\n",
    "        Converts various image types (path, Pillow, NumPy array) to an OpenCV image (NumPy array)\n",
    "        and resizes it.\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            img = cv2.imread(image)\n",
    "            if img is None:\n",
    "                raise FileNotFoundError(f\"Image file not found or could not be read: {image}\")\n",
    "        elif isinstance(image, Image.Image):\n",
    "            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            img = image\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image type. Must be a file path (string), Pillow Image, or NumPy array.\")\n",
    "        return self._resize_frame(img)\n",
    "\n",
    "    def _draw_time_on_frame(self, frame, time_in_sec, position, color, font_scale, thickness, font, time_format):\n",
    "        \"\"\"\n",
    "        Draws the current time onto a video frame with a specified format.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): The OpenCV image frame.\n",
    "            time_in_sec (float): The time in seconds to display.\n",
    "            position (tuple): (x, y) coordinates for the text.\n",
    "            color (tuple): (B, G, R) color for the text.\n",
    "            font_scale (float): Font scale factor.\n",
    "            thickness (int): Line thickness for the text.\n",
    "            font (int): OpenCV font type (e.g., cv2.FONT_HERSHEY_SIMPLEX).\n",
    "            time_format (str): The format string using placeholders (HH, MM, SS, MS).\n",
    "                               Placeholders are case-insensitive.\n",
    "        \"\"\"\n",
    "        total_seconds_int = int(time_in_sec)\n",
    "        milliseconds = int((time_in_sec - total_seconds_int) * 10)\n",
    "        seconds = total_seconds_int % 60\n",
    "        minutes = (total_seconds_int // 60) % 60\n",
    "        hours = total_seconds_int // 3600\n",
    "\n",
    "        replacements = {\n",
    "            \"HH\": f\"{hours:02}\", \"hh\": f\"{hours:02}\",\n",
    "            \"MM\": f\"{minutes:02}\", \"mm\": f\"{minutes:02}\",\n",
    "            \"SS\": f\"{seconds:02}\", \"ss\": f\"{seconds:02}\",\n",
    "            \"MS\": f\"{milliseconds:01}\", \"ms\": f\"{milliseconds:01}\"\n",
    "        }\n",
    "\n",
    "        time_str = time_format\n",
    "        for placeholder, value in replacements.items():\n",
    "            time_str = time_str.replace(placeholder, value)\n",
    "\n",
    "        cv2.putText(frame, time_str, position, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        return frame\n",
    "\n",
    "    def add_image(self, image, duration_sec,\n",
    "                  show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                  time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                  display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds a single image to the video for a specified duration, with optional time display.\n",
    "\n",
    "        Args:\n",
    "            image: The image to add. Can be a file path (string), a Pillow Image object,\n",
    "                   or an OpenCV image (NumPy array).\n",
    "            duration_sec (float): The duration (in seconds) for which the image should be displayed.\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "                                              Defaults to self.default_show_time_text.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the text.\n",
    "                                                  Defaults to self.default_time_text_position.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the text.\n",
    "                                               Defaults to self.default_time_text_color.\n",
    "            time_text_font_scale (float, optional): Font scale factor.\n",
    "                                                    Defaults to self.default_time_text_font_scale.\n",
    "            time_text_thickness (int, optional): Line thickness for the text.\n",
    "                                                 Defaults to self.default_time_text_thickness.\n",
    "            time_text_font (int, optional): OpenCV font type.\n",
    "                                            Defaults to self.default_time_text_font.\n",
    "            display_time_start_offset (float, optional): The starting time (in seconds) to display\n",
    "                                                         on the video for this segment.\n",
    "                                                         Precedence: local param > global param > actual video current time.\n",
    "            time_display_format (str, optional): The format for the time string (e.g., \"HH:MM:SS.MS\").\n",
    "                                                 Placeholders are case-insensitive.\n",
    "                                                 Precedence: local param > default class param.\n",
    "        \"\"\"\n",
    "        frame = self._convert_to_cv2(image)\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "\n",
    "        # Resolve text display parameters\n",
    "        _show_text = self.default_show_time_text if show_time_text is None else show_time_text\n",
    "        _position = self.default_time_text_position if time_text_position is None else time_text_position\n",
    "        _color = self.default_time_text_color if time_text_color is None else time_text_color\n",
    "        _font_scale = self.default_time_text_font_scale if time_text_font_scale is None else time_text_font_scale\n",
    "        _thickness = self.default_time_text_thickness if time_text_thickness is None else time_text_thickness\n",
    "        _font = self.default_time_text_font if time_text_font is None else time_text_font\n",
    "        _time_format = self.default_time_display_format if time_display_format is None else time_display_format\n",
    "\n",
    "        # Determine the base time for display based on precedence\n",
    "        _base_display_time_for_this_segment = self.current_time # Default fallback: actual video time\n",
    "\n",
    "        if display_time_start_offset is not None:\n",
    "            # Local override takes highest precedence\n",
    "            _base_display_time_for_this_segment = display_time_start_offset\n",
    "        elif self.global_display_time_offset_start is not None:\n",
    "            # Global override applies if no local override. Use the continuous display_time_counter.\n",
    "            _base_display_time_for_this_segment = self.display_time_counter\n",
    "\n",
    "        for i in range(frame_count):\n",
    "            # Calculate the time to display for the current frame\n",
    "            current_display_time = _base_display_time_for_this_segment + (i / self.fps)\n",
    "            frame_to_write = frame.copy() # Use a copy to avoid drawing on the original frame for next iteration\n",
    "            if _show_text:\n",
    "                frame_to_write = self._draw_time_on_frame(\n",
    "                    frame_to_write, current_display_time, _position, _color, _font_scale, _thickness, _font, _time_format\n",
    "                )\n",
    "            self.video_writer.write(frame_to_write)\n",
    "\n",
    "        # Always update the actual video time\n",
    "        self.current_time += duration_sec\n",
    "\n",
    "        # Only update display_time_counter if global offset is active AND no local offset was used\n",
    "        if display_time_start_offset is None and self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration_sec\n",
    "\n",
    "\n",
    "    def add_images_from_list(self, images, total_duration_sec,\n",
    "                             show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                             time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                             display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds several images to the video, distributing them evenly over a total duration,\n",
    "        with optional time display on each image.\n",
    "\n",
    "        Args:\n",
    "            images: Can be a string (directory path), a list of strings (image file paths),\n",
    "                    a list of OpenCV images (numpy arrays), or a list of Pillow images.\n",
    "            total_duration_sec (float): The total duration (in seconds) that these images\n",
    "                                        should occupy in the video.\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "                                              Defaults to self.default_show_time_text.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the text.\n",
    "                                                  Defaults to self.default_time_text_position.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the text.\n",
    "                                               Defaults to self.default_time_text_color.\n",
    "            time_text_font_scale (float, optional): Font scale factor.\n",
    "                                                    Defaults to self.default_time_text_font_scale.\n",
    "            time_text_thickness (int, optional): Line thickness for the text.\n",
    "                                                 Defaults to self.default_time_text_thickness.\n",
    "            time_text_font (int, optional): OpenCV font type.\n",
    "                                            Defaults to self.default_time_text_font.\n",
    "            display_time_start_offset (float, optional): The starting time (in seconds) to display\n",
    "                                                         on the video for this segment.\n",
    "                                                         Precedence: local param > global param > actual video current time.\n",
    "            time_display_format (str, optional): The format for the time string (e.g., \"HH:MM:SS.MS\").\n",
    "                                                 Placeholders are case-insensitive.\n",
    "                                                 Precedence: local param > default class param.\n",
    "        \"\"\"\n",
    "        image_list = []\n",
    "\n",
    "        if isinstance(images, str) and os.path.isdir(images):\n",
    "            for filename in sorted(os.listdir(images)):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    image_list.append(os.path.join(images, filename))\n",
    "        elif isinstance(images, list):\n",
    "            if all(isinstance(img, str) for img in images):\n",
    "                image_list = images\n",
    "            elif all(isinstance(img, (np.ndarray, Image.Image)) for img in images):\n",
    "                image_list = images\n",
    "            else:\n",
    "                raise ValueError(\"List must contain only strings (paths), OpenCV images, or Pillow images.\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported 'images' type. Must be a directory path (string), a list of paths, a list of OpenCV images, or a list of Pillow images.\")\n",
    "\n",
    "        if not image_list:\n",
    "            print(\"No images found to add.\")\n",
    "            return\n",
    "\n",
    "        single_image_duration = total_duration_sec / len(image_list)\n",
    "\n",
    "        for i, img in enumerate(image_list):\n",
    "            # The display_time_start_offset passed to add_image needs to be carefully managed.\n",
    "            # If the user provided a display_time_start_offset to add_images_from_list,\n",
    "            # we calculate the offset for each individual image within that batch.\n",
    "            # If not, we pass None, allowing add_image to use the global_display_time_offset_start\n",
    "            # and its internal display_time_counter, which is the desired behavior for continuous time.\n",
    "            effective_display_offset_for_image_batch = None\n",
    "            if display_time_start_offset is not None:\n",
    "                effective_display_offset_for_image_batch = display_time_start_offset + (i * single_image_duration)\n",
    "\n",
    "            self.add_image(img, single_image_duration,\n",
    "                           show_time_text=show_time_text,\n",
    "                           time_text_position=time_text_position,\n",
    "                           time_text_color=time_text_color,\n",
    "                           time_text_font_scale=time_text_font_scale,\n",
    "                           time_text_thickness=time_text_thickness,\n",
    "                           time_text_font=time_text_font,\n",
    "                           display_time_start_offset=effective_display_offset_for_image_batch, # Pass the calculated offset or None\n",
    "                           time_display_format=time_display_format)\n",
    "\n",
    "    def add_text_below_image(self, image, text, duration_sec,\n",
    "                             text_box_height_ratio=0.2,\n",
    "                             background_color=(0, 0, 0),\n",
    "                             text_color=(255, 255, 255),\n",
    "                             text_horizontal_alignment=\"center\",\n",
    "                             text_vertical_alignment=\"center\",\n",
    "                             font_scale=0.7, thickness=2, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                             show_time_text=None, time_text_position=None, time_text_color=None,\n",
    "                             time_text_font_scale=None, time_text_thickness=None, time_text_font=None,\n",
    "                             display_time_start_offset=None, time_display_format=None):\n",
    "        \"\"\"\n",
    "        Adds an image with text displayed in a box below it for a given duration.\n",
    "        The combined content fits within the video's frame_size.\n",
    "\n",
    "        Args:\n",
    "            image: The input image (path, Pillow, or NumPy array).\n",
    "            text (str): The text string to display.\n",
    "            duration_sec (float): The duration (in seconds) for which this combined frame should be displayed.\n",
    "            text_box_height_ratio (float): Ratio (0.0 to 1.0) of the frame height allocated to the text box.\n",
    "            background_color (tuple): (B, G, R) color for the text box background.\n",
    "            text_color (tuple): (B, G, R) color for the text.\n",
    "            text_horizontal_alignment (str): Horizontal alignment of text (\"left\", \"center\", \"right\").\n",
    "            text_vertical_alignment (str): Vertical alignment of text (\"top\", \"center\", \"bottom\").\n",
    "            font_scale (float): Font scale factor for the text.\n",
    "            thickness (int): Line thickness for the text.\n",
    "            font (int): OpenCV font type (e.g., cv2.FONT_HERSHEY_SIMPLEX).\n",
    "            # Parameters for optional time display (same as add_image)\n",
    "            show_time_text (bool, optional): Whether to show the current time on the image.\n",
    "            time_text_position (tuple, optional): (x, y) coordinates for the time text.\n",
    "            time_text_color (tuple, optional): (B, G, R) color for the time text.\n",
    "            time_text_font_scale (float, optional): Font scale factor for time text.\n",
    "            time_text_thickness (int, optional): Line thickness for time text.\n",
    "            time_text_font (int, optional): OpenCV font type for time text.\n",
    "            display_time_start_offset (float, optional): The starting time to display.\n",
    "            time_display_format (str, optional): The format for the time string.\n",
    "        \"\"\"\n",
    "        if not (0.0 <= text_box_height_ratio <= 1.0):\n",
    "            raise ValueError(\"text_box_height_ratio must be between 0.0 and 1.0.\")\n",
    "\n",
    "        original_img_frame = self._convert_to_cv2(image)\n",
    "        \n",
    "        video_width, video_height = self.frame_size\n",
    "        text_box_height = int(video_height * text_box_height_ratio)\n",
    "        image_display_height = video_height - text_box_height\n",
    "\n",
    "        if image_display_height <= 0:\n",
    "            raise ValueError(\"Image display height is zero or negative. Reduce text_box_height_ratio or increase frame_size.\")\n",
    "\n",
    "        resized_img_for_top = cv2.resize(original_img_frame, (video_width, image_display_height))\n",
    "\n",
    "        text_section = np.full((text_box_height, video_width, 3), background_color, dtype=np.uint8)\n",
    "\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        text_width, text_height_actual = text_size\n",
    "\n",
    "        text_x, text_y = 0, 0\n",
    "\n",
    "        if text_horizontal_alignment.lower() == \"center\":\n",
    "            text_x = (video_width - text_width) // 2\n",
    "        elif text_horizontal_alignment.lower() == \"right\":\n",
    "            text_x = video_width - text_width - 10\n",
    "        else: # Default to \"left\"\n",
    "            text_x = 10\n",
    "\n",
    "        if text_vertical_alignment.lower() == \"center\":\n",
    "            text_y = (text_box_height + text_height_actual) // 2\n",
    "        elif text_vertical_alignment.lower() == \"bottom\":\n",
    "            text_y = text_box_height - 10\n",
    "        else: # Default to \"top\"\n",
    "            text_y = text_height_actual + 10\n",
    "\n",
    "        cv2.putText(text_section, text, (text_x, text_y), font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        final_frame = np.vstack((resized_img_for_top, text_section))\n",
    "\n",
    "        frame_count = int(self.fps * duration_sec)\n",
    "\n",
    "        _show_text = self.default_show_time_text if show_time_text is None else show_time_text\n",
    "        _time_position = self.default_time_text_position if time_text_position is None else time_text_position\n",
    "        _time_color = self.default_time_text_color if time_text_color is None else time_text_color\n",
    "        _time_font_scale = self.default_time_text_font_scale if time_text_font_scale is None else time_text_font_scale\n",
    "        _time_thickness = self.default_time_text_thickness if time_text_thickness is None else time_text_thickness\n",
    "        _time_font = self.default_time_text_font if time_text_font is None else time_text_font\n",
    "        _time_format = self.default_time_display_format if time_display_format is None else time_display_format\n",
    "\n",
    "        _base_display_time_for_this_segment = self.current_time\n",
    "        if display_time_start_offset is not None:\n",
    "            _base_display_time_for_this_segment = display_time_start_offset\n",
    "        elif self.global_display_time_offset_start is not None:\n",
    "            _base_display_time_for_this_segment = self.display_time_counter\n",
    "\n",
    "\n",
    "        for i in range(frame_count):\n",
    "            current_display_time = _base_display_time_for_this_segment + (i / self.fps)\n",
    "            frame_to_write = final_frame.copy()\n",
    "            if _show_text:\n",
    "                frame_to_write = self._draw_time_on_frame(\n",
    "                    frame_to_write, current_display_time, _time_position, _time_color, _time_font_scale, _time_thickness, _time_font, _time_format\n",
    "                )\n",
    "            self.video_writer.write(frame_to_write)\n",
    "        \n",
    "        self.current_time += duration_sec\n",
    "\n",
    "        if display_time_start_offset is None and self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration_sec\n",
    "\n",
    "    def add_video(self, video_path):\n",
    "        \"\"\"\n",
    "        Adds another video clip to the current video. The added video retains its original duration.\n",
    "\n",
    "        Args:\n",
    "            video_path (str): Path to the video file to be added.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise FileNotFoundError(f\"Video file not found or could not be opened: {video_path}\")\n",
    "\n",
    "        video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if video_fps == 0:\n",
    "            video_fps = self.fps\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = self._resize_frame(frame)\n",
    "            self.video_writer.write(frame)\n",
    "\n",
    "        duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / video_fps\n",
    "        cap.release()\n",
    "        self.current_time += duration\n",
    "        # If global display offset is active, and we are adding a video (which doesn't have its own\n",
    "        # display_time_start_offset parameter), we should also advance the display_time_counter.\n",
    "        if self.global_display_time_offset_start is not None:\n",
    "            self.display_time_counter += duration\n",
    "\n",
    "\n",
    "    def add_audio(self, audio_path, audio_clip_start=None, audio_clip_end=None, video_start_offset=None):\n",
    "        \"\"\"\n",
    "        Adds an audio clip to the video timeline.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str): Path to the audio file.\n",
    "            audio_clip_start (float, optional): The start time (in seconds) within the audio file itself.\n",
    "                                                Defaults to 0 (beginning of the audio file).\n",
    "            audio_clip_end (float, optional): The end time (in seconds) within the audio file itself.\n",
    "                                              Defaults to the end of the audio clip.\n",
    "            video_start_offset (float, optional): The time (in seconds) on the video timeline where this\n",
    "                                                  audio should start. If None, it starts at the current\n",
    "                                                  end time of the video (`self.current_time`).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Could not load audio file {audio_path}: {e}\")\n",
    "\n",
    "        if audio_clip_start is not None or audio_clip_end is not None:\n",
    "            if audio_clip_start is not None and audio_clip_end is not None and audio_clip_start > audio_clip_end:\n",
    "                raise ValueError(\"audio_clip_start cannot be greater than audio_clip_end.\")\n",
    "            \n",
    "            start_subclip = audio_clip_start if audio_clip_start is not None else 0\n",
    "            end_subclip = audio_clip_end if audio_clip_end is not None else audio_clip.duration\n",
    "            \n",
    "            audio_clip = audio_clip.subclip(start_subclip, end_subclip)\n",
    "\n",
    "        offset_on_video = video_start_offset if video_start_offset is not None else self.current_time\n",
    "\n",
    "        self.audio_clips.append((audio_clip, offset_on_video))\n",
    "\n",
    "    def get_video_duration(self):\n",
    "        \"\"\"\n",
    "        Returns the current duration of the video content in seconds.\n",
    "        \"\"\"\n",
    "        return self.current_time\n",
    "\n",
    "    def save(self, output_path):\n",
    "        \"\"\"\n",
    "        Finalizes the video and merges audio if present.\n",
    "        \"\"\"\n",
    "        self.video_writer.release()\n",
    "\n",
    "        final_clip = VideoFileClip(self.temp_video_path)\n",
    "\n",
    "        if self.audio_clips:\n",
    "            all_audios = []\n",
    "            for audio, offset in self.audio_clips:\n",
    "                all_audios.append(audio.set_start(offset))\n",
    "            \n",
    "            composite_audio = CompositeAudioClip(all_audios)\n",
    "            \n",
    "            final_clip = final_clip.set_audio(composite_audio)\n",
    "\n",
    "        print(f\"Saving video to {output_path}...\")\n",
    "        final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "        final_clip.close()\n",
    "        os.remove(self.temp_video_path)\n",
    "        print(\"Video saved successfully and temporary file removed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de60dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SD15ImageGenerator:\n",
    "    def __init__(self, model_id=\"runwayml/stable-diffusion-v1-5\", use_cuda=True, num_inference_steps=25):\n",
    "        \"\"\"\n",
    "        Initialize the Stable Diffusion 1.5 pipeline and inference settings.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.intermediate_images = []\n",
    "\n",
    "        # Load the safety checker and feature extractor\n",
    "        # You might need to specify the subfolder if they are not at the top level of the model_id\n",
    "        safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        feature_extractor = CLIPFeatureExtractor.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
    "        self.negative_prompt = \"\"\"deformed, distorted, disfigured, bad anatomy, ugly, tiling, poorly drawn hands, poorly drawn face, \n",
    "                                  out of frame, low quality, jpeg artifacts, duplicate, morbid, mutilated, extra fingers, mutated hands,  mutation, blurry, dehydrated, \n",
    "                                  bad proportions, extra limbs, cloned face, gross proportions, malformed limbs, missing arms, missing legs, extra hands, fused fingers, wrong hand, \n",
    "                                  long neck, worst quality, watermark, signature, text, error, cropped, username, logo, lowres, oversaturated, washed out, \n",
    "                                  cloned, bad composition, crosseyed , squint, lazy eye , bad eyes, wrong eyes, missing teeth, bad teeth, ugly teeth, open mouth, too many teeth,\n",
    "                                  extra tongue, wrong mouth, ugly mouth, bad mouth, bad nose, ugly nose, wrong nose, missing nose, bad ear, ugly ear, wrong ear, missing ear,\n",
    "                                  extra ear, double ear, three ears, mutated ear, long ear, short ear, big ear, small ear,\n",
    "                                  bad hair, ugly hair, wrong hair, missing hair, bad skin, ugly skin, wrong skin, \n",
    "                                  missing skin, extra skin, mutated skin, bad clothing, ugly clothing, wrong clothing,missing clothing, mutated clothing, \n",
    "                                  big clothing, small clothing, bad background, ugly background, wrong background, bad lighting, ugly lighting, wrong lighting\"\"\"\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "             safety_checker=safety_checker,\n",
    "            feature_extractor=feature_extractor # Don't forget the feature_extractor\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _capture_step(self, step, timestep, latents):\n",
    "        \"\"\"\n",
    "        Internal callback to capture the image at each step.\n",
    "        \"\"\"\n",
    "        # Decode latent to image at this step\n",
    "        with torch.no_grad():\n",
    "            image = self.pipe.vae.decode(latents / self.pipe.vae.config.scaling_factor).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image_pil = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "            self.intermediate_images.append(image_pil)\n",
    "\n",
    "    def generate_image(self, prompt, negative_prompt=None, guidance_scale=7.5):\n",
    "        \"\"\"\n",
    "        Generate image and collect intermediate steps.\n",
    "        Returns a list of PIL images (one per step).\n",
    "        \"\"\"\n",
    "        self.intermediate_images = []\n",
    "        negative_prompt = negative_prompt or self.negative_prompt\n",
    "\n",
    "        with torch.autocast(self.device) if self.device == \"cuda\" else torch.no_grad():\n",
    "            _ = self.pipe(\n",
    "                prompt=\"high resolution image of: \"+prompt +\" ,8K, best quality, masterpiece, photorealistic, ultra-detailed, sharp focus\",\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=self.num_inference_steps,\n",
    "                callback=self._capture_step,\n",
    "                callback_steps=1  # capture every step\n",
    "            )\n",
    "\n",
    "        return self.intermediate_images\n",
    "\n",
    "    def save_image(self, image: Image.Image, output_path: str):\n",
    "        \"\"\"\n",
    "        Save a single PIL image to the specified path.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        image.save(output_path)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "    def save_images(self, images, directory=\"generated\"):\n",
    "        \"\"\"\n",
    "        Save a list of images to the given directory.\n",
    "        \"\"\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        for i, img in enumerate(images):\n",
    "            path = os.path.join(directory, f\"step_{i:02d}.png\")\n",
    "            img.save(path)\n",
    "        print(f\"Saved {len(images)} images to '{directory}/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7fea56",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5f139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_evenly_distributed_values(data):\n",
    "    \"\"\"\n",
    "    Generates evenly distributed values for each tuple (number, start, end) in a list.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of tuples, where each tuple is (number, start, end).\n",
    "                     'number' is the count of values to generate, 'start' is the\n",
    "                     beginning of the range, and 'end' is the end of the range.\n",
    "\n",
    "    Returns:\n",
    "        list: A single list containing all the generated evenly distributed values.\n",
    "    \"\"\"\n",
    "    all_values = []\n",
    "    for num, start, end in data:\n",
    "        # Generate 'num' evenly distributed values between 'start' and 'end'\n",
    "        # np.linspace includes both start and end points\n",
    "        if num > 0:\n",
    "            generated_values = np.linspace(start, end, num).tolist()\n",
    "            all_values.extend(generated_values)\n",
    "    return all_values\n",
    "\n",
    "# Example Usage:\n",
    "# data1 = [(5, 0, 10), (3, 100, 102)]\n",
    "# result1 = generate_evenly_distributed_values(data1)\n",
    "# print(f\"Result for data1: {result1}\")\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]\n",
    "\n",
    "# data2 = [(1, 5, 5), (4, -2, 2)]\n",
    "# result2 = generate_evenly_distributed_values(data2)\n",
    "# print(f\"Result for data2: {result2}\")\n",
    "# # Expected output for data2: [5.0, -2.0, -0.6666666666666666, 0.6666666666666666, 2.0]\n",
    "\n",
    "# data3 = []\n",
    "# result3 = generate_evenly_distributed_values(data3)\n",
    "# print(f\"Result for data3: {result3}\")\n",
    "# # Expected output for data3: []\n",
    "\n",
    "# data4 = [(0, 1, 10)]\n",
    "# result4 = generate_evenly_distributed_values(data4)\n",
    "# print(f\"Result for data4: {result4}\")\n",
    "# # Expected output for data4: []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf975f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_filename_by_datetime(postfix:str, extension: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a filename based on the current date and time with a specified extension.\n",
    "\n",
    "    The format of the filename will be 'YYYY-MM-DD-HH-MM-SS.extension'.\n",
    "\n",
    "    Args:\n",
    "        extension (str): The file extension (e.g., 'mp4', 'txt', 'jpg').\n",
    "                         It should not include the leading dot.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated filename.\n",
    "    \"\"\"\n",
    "    # Get the current date and time\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # Format the datetime object into a string\n",
    "    # YYYY: Year with century\n",
    "    # MM: Month as a zero-padded decimal number\n",
    "    # DD: Day of the month as a zero-padded decimal number\n",
    "    # HH: Hour (24-hour clock) as a zero-padded decimal number\n",
    "    # MM: Minute as a zero-padded decimal number\n",
    "    # SS: Second as a zero-padded decimal number\n",
    "    timestamp_str = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    # Construct the full filename\n",
    "    filename = f\"{timestamp_str}_{postfix}.{extension}\"\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1852187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_text_to_image(\n",
    "    image,\n",
    "    text: str,\n",
    "    org: tuple[int | None, int | None] = (10, 30),  # Bottom-left corner of the text string\n",
    "    font_face: int = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    font_scale: float = 1.0,\n",
    "    color: tuple[int, int, int] = (0, 0, 0),  # BGR color (Black by default)\n",
    "    thickness: int = 2,\n",
    "    background_color: tuple[int, int, int] = (255, 255, 255), # White background for new canvas\n",
    "    text_background_color: tuple[int, int, int] = (255, 255, 255), # White background for text by default\n",
    "    text_background_transparency: float = 0.8, # 80% transparency by default\n",
    "    padding_x: int = 5, # Horizontal padding for text background\n",
    "    padding_y: int = 5 # Vertical padding for text background\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds text to an image, extending the image size if the text falls outside\n",
    "    the original boundaries.\n",
    "\n",
    "    Args:\n",
    "        image: The input image. Can be an OpenCV (numpy.ndarray) or Pillow (PIL.Image.Image) image.\n",
    "        text (str): The text string to add.\n",
    "        org (tuple[int | None, int | None]): The bottom-left corner of the text string in (x, y) coordinates.\n",
    "                               Defaults to (10, 30). If x or y is None, it will be centered in that direction.\n",
    "        font_face (int): Font type. See cv2.FONT_HERSHEY_* for options.\n",
    "                         Defaults to cv2.FONT_HERSHEY_SIMPLEX.\n",
    "        font_scale (float): Font scale factor multiplied by the font-specific base size.\n",
    "                            Defaults to 1.0.\n",
    "        color (tuple[int, int, int]): Text color in BGR format. Defaults to (0, 0, 0) (Black).\n",
    "        thickness (int): Thickness of the text lines. Defaults to 2.\n",
    "        background_color (tuple[int, int, int]): Color to fill the extended canvas if the image\n",
    "                                                  needs to be resized. Defaults to (255, 255, 255) (White).\n",
    "        text_background_color (tuple[int, int, int]): Color of the text's background in BGR format.\n",
    "                                                       Defaults to (255, 255, 255) (White).\n",
    "        text_background_transparency (float): Transparency of the text background.\n",
    "                                              Value between 0.0 (fully transparent) and 1.0 (fully opaque).\n",
    "                                              Defaults to 0.5 (50% transparent).\n",
    "        padding_x (int): Horizontal padding to add around the text background. Defaults to 5 pixels.\n",
    "        padding_y (int): Vertical padding to add around the text background. Defaults to 5 pixels.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The image with the added text, in OpenCV (BGR) format.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Handle Image Input: Convert Pillow image to OpenCV format if necessary\n",
    "    if isinstance(image, Image.Image):\n",
    "        # Convert PIL image to NumPy array (OpenCV format - BGR)\n",
    "        # PIL uses RGB, OpenCV uses BGR, so convert color channels\n",
    "        img_np = np.array(image)\n",
    "        if img_np.ndim == 2: # Grayscale image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
    "        elif img_np.shape[2] == 4: # RGBA image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR)\n",
    "        else: # RGB image\n",
    "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        img_cv = image\n",
    "        # Ensure the image is BGR (3 channels) if it's grayscale\n",
    "        if img_cv.ndim == 2:\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_GRAY2BGR)\n",
    "        elif img_cv.shape[2] == 4: # Handle RGBA if passed as numpy array\n",
    "            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGBA2BGR)\n",
    "    else:\n",
    "        raise TypeError(\"Input image must be a PIL Image or a NumPy array (OpenCV format).\")\n",
    "\n",
    "    # Get original image dimensions\n",
    "    h_orig, w_orig = img_cv.shape[:2]\n",
    "\n",
    "    # 2. Calculate Text Size\n",
    "    # getTextSize returns ((width, height), baseline)\n",
    "    # text_w is the width of the text string\n",
    "    # text_h is the height of the text string (from top of ascenders to bottom of descenders)\n",
    "    # baseline is the distance from the bottom-left point (org) to the baseline of the text.\n",
    "    (text_w, text_h), baseline = cv2.getTextSize(text, font_face, font_scale, thickness)\n",
    "\n",
    "    # Initialize new dimensions and offsets for the original image\n",
    "    new_w, new_h = w_orig, h_orig\n",
    "    offset_x, offset_y = 0, 0\n",
    "\n",
    "    # Determine initial current_org_x and current_org_y based on user input or centering\n",
    "    # Use float for calculations to avoid integer division issues, then cast to int for coordinates\n",
    "    initial_org_x = org[0] if org[0] is not None else int((w_orig - text_w) / 2)\n",
    "    # For vertical centering, org[1] is the baseline. We want the text's vertical center\n",
    "    # (org[1] - text_h / 2) to align with the image's vertical center (h_orig / 2).\n",
    "    # So, org[1] = h_orig / 2 + text_h / 2.\n",
    "    initial_org_y = org[1] if org[1] is not None else int((h_orig + text_h) / 2)\n",
    "\n",
    "    current_org_x, current_org_y = initial_org_x, initial_org_y\n",
    "\n",
    "\n",
    "    # 3. Determine New Image Dimensions (if text is outside)\n",
    "\n",
    "    # Calculate bounding box for text with padding for extension check\n",
    "    # These are potential coordinates if the image were to be extended\n",
    "    padded_x1 = current_org_x - padding_x\n",
    "    padded_y1 = current_org_y - text_h - padding_y # Top of text + padding\n",
    "    padded_x2 = current_org_x + text_w + padding_x\n",
    "    padded_y2 = current_org_y + baseline + padding_y # Bottom of text + padding\n",
    "\n",
    "    # Check for left extension (text background starts before x=0)\n",
    "    if padded_x1 < 0:\n",
    "        offset_x = -padded_x1\n",
    "        new_w += offset_x\n",
    "        # Adjust current_org_x for the new canvas, so the text starts at the padded edge\n",
    "        current_org_x += offset_x\n",
    "\n",
    "    # Check for top extension (text background top edge is above y=0)\n",
    "    if padded_y1 < 0:\n",
    "        offset_y = -padded_y1\n",
    "        new_h += offset_y\n",
    "        # Adjust current_org_y for the new canvas, so the text baseline is at the padded edge\n",
    "        current_org_y += offset_y\n",
    "\n",
    "    # Recalculate padded coordinates based on potentially adjusted current_org_x, current_org_y\n",
    "    # This is crucial because if offsets were applied, the text's position relative to the new\n",
    "    # canvas's (0,0) has changed.\n",
    "    padded_x1 = current_org_x - padding_x\n",
    "    padded_y1 = current_org_y - text_h - padding_y\n",
    "    padded_x2 = current_org_x + text_w + padding_x\n",
    "    padded_y2 = current_org_y + baseline + padding_y\n",
    "\n",
    "    # Check for right extension (text background extends beyond original width)\n",
    "    if padded_x2 > new_w:\n",
    "        new_w = padded_x2\n",
    "\n",
    "    # Check for bottom extension (text background extends beyond original height)\n",
    "    if padded_y2 > new_h:\n",
    "        new_h = padded_y2\n",
    "\n",
    "    # 4. Create new canvas if needed and paste original image\n",
    "    if new_w > w_orig or new_h > h_orig:\n",
    "        # Create a new blank canvas with the background color\n",
    "        new_image_canvas = np.full((new_h, new_w, 3), background_color, dtype=np.uint8)\n",
    "\n",
    "        # Paste the original image onto the new canvas at the calculated offset\n",
    "        new_image_canvas[offset_y : offset_y + h_orig, offset_x : offset_x + w_orig] = img_cv\n",
    "        img_cv = new_image_canvas\n",
    "    \n",
    "    # Update the org coordinates to reflect the new canvas if it was extended\n",
    "    # This is important because putText will draw relative to the current image's top-left\n",
    "    adjusted_org = (current_org_x, current_org_y)\n",
    "\n",
    "    # 5. Place Text Background (before text)\n",
    "    if text_background_transparency > 0:\n",
    "        # Calculate the top-left and bottom-right corners of the text bounding box with padding\n",
    "        x1_bg = adjusted_org[0] - padding_x\n",
    "        y1_bg = adjusted_org[1] - text_h - padding_y\n",
    "        x2_bg = adjusted_org[0] + text_w + padding_x\n",
    "        y2_bg = adjusted_org[1] + baseline + padding_y\n",
    "\n",
    "        # Ensure coordinates are within image bounds (important if text is at edges)\n",
    "        x1_bg = max(0, x1_bg)\n",
    "        y1_bg = max(0, y1_bg)\n",
    "        x2_bg = min(img_cv.shape[1], x2_bg)\n",
    "        y2_bg = min(img_cv.shape[0], y2_bg)\n",
    "\n",
    "        if x2_bg > x1_bg and y2_bg > y1_bg: # Only draw if the bounding box is valid\n",
    "            # Create a rectangle for the background\n",
    "            overlay = img_cv.copy()\n",
    "            cv2.rectangle(overlay, (x1_bg, y1_bg), (x2_bg, y2_bg), text_background_color, -1) # -1 fills the rectangle\n",
    "\n",
    "            # Blend the overlay with the original image\n",
    "            alpha = text_background_transparency\n",
    "            cv2.addWeighted(overlay, alpha, img_cv, 1 - alpha, 0, img_cv)\n",
    "\n",
    "    # 6. Place Text\n",
    "    cv2.putText(img_cv, text, adjusted_org, font_face, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    # 7. Return Image\n",
    "    return img_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de65b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_random_prompt(folder_path: str) -> tuple[str, str] | None:\n",
    "    \"\"\"\n",
    "    Finds all .txt files in a specified folder, reads all lines from them,\n",
    "    and returns a single random line along with its originating filename (without extension).\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder to search for .txt files.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str] | None: A tuple containing the filename (without extension)\n",
    "                                and the randomly selected line, or None if no .txt\n",
    "                                files are found or if they are empty.\n",
    "    \"\"\"\n",
    "    # all_lines will now store tuples of (filename_without_extension, line_content)\n",
    "    all_lines = []\n",
    "    \n",
    "    # Check if the provided path is a valid directory\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: Folder '{folder_path}' not found or is not a directory.\")\n",
    "        return None\n",
    "\n",
    "    # Iterate over all files in the specified folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Check if it's a file and has a .txt extension\n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # Read all lines from the current .txt file\n",
    "                    lines_from_file = f.readlines()\n",
    "                    \n",
    "                    # Get the filename without extension\n",
    "                    filename_without_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "                    # Extend the main list with tuples of (filename_without_ext, cleaned_line)\n",
    "                    # Only add non-empty lines after stripping whitespace\n",
    "                    for line in lines_from_file:\n",
    "                        if line[0] == '#':\n",
    "                            continue\n",
    "                        cleaned_line = line.strip()\n",
    "                        if cleaned_line: # Only add if the line is not empty after stripping\n",
    "                            all_lines.append((filename_without_ext, cleaned_line))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "                continue # Continue to the next file even if one fails\n",
    "\n",
    "    # Check if any lines were collected\n",
    "    if not all_lines:\n",
    "        print(f\"No .txt files found or all found files are empty in '{folder_path}'.\")\n",
    "        return None\n",
    "    #print(*all_lines, sep='\\n' )\n",
    "    # Select and return a random (filename, line) tuple from the collected lines\n",
    "    return random.choice(all_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b44ab",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae56ed7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7372749e134f8e89a5b7c76aae40c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed a non-standard module StableDiffusionSafetyChecker(\n",
      "  (vision_model): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(257, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
      "). We cannot verify whether it has the correct type\n"
     ]
    }
   ],
   "source": [
    "generator = SD15ImageGenerator(num_inference_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088ce93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for data1: [0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.03333333333333333, 0.04942528735632184, 0.06551724137931034, 0.08160919540229886, 0.09770114942528735, 0.11379310344827587, 0.12988505747126436, 0.14597701149425288, 0.16206896551724137, 0.1781609195402299, 0.19425287356321838, 0.21034482758620687, 0.2264367816091954, 0.2425287356321839, 0.25862068965517243, 0.2747126436781609, 0.2908045977011494, 0.30689655172413793, 0.32298850574712645, 0.3390804597701149, 0.35517241379310344, 0.37126436781609196, 0.3873563218390804, 0.40344827586206894, 0.41954022988505746, 0.435632183908046, 0.4517241379310345, 0.46781609195402296, 0.4839080459770115, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.44166666666666665, 0.3833333333333333, 0.325, 0.26666666666666666, 0.20833333333333331, 0.15000000000000002, 0.09166666666666667, 0.03333333333333333, 5.202]\n",
      "len 100, sum 31.602\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage:\n",
    "speed_stop=0.5\n",
    "#speed_distribution = [(40, 1/30, speed_stop), (40, speed_stop, speed_stop), (19, speed_stop, 1/30)]\n",
    "speed_distribution = [(30, 1/30, 1/30), (30,1/30, speed_stop),(30, speed_stop, speed_stop), (9, speed_stop, 1/30)]\n",
    "result1 = generate_evenly_distributed_values(speed_distribution)\n",
    "result1.append(5.202)\n",
    "print(f\"Result for data1: {result1}\")\n",
    "print(f\"len {len(result1)}, sum {sum(result1)}\" )\n",
    "# # Expected output for data1: [0.0, 2.5, 5.0, 7.5, 10.0, 100.0, 101.0, 102.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70851b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Mirror\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d615b9c7c84927936d8a9024735c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-21-51_Object.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-21-51_Object.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-21-51_ObjectTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-21-51_Object.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-21-51_Object.mp4\n",
      "Video saved successfully and temporary file removed.\n"
     ]
    }
   ],
   "source": [
    "def gen_one_video():\n",
    "    category,prompt = get_random_prompt(propmt_dir)\n",
    "    print(category, prompt)\n",
    "        \n",
    "    images = generator.generate_image(prompt+ \" (\"+category+\")\" , guidance_scale=7.5)\n",
    "    editor = VideoEditor(fps=30, frame_size=(512, 512))\n",
    "    #generator.save_images(images, directory=\"generated\")\n",
    "    #editor.add_images_from_list(images, duration_sec=30)\n",
    "    if(len(result1) != len(images   )):\n",
    "        print(\"time_duration and number of images are not matched.\")\n",
    "    logo=Image.open(str(assts_dir/\"aiartstudio_logo.png\"))\n",
    "    editor.add_image(logo, 3)  # Add logo for 3 seconds\n",
    "\n",
    "    editor.set_global_display_time_offset_start(0)  # Set global offset to 0 seconds\n",
    "    for index, duration in enumerate(result1):\n",
    "        img = images[index] \n",
    "        img1=add_text_to_image(img, f\"Guess what AI painting now?!\",org=(5,535),font_scale=0.8)\n",
    "        editor.add_image(img1, duration,show_time_text=True,time_display_format=\"SS.MS\",time_text_position=(400, 505),time_text_font_scale=0.8)\n",
    "\n",
    "    img=add_text_to_image(images[-1], \"Time Up, AI draw:\",org=(None,250))\n",
    "    img=add_text_to_image(img, prompt,org=(None,300),color=(0, 0, 255), font_scale=1.2, thickness=3)\n",
    "    editor.add_image(img, 5,show_time_text=False)\n",
    "    editor.add_audio(str(assts_dir/\"Long Distance.mp3\"),audio_clip_end=editor.get_video_duration(),video_start_offset=0)  # Add audio starting at the beginning of the video\n",
    "    #editor.add_image(images[-1],3)  # Add last image for 3 seconds\n",
    "    fileName=generate_filename_by_datetime(category, \"mp4\")\n",
    "    #full_path_filename = str(GDrive_dir/fileName)\n",
    "    #print(full_path_filename)\n",
    "    editor.save(f\"videos/{fileName}\")\n",
    "\n",
    "gen_one_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222490ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal Buffalo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d163e48cc6ff4d3ebe06ab854ff395f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-22-48_Animal.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-22-48_Animal.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-22-48_AnimalTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-22-48_Animal.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-22-48_Animal.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Celeb LeBron James\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774c1d3666694a669b1f4cdff1c15732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-23-45_Celeb.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-23-45_Celeb.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-23-45_CelebTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-23-45_Celeb.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-23-45_Celeb.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Building Western Wall (Israel)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8f8cbf652d45ca96661de4faf5cd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-24-42_Building.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-24-42_Building.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-24-42_BuildingTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-24-42_Building.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-24-42_Building.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Building Canton Tower (China)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea13d6b667fb42268f8eb74b6fefe529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-25-39_Building.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-25-39_Building.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-25-39_BuildingTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-25-39_Building.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-25-39_Building.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Object Coffee maker\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738c72754c6d493893107ecedcce9e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-26-36_Object.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-26-36_Object.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-26-36_ObjectTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-26-36_Object.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-26-36_Object.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Place Glasgow (United Kingdom)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c48b0dd9aa4f45977339746a1741a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-27-33_Place.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-27-33_Place.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-27-33_PlaceTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-27-33_Place.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-27-33_Place.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Animal Sloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e51bd84b9e46abb219fd1c3818388b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-28-30_Animal.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-28-30_Animal.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-28-30_AnimalTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-28-30_Animal.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-28-30_Animal.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Place Florence (Italy)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6266a8be664b319eb37cceb96b08ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-29-27_Place.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-29-27_Place.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-29-27_PlaceTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-29-27_Place.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-29-27_Place.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Celeb Andy Warhol\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed349c7ce3ad4155a182da9144e83e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-30-24_Celeb.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-30-24_Celeb.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-30-24_CelebTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-30-24_Celeb.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-30-24_Celeb.mp4\n",
      "Video saved successfully and temporary file removed.\n",
      "Building Flatiron Building (USA)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9efa5d8ca4b4abb981e5e00429f8d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to videos/2025-07-26-15-31-22_Building.mp4...\n",
      "Moviepy - Building video videos/2025-07-26-15-31-22_Building.mp4.\n",
      "MoviePy - Writing audio in 2025-07-26-15-31-22_BuildingTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2025-07-26-15-31-22_Building.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2025-07-26-15-31-22_Building.mp4\n",
      "Video saved successfully and temporary file removed.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    gen_one_video()\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
